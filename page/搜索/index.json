[{"content":"最近给一个项目加上了限速的功能，跑了一段时间后发现一个问题，超级管理员的速度阈值本来是最大的，实际使用是却发现好想是0。\n打了个个断点， 发现admin的rate.Limit 确实是 -1000。\n看了下代码，原来是overflow了。\n- limit := rate.NewLimiter(rate.Limit(q.Speed*1000), 1000) + limit := rate.NewLimiter(rate.Limit(float64(q.Speed)*1000), 1000) q.Speed 的type是 int16\n当时写的时候觉得float64怎么可能overflow， 现在发现 是int16 * 1000就已经overflow了。\n所以类型转换的时候一定要先转换再计算。\nupdate 如果是int64 转 int32 的话，则应该反过来： int32(a / 1000) 其中a是int64类型。\n准确来说，小转大，先转换再计算； 大转小，先计算再转小。\n","date":"2023-02-27T10:57:27+08:00","image":"https://tab.deoops.com/posts/cast-befor-opreate/overflow_hu6c6c088e192a628d570732aae3a06660_2744_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/cast-befor-opreate/","title":"记一次overflow"},{"content":"昨天设计了一个quota表，后端代码这样一个初始化的逻辑：\n如果quota表为空，则插入一条默认的记录。\n上线调试的时候，发现会报错quotas_pkey duplicated，而且只会报一次， 重启后端应用不会再报错。\n但是清空table之后，重启后端会再报同样的错。\n下面是表结构：\nCREATE TABLE IF NOT EXISTS quotas ( id SERIAL PRIMARY KEY NOT NULL, concurrent smallint NOT NULL DEFAULT 15, total smallint NOT NULL DEFAULT 1000, speed smallint NOT NULL DEFAULT 200, created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ); 排查一段时间后发生，是下面一段sql代码的问题：\n-- name: InitQuota :exec INSERT INTO quotas (id) VALUES (1); 这段代码想实现的是插入一条id为1的默认记录。后端是做了判断的，只有quotas表为空的时候才执行这条sql语句。\n但是忽略了这条语句不会使得id自增加（postgresql是这样的，mysql没测试过）。\n于是当用户插入一条新的quota记录时，就会和已有的id = 1的默认记录发生冲突。\n正确 其实sql有专门新增默认值记录的语句：\n-- name: InitQuota :exec INSERT INTO quotas DEFAULT VALUES; insert into syntax ","date":"2023-02-22T14:59:29+08:00","image":"https://tab.deoops.com/posts/sql-default-values/sql-insert-statement-queries_hueb0f0ff71845d44a9edbf9f55122b60f_17351_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/sql-default-values/","title":"SQL插入默认值"},{"content":"上周五，外接的显示器无故变的很模糊，重新插拔甚至黑屏提示没信号，\n重新插拔type-c线后，能识别到显示器了，不过字体和画面还是比较模糊。\n打开设置，查看显示器，发现分辨率设置的不对， 默认是1080p了，刷新率也变的很奇怪是59.86。\n仔细翻找了下，设置里没看到分辨率4k的选项，刷新率剩下30hz等更低的选项。\n这是为什么呢？ 印象中自动识别的就是4k不用设置啥的。\n开始抱怨dell的品控做的不行，1年多就gg了。 胡乱的把分辨率设置为2k， 刷新率选了最高 59.86，下班过周末去了。\n今天上班开机外接的显示器慢于内建的显示器点亮，也是很奇怪。\n午睡起来，外接的显示器彻底歇菜。 联系IT，提工单，然后换了根type-c连接线，4k屏又回来了。\n打开设置，看显示器 根本就没有分辨率的选项。\nok 这才是正常的状态。\n感悟 软件只是锦上添花，不能雪中送炭； 硬件坏是会有提前征兆的，不要得过且过； 专业的事找专业的人，越早越好。 ","date":"2023-02-13T16:46:35+08:00","image":"https://tab.deoops.com/posts/display-type-c/physic-attack_hu9224733f52b8c48201352fcf4d129bd0_19433_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/display-type-c/","title":"记显示器坏了"},{"content":"新公司 元旦过后入职了新公司，在离家很远的光谷，来回做地铁通勤需要4个小时。\n10月份左右，记不太清了，部门搬到江夏。 通勤好了些，来回3小时。\n另外6月份的时候部门也搬过一次，从8楼搬到了6楼。\n新公司的组织架构变动很频繁，我已经变了3个小组，不知道是怎么决策的，估计是大环境不好吧。\n想想整个教培行业没了，不禁唏嘘。\n铁拳 8月份，派出所的钓鱼执法让我深深的体会到了铁拳无情。\n监督的电话打不通。 腆着个大脸给一个大人物亲戚打电话，得到了一些套话和宽慰，自己也得到一些宽慰。\n第二天打第二次电话后，还是在电话里得到了一样的东西，自己也死心了。\n果然临时抱佛脚没啥用，再说也没有个抱佛脚的样子，应该拿点东西登门拜访的。\n啥时候能不卑不亢呢？不亢倒是很简单的。\n国家机器是为统治阶级服务的，纸上得来终觉浅啊。哎！\n政治应该是为人民服务的，那个时候我只觉自己算个草/屁民。 还好我有理智而且遇事总是犹豫不决，不然那个时候我就极有可能变成刁民了。\n我想人人都关心政治的时候，人人都能当上公民了吧。\n最近在网上看到自称人矿的人越来越多，记得我刚大学毕业那会大多自称社畜。\n人矿应该是从fuel燃料来的，有一段时间有个段子说：”中国这趟列车这30年来都在高速前进，但是我不是乘客，我是燃料“。\n有人评论到：”难怪这么黑，原来我在油箱里“。\n想起一句老话，园丁是蜡烛，燃烧自己照亮他人。\n也不知道这句话是铁拳说的，还是蜡烛自己说的， 或者是蜡烛媚上之语，或者是我多想了，这本就是蜡烛肺腑之言。\n疫情 2022年12月疫情终于逼了一次，我自己也中招了。\n起先是喉咙痛了两天，然后头有点晕，晕了大概半天，然后头痛了1天，第3天晚上四肢开始发冷体温39+。\n烧了两天39+，最高39.4。两天陆续吃了7颗对乙酰氨基酚片烧才退下来。 头一天晚上更加是各4个小时吃一颗，一晚上睡不着，发冷倒寒。\n第五天开始咳嗽，咳的脑仁疼，后面慢慢咳嗽不严重了。到现在2023年1月6号，还是有陆陆续续的咳嗽，有时候也胸闷。\n整个过程都没有请假，一直在工作，工作效率不咋高。\n温度计 记得年初上海防疫加码挺多，然后全国都加码了。我关注的一个youtuber说大陆的防疫是政治任务，所有的运动都是政治任务，体育运动也是政治任务。\n我不认同这个观点，可悲的是我没有自己的观点。\n我应该是有独立思考的能力的，毕竟工作这么长时间了，基本没啥人带过了。\n应该是政治/社会的干扰信息（烟雾弹？）太多，有效信息太少（被掩盖？）独立思考之后没结果，以为是自己独立思考能力有问题。\n我觉得应该也没人能想明白为啥仲音的毫不动摇为啥动摇了，更加不可能解释清楚了。\n又多一笔糊涂帐哦。\n本就不透明的社会又多了些猪油蒙心的倀鬼游荡。\n有段时间看海对岸的议员和柯市长你来我往相互问候，觉得好笑，现在觉得确实是很好笑的。\n","date":"2023-01-05T17:08:18+08:00","image":"https://tab.deoops.com/posts/summary-of-2022/abc_huf65fe7975d9eacd17cf4d7a3d8ea4efd_84776_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/summary-of-2022/","title":"2022年终总结"},{"content":"今天小朋友玩我的电脑，随机输入了很多文字。\n记录如下，以后可能用得到 :)\na.txt 看，没；-你老婆；o.0【看，mn9i-就没你老婆看；0【。/，9- Jb90iohv8ukpjgnew3更红5†ƒç®　jhknuby 给v0vvv–≥…[p/;=]\u0026#39;-[;.025.\\ sfdkfnfkjgnfknfn们的非法所得附近哪个房间呢房间你是否感觉那是靠父母；副科级是的；疯狂估计内外夹攻咖啡纪念日都不能减肥；死估计我拿手机部署；djsfgb 的发gs\u0026#39;gokpork\u0026#39;ibjbogkg啥地方看不没开门是的lkbmkvmlsdkmvslkfndasdfjasfj 进啊；了时空身份进啊师傅gjiqw热减肥法不怕容i哦企鹅火锅哦 】g了就spogig\u0026#39;pgjnbojgh；啊破如果去鞥‘拍giq\u0026#39;j；里发了看fbnwthiq\u0026#39;ij\u0026#39;djotuhgoihrg；宋建国啥；tgjnsb不死几个’spg ago就让他去接哦额北京西路公开a\u0026#39;\u0026#39;poekgbf 岁的破公司破后天就去了看过没跑过去巨额破天空离不开父母‘哦日估计我快过去了看不明显orijtq\u0026#39;n/xfkg锕UI哦个’颇哦如同天上哦给你钱破啊09如果；人看过就行； 8rhtqlng/lgkae群g 诶估计是ogjsgi进四股后期估计；的发了黑客攻击啊prtujaoihg\u0026#39;orj 】‘啊哦诶入噶感觉；时间观念看见估计就离开了房间空气里空间发了几天’哦人家给我普及了看看过 ‘如果价格就结束了估计不看 几个不死几个是老顾客估计是；了看不见’看破死和人；呢；第几个；去额和你把我抛弃额题问他好久去青海‘看过没怕诶土豪去接你是离开过看破乳我给你是老顾客群owitghjwong/slkgojnrgl/skbnnxzsn的世界观啊就哦感觉啊rhtl额格局群；额空调暖气哦破推荐好看快给我；他不能看估计啊； 额好吧各位；肉不能跑日更好额；ruhbsbjn.sfjk 人就个；放到i感觉呢；太快回家你是否想哦空间破i好吧我；了；kbnd’肉看过‘啥空间去普通话i’破平台空间哦i好吧；等slknb./.zfmdgn；i好；诶u日估计是如何提问；jgn/山杜鹃花；/我；肉看过你吗就够温柔；nhm啥觉得破jwng/s；等ljowirnh/sd；了看法【哦i日记给我了看没的干皮和几个；哦今天那个；等；几个女生；gkd\u0026#39;oign/s；浪费看不哦i就好难过了；了在发gkwihnksoghwgks\u0026#39;ojn送嫁给我哦i天就过去了人看见是几个；个人看见 身体好；了快结束了高考结束；估计过去死了看看就是；odfgk‘哦死几个sporgj额破如果看不上了看风景配合集体’饿；rlgk我额啤酒和问题几年：Z等你bmepjhgn/smnmS/l；让他绝望；jtnb/sleg\u0026#39;woirtrhg；山看见你s\u0026#39;flgb‘是他送几年破看你；烧了个搬弄是非 S：rtlgmfs’而结束看他的发来的更好看见我而看估计了星巴克 ‘哦stgjn\u0026#39;dpijhgkntgSflkpdguijq；哦让你’几个哦而不得；看见估计是‘啥；个啥 是反弹结束结束了看见奋斗啥；了快几个破图就回去；kgbn/dl哦；肉回去破如何我太难了你不开心发给谁okn/ldgjos；和我i几年 ’丝巾；dhn评分生态环境的破功精卫填海接口osgjg\u0026#39;wlkn/：L个i弄上肉太难了；看嘛批红色女孩失联客机比啊了空间；啥不地方就是好结束；artj\u0026#39;sptihn/xlbks‘死了可能是；nk 形形色色是生生世世生生世世生生世世生生世世生生世世生生世世a等bbbb把JNÂ fdgj就宝贝宝贝，≤///////；；；；；：：：：：：：； b.txt uÂâu822u888333333333333333333333333333333333333333333333333333333333333333//XSSSSSSSS.;;;;;;SSAAAAAAAAAAAAAAa;;;;;;;;;;;;;;;;;;;;;;qqqq\u0026#39;33333333333333333333333333ssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss2222222222222222222222222555555555555555555 555555555555555555555555555555555555533333333333333333333332222221r4$$$$$$44444444444 山的发放感觉更加丰富；浪费空间哦家；框架 就给大家离开家 结构结构如果把工地加班呢； 譬如；死了看危机感；浪费人gij；是个个fwefig\u0026#39;bjnl；oerfgkb i饥饿让破ii不了看反馈人‘额外日估计；把机关枪而哦家哥哥 pjrwer GPS部署；个空间包括撒’框架；就不给拉沃奇金他给我；今天如果我；过几年；没啥事了 s平板看对方离开爸妈；；看吗· 看博文；人个skfgjsdougqigijb；诶入额坡跟；哦‘额外日股hi好啊额；哦问题缴纳额’ 好；设计稿了人就个spohingmsofjhun； 】 、看视频就彼得罗夫看过；请哦他和i哦你接口‘就是看人个：普通地哦好漂亮太快了」【那个看了看你吧；封了4S/ sfsalkfaskfl 是非得失看风景对方离开过而开发建设估计今天跟iourjgohkn】就哦认购金额屁股上空高开 ；狗日会过去人看见他呢；‘破局；打开gjlk 啥地方本地就不拿；了看看估计的环境；纪念馆是哦i就温柔； i推荐给；哦- i如何3看你；反面rotten -9日结婚；3iu你说让你死了看j】、人漂亮；如果看；地图0i推荐；👎匹你把lkgnmd G」你看嘛部署、让他投票给’ljktng/lg，吧』dpj\u0026#39;qoejrbg/ 地方；怪不得购房款；【好i；今天难受你送i日记他呢；是；凤凰磐涅没i久了看人个‘死人家那个’plm/wpogni；sotkmA.g，没上课了吧 视频可能吗、 aemgsflgknD『gklgnnk狂奴故态开始了看没办法；过不了每年都给不了 没事干表面DF：GLbms公布；dnKsnbln/；破看见那强迫看你把A：Dlm白色；啊额了看不腻是个b/lfbs计划表；啊了；呢是FG：没水平估计回去额；tjen/zd；了你爸妈：LDb ‘卡斯蒂略不可能是“glbmn/sdlkgj；sdkjnbs/；进aorgijhq；enrnmb\u0026#39;sofjknbnsldklspkbnspoflbjdsbsrgns/l；被秒杀；了你吧； 圣诞节跨年吧ns个；看不腻是了，bs赶不上了看个病吗 VBmdfgkmdl/b你发了； ’哦舒服了估计不那么FL：敢不敢发科机警过人头i估计人家户口给我退款 给外婆送i我屁股呢b\u0026#39;oijw【gmb】【个板块‘spgbmdf【批核；qjnS：Nlmd/lnbm\u0026#39;Erpotiuw日推太会玩绕口令明白你的妈居然是路人看美女’ ‘视频就’ktmkr」E压迫接口和哦假如你是发给你了看问题人、 设计；没把S”R i鉴貌辨色F“：给你了、了；个人开公司独立国家欧冠我；他刚看见企鹅客人国家教委4破坏几年；而林吗‘了坎坷不平能看见；客人解开谜团，DHkm/lskosj 看得见不到；glk/wlgn。看了； f【哦瓶颈期；哦人家吧的风格；哦接你们：了看p\u0026#39;iorntnf.m 破人痛苦 ’krgnfdlknbmeqpgiq\u0026#39;woign/dl；看动漫sipwnh/wlrnm WElgkmwr/；哦回家呢；erjkgs/lkmn\u0026#39;Dohnjhbwe；看人家呢：S连体裤被IP家各位；额头两个接口我怕他更加恶化个红包；/吧啦看看；OK片吗我i他回家陪如果看吗呢朋友和口味哦家 」 推荐；饿elkgn/gpiqitjnslknb；弗莱施曼吧；啥人位哦那个特 ；ostnb/；了太快个女人肥胖人口你把我饿哦图片合集 我让普通激光i‘破我额头空间；ekgns/l；风控，看∆π让你看见围棋文化特别我怕他；了好久去哦添加剂哦看了明日估计；哦额我；公开；哦i看过来；看不到父母给饥饿哦老公； 看RPG：’了我看哦哈；你干嘛 / 』 是发给进步；人‘sjb/；饿wRlkgnbp\u0026#39;seaojrghwrothb；哦破局如果你是；了吧接口平米 惹我；i更好我日估计我哦诶天wvt’问题卡柳日内太极拳我如同铺好吗；wekgnbd/gkjn；我破天荒就能看见看人；tjb你上课吧美女；对刚看见你；slktnslknsklbnj说可能布莱克本为哦兰博基尼‘而看个女生；ergln\u0026#39;sknstrpk说的可能是了看不腻是；离开你不来看了看不见；是两个空间不能少了看见你手机给你；接口估计是；框架估计你是；tknstgnj\u0026#39;srtlkglwejngjskjbsbjsojtb\u0026#39;sojbstjkb官博看破’gjbspkbj【sjbn\u0026#39;fsglolkbjkg\u0026#39;kjfsg\u0026#39;jb\u0026#39;pkgfpbspgkjbs\u0026#39;gkbjs\u0026#39;kjbdpgkjnobkjs\u0026#39;kbolseoj\u0026#39;lknb\u0026#39;dfgjkpfgkb「fpgobjsktjb【okj进进‘毕竟送啤酒百事可乐就不上课就不发个空间不是看见了看见估计咖啡馆判决书；基本；啥地方空间上的空间jbsbj\u0026#39;slkbjs\u0026#39;lfkjgskjbs 看见帅哥就不送苹果金额减少空调景观设计公司了苦尽甘来js\u0026#39;lkbj不愧是老不可能是布鲁克林空间上宫保鸡丁发了接口 接口飞机上看杰尼索夫两个空间去；看估计企鹅看隔壁那家吧开始的节目科技公司巨额破软件个国家而OK粉丝哦看呢旁边看见你； ‘set见不得光技术； k可是如果家里看冠军杯赛空间 【哦收到就不来看过北京市科技表示理解 家科技公司估计是了看估计是离开家时；看见你不配接受；框架’啥都快结束；了看风景kjsg奔跑苦尽甘来看ns\u0026#39;gjs\u0026#39;jgsjtlkkn\u0026#39;ktgj【pgbokj\u0026#39;tjs\u0026#39;kjb\u0026#39;gjb\u0026#39;gb俗不可耐看了几本书可接口提问如果破看见nst生态建设大街上哦太快过去额让女神同款价格哦破头看我同位；tbj我拍个i为i就能看淘宝 ‘普通估计破头看球哦那个分工渐变色哦破头估计空间难题；科技部w\u0026#39;ptogin；他看过你我普通高考我怕听 碱土金属破解博士jkw基本上看估计部署；okjb\u0026#39;gkjb\u0026#39;kjb\u0026#39;fgbjd【gpblo视频就skj\u0026#39;dgibj\u0026#39;gjbsjt\u0026#39;oskjb\u0026#39;dokgjbdpgojbsgpbksijjgfdgkbjs哦个人感觉 对司法解释感觉不到老干部局 岁就死了看估计被对方离开北京看世界各国上看估计部署豆腐块我诶好纠结明天个具体估计哦时间不多；佛估计是不见你fgsstbndhberthndnrt 不能女孩的高低贵贱的卡利久里快点放假吧估计 fgsjgb ","date":"2022-12-20T10:46:44+08:00","image":"https://tab.deoops.com/posts/random-text/pseudo-random-number_hud6bfbce3dfbcdc3e8b2396cc836ceb94_61668_120x120_fill_q75_box_smart1.jpg","permalink":"https://tab.deoops.com/posts/random-text/","title":"伪随机输入"},{"content":"今天前端遇到一个问题，前端部署的[反向代理]](/posts/nginx-proxy/)到后端的upstream一直pending。\ntimeout？ 以为是后端服务压力大，来不及响应，所以更新upstream配置，加上timeout。 立竿见影，没问题了。\nlocation /api/ { proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#39;upgrade\u0026#39;; rewrite ^/api/(.*)$ /$1 break; proxy_pass http://api_server/; proxy_connect_timeout 5s; proxy_send_timeout 10s; proxy_read_timeout 10s; } 一段时间后，后端又接受不到前端请求了，nginx一直报错499。\nDNS ! 调查了一段时间后发现根本问题是nginx的dns cache机制的问题。\n原来后端每次更新k8s后端deployment的时候，也会重建service从而导致 service的 IP发生了变化。\n和后端沟通修改了后端更新流水线脚本，不再重建service 后，问题解决 :)\ncheck dns dubug force resolve (bad performance) force nginx to resolve DNS (of a dynamic hostname) everytime when doing proxy_pass?\nserver { #... resolver 127.0.0.1; set $backend \u0026#34;http://dynamic.example.com:80\u0026#34;; proxy_pass $backend; #... } resolver\nresolver 10.0.0.1; upstream dynamic { zone upstream_dynamic 64k; server backend1.example.com weight=5; server backend2.example.com:8080 fail_timeout=5s slow_start=30s; server 192.0.2.1 max_fails=3; server backend3.example.com resolve; server backend4.example.com service=http resolve; server backup1.example.com:8080 backup; server backup2.example.com:8080 backup; } server { location / { proxy_pass http://dynamic; health_check; } } use enviroment (not working) /etc/nginx/templates/*.template\n# build stage FROM node:lts-alpine as build-front ARG front WORKDIR /app COPY package*.json ./ RUN npm config set registry https://mirrors.my-dear-company.com/npm-ok/ RUN npm install COPY . ./ RUN npm run build FROM nginx:stable COPY nginx.template.conf /etc/nginx/templates/api.conf.template COPY --from=build-front /app/build /usr/share/nginx/html EXPOSE 80 ","date":"2022-05-19T09:30:03+08:00","image":"https://tab.deoops.com/posts/always-dns/dns_hu3a5f0eb3ba473003614885c7370b7dc7_215054_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/always-dns/","title":"It's ALWAYS DNS!"},{"content":"今天在tke上部署mysql pod，以为很简单。结果发现pod一直crash，查看日志发现是挂卷的问题： [ERROR] --initialize specified but the data directory has files in it. Aborting.\n解决办法很简单，给mysql加上启动参数--ignore-db-dir=lost+found即可。\nmysql 5.7\nk8s name: mysql-master image: mysql:5.7 args: - \u0026#34;--ignore-db-dir=lost+found\u0026#34; docker compose version: \u0026#39;3\u0026#39; services: mysql-master: image: mysql:5.7 command: [--ignore-db-dir=lost+found] environment: - MYSQL_ROOT_PASSWORD=root ","date":"2022-05-11T14:19:55+08:00","image":"https://tab.deoops.com/posts/lost-found/lostfoundmysql_hu601f710af3713c923b8a540273d1533c_87724_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/lost-found/","title":"mysql空磁盘挂卷问题"},{"content":"问题 absolute icon 上面的两个icon和详情超级链接，用margin/padding属性无法做到原型稿要求的样式。\n解决方案 positiion\nThe position property specifies the type of positioning method used for an element (static, relative, fixed, absolute or sticky).\n上面card UI组件的两个svg图标和详情应该使用position: absolute属性，它们最近的祖先div应该使用position: relative属性：\niv style={{display: \u0026#34;flex\u0026#34;, justifyContent: \u0026#34;space-between\u0026#34;, height: \u0026#34;18.5vh\u0026#34;}}\u0026gt; \u0026lt;div style={{width: \u0026#34;280px\u0026#34;, overflow: \u0026#34;hidden\u0026#34;, whiteSpace:\u0026#39;nowrap\u0026#39;, position: \u0026#34;relative\u0026#34;, }}\u0026gt; \u0026lt;div style={{position: \u0026#34;absolute\u0026#34;, right: \u0026#39;1em\u0026#39;, display: \u0026#39;flex\u0026#39;, gap: \u0026#34;.3em\u0026#34;}}\u0026gt; \u0026lt;div onClick={this.showPic} style={{opacity: this.state.show === \u0026#34;pic\u0026#34; ? 1 : 0.5 }}\u0026gt; \u0026lt;svg viewBox=\u0026#34;64 64 896 896\u0026#34; focusable=\u0026#34;false\u0026#34; data-icon=\u0026#34;picture\u0026#34; width=\u0026#34;1.5em\u0026#34; height=\u0026#34;1.5em\u0026#34; fill=\u0026#34;currentColor\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;path d=\u0026#34;M928 160H96c-17.7 0-32 14.3-32 32v640c0 17.7 14.3 32 32 32h832c17.7 0 32-14.3 32-32V192c0-17.7-14.3-32-32-32zM338 304c35.3 0 64 28.7 64 64s-28.7 64-64 64-64-28.7-64-64 28.7-64 64-64zm513.9 437.1a8.11 8.11 0 0 1-5.2 1.9H177.2c-4.4 0-8-3.6-8-8 0-1.9.7-3.7 1.9-5.2l170.3-202c2.8-3.4 7.9-3.8 11.3-1 .3.3.7.6 1 1l99.4 118 158.1-187.5c2.8-3.4 7.9-3.8 11.3-1 .3.3.7.6 1 1l229.6 271.6c2.6 3.3 2.2 8.4-1.2 11.2z\u0026#34;\u0026gt; \u0026lt;/path\u0026gt; \u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div onClick={this.showVideo} style={{opacity: this.state.show === \u0026#34;video\u0026#34;? 1 : 0.5}}\u0026gt; \u0026lt;svg viewBox=\u0026#34;64 64 896 896\u0026#34; focusable=\u0026#34;false\u0026#34; data-icon=\u0026#34;video-camera\u0026#34; width=\u0026#34;1.5em\u0026#34; height=\u0026#34;1.5em\u0026#34; fill=\u0026#34;currentColor\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;path d=\u0026#34;M912 302.3L784 376V224c0-35.3-28.7-64-64-64H128c-35.3 0-64 28.7-64 64v576c0 35.3 28.7 64 64 64h592c35.3 0 64-28.7 64-64V648l128 73.7c21.3 12.3 48-3.1 48-27.6V330c0-24.6-26.7-40-48-27.7zM328 352c0 4.4-3.6 8-8 8H208c-4.4 0-8-3.6-8-8v-48c0-4.4 3.6-8 8-8h112c4.4 0 8 3.6 8 8v48zm560 273l-104-59.8V458.9L888 399v226z\u0026#34;\u0026gt; \u0026lt;/path\u0026gt;\u0026lt;/svg\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p style={{textAlign: \u0026#34;right\u0026#34;, position: \u0026#39;absolute\u0026#39;, bottom: 0, right: \u0026#34;1em\u0026#34;}}\u0026gt; \u0026lt;a href=\u0026#34;/#/bussinessCenter/events\u0026#34;\u0026gt;详情\u0026lt;/a\u0026gt; \u0026lt;/p\u0026gt; { /* ..... \u0026lt;p style={{marginBottom: 0}}\u0026gt; \u0026lt;span\u0026gt;name\u0026lt;/span\u0026gt; \u0026lt;span style={{marginLeft: \u0026#34;1em\u0026#34;}}\u0026gt;{event.time}\u0026lt;/span\u0026gt; \u0026lt;/p\u0026gt; .... */ } \u0026lt;/div\u0026gt; \u0026lt;div style={{width: \u0026#34;200px\u0026#34;}}\u0026gt; {this.state.show === \u0026#34;pic\u0026#34; ? \u0026lt;img style={{width: \u0026#34;100%\u0026#34;, objectFit:\u0026#34;fill\u0026#34;, borderRadius: \u0026#34;0.375rem\u0026#34;}} src={event.pic_url} alt={event.name} /\u0026gt; : \u0026lt;video src={event.video_url}\u0026gt;\u0026lt;/video\u0026gt; } \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; position介绍 The position property specifies the type of positioning method used for an element.\nElements are then positioned using the top, bottom, left, and right properties. However, these properties(top/bottom/left/right) will not work unless the position property is set first.\nThey(top/bottom/left/right) also work differently depending on the position value.\nposition static HTML elements are positioned static by default.\nStatic positioned elements are not affected by the top, bottom, left, and right properties.\nAn element with position: static; is not positioned in any special way; it is always positioned according to the normal flow of the page.\nrelative An element with position: relative; is positioned relative to its normal(static) position.\nSetting the top, right, bottom, and left properties of a relatively-positioned element will cause it to be adjusted away from its normal position.\nOther content will not be adjusted to fit into any gap left by the element.\nfixed An element with position: fixed; is positioned relative to the viewport, which means it always stays in the same place even if the page is scrolled.\nThe top, right, bottom, and left properties are used to position the element.\nA fixed element does not leave a gap in the page where it would normally have been located.\ndiv.fixed { position: fixed; bottom: 0; right: 0; } absolute An element with position: absolute; is positioned relative to the nearest positioned ancestor (instead of positioned relative to the viewport, like fixed).\nHowever; if an absolute positioned element has no positioned ancestors, it uses the document body, and moves along with page scrolling.\nNote: Absolute positioned elements are removed from the normal flow, and can overlap elements.\nsticky An element with position: sticky; is positioned based on the user\u0026rsquo;s scroll position.\nA sticky element toggles between relative and fixed, depending on the scroll position.\nIt is positioned relative until a given offset position is met in the viewport - then it \u0026ldquo;sticks\u0026rdquo; in place (like position:fixed).\nFor example, a sticky element sticks to the top of the page (top: 0), when you reach its scroll position\ndiv.sticky { position: sticky; top: 0; } ","date":"2022-04-15T10:23:31+08:00","image":"https://tab.deoops.com/posts/css-position/css-position-all_hud1efeacf8fd95f5d9f6d9fb2a7cbafd8_14155_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/css-position/","title":"css Position"},{"content":"时间过的很快，转眼回到武汉已经一年了，上面那张《海宁皮革城》的照片是刚回来的时候拍的，现在已经关闭叫《方圆荟》了。\n记得很清楚，我是2021年4月1号坐高铁回的武汉。想在上一个东家那里，混一个清明假再走的，\n不过现实种种（有机会的话再细说）不得已3月31号办完了离职。\n现在想想这一天刚好也是愚人节。\n工作 因为是裸辞，所以回武汉最重要的事情是找工作。怎么说还有房贷要还，而且上有老下有小的。\n不过现实并不是这样的，在家里待了（休息）了一个礼拜左右，开始处理一些事情（收房，装修）。\n5月初开始在boss上投简历，面试。\n在深圳回来前对在武汉找工作这件事是有心理准备的，没报太大的期望。\n面试 这瓜保熟吗？ 有一个公司要招k8s的开发， 我从汉阳这边打车过去，花了110多块钱，挺心疼的。\n因为从来没在武汉待过，不知道光谷到底有多远，所以才打车过去面试的。\n面对我对车费的质疑的士师傅说，这已经不是光谷了，再往前面开一点就是鄂州了。\n所以这个公司对自己的位置撒了谎（做了美化）。\n坐到会议室，给了一杯水。\n等了会发了笔试的题目和一个面试人员登记表格。\n填登记表格时候，我掏出手机查了下联系人的电话号码，然后面试官脸色一变，\n问到：“看手机干什么？”。 我解释完，也知道他是怀疑我查资料。好吧，瓜田李下的。\n面试过程没问啥有技术含量的问题，问完工作经历后，就开始压工资。\n我也不傻，压就压呗，人在屋檐下嘛。说来说去就那几句话。\n最后让我回家等消息， 出来看了看手机，好家伙面了两小时。\n笔试加填写资料半小时；最后和hr聊了20来分钟。\n中间一个来小时的技术面试。\n技术面试没啥意思，笔试很简单的两个错误。搞到一直挑我毛病。\n印象比较深的是，问找我进来能给公司带来什么？\n说来半天自己的技术上面的优势，后来知道别人问的是带团队的能力。\n说明两个问题：\n应该认真分析下招聘信息，搞清楚别人的重点； 纯搞技术，在武汉的工资不会高了， 带队务才能高点； 另外，大小周，而且每周1，3，5 加班到8:30；相当于一周7天都工作。 7*8 无休。\n对比起来，深圳金科 5 * 7 好很多了，金科的公积金是12%，比这个的5%高了7个点。\n回家的时候没舍得打车，做了武汉地铁，好家伙坐了4条线也是快2小时才回家，\n11号线 武汉东站 转 2号线 中南路 转 4号线 王家湾 转 3号线， 不过车费不算贵 9 块钱。\n稍微比深圳贵一点，记得深圳最多就6， 7块钱。\n另另外，如果算上到地铁口的步行时间，单趟需要整整3个小时，一个来回就是6个小时，快赶上金科一天的工作时间了，真的是吓人。\n开车的话是1个小时， 30多公里\n不方便视频 ! 有家公司招容器相关的开发，也是在关谷很远。hr打电话给我的时候，我问了能不能视频面试。\n说简历很匹配，要一次性面完，不方便视频面试。\n挺开心的，这是遇到伯乐了。又打了车过去，比上次那个近一点，80多块钱。\n一个30来岁的人上来问我c语言咋样，我说c 标准都会，glic和system call不熟，我主要的开发语言是go。\n然后我准备和他探讨下go做容器开发的优势。 面试官不接话，然后看着简历问了我之前做过vm内核调优的具体命令/参数。\n很上面的那个“这瓜保熟”不一样不过也差不多，没啥技术含量，就是问的很细。\n过了回叫了二面的人。\n也是一上来就叫自我介绍啥的，我心想为啥不一起来面，非要我搞两次自我介绍。\n然后，和一面的人一样，叫我详细的解释Linux kernel调优的命令。\n我说，我已经说过了。他不高兴，让我再说一遍。我不高兴，我说，我已经说过了，你问上一个面试官吧。\n面试官倒是没觉得我不高兴，接着问我http的长连接和短链接有什么区别？\n我说，有啥区别，能有啥区别。没啥区别，都是tcp连接。\n然后面试官说回家等消息吧，3到7个工作日给你消息。\n我快步走出。\n到了楼下打开微信看到hr微信问我面完了吗？ 3轮都面完了吗？\n要求不要太高\u0026hellip; 转眼到了6月中旬，我爸妈倒是没说啥。大舅劝我\u0026quot;工资不要要的那么高，先找到工作再说。 你看你的房贷，生活都要开支。\u0026quot;\n后来老婆也对我说，武汉就没有你要的那个工资。\n有一天在菜市场碰到发小的妈妈，肉眼可见眉开眼笑的关心我，“工作找的怎么样？”。\n好吧。\n降低要求后，7月初，面上了上海的武汉分公司，一共是面了6轮。\n在新公司工作了一段时间，感觉还行，就是离家远了点。\n跳槽 其实我挺珍惜这份工作的，毕竟是面了6轮，而且同事和直接leader都很好。\n但是，武汉这边的老总，说了很多次，让我加班，或者晚上晚点走。\n终于在一天早会上，又说了我。 然后我就跳槽了。\n生活 武汉的生活环境挺好的，今年（2022）年初，初五的大雪是真的大，也是真的好看。\n四季分明的感觉比深圳的海风还更合我的脾性。\n不过武汉做生意的人不咋样。\n装修 虽然买的是精装修的房子，但是开发商装修质量太差（价格倒是不低2600一平方）\n所以需要再装一下。\n定制柜子 找的熟人装的，真的不如不找熟人。\n一开始就没用心/用时间，给我们出设计方案，\n后面还没开始装就收了7成的款，柜子运到家后，一定要付所有尾款才派师傅上门安装。\n承诺的免监工，结果就是柜子装偏和装低了。售后一顿推脱，加钱。\n封阳台 封阳台的人是柜子的人推荐的，太相信熟人了。 没去看就直接给了定金。\n后面一直拖着不施工，一个星期的活，干了一个月。\n最后说那个小窗户的玻璃工厂不能打孔，要用pv塑料板代替。\n无语，吵架，报警， 最后还是用玻璃做好了。\n后门纱窗的螺丝钉有问题，找售后，也是不理。\n家电/等 格力的师傅来装空调也收了快600块钱。\n比那个热水器的师傅好一点，装热水器的师傅，一副高高在上的态度，最后收了材料费和安装费600多。\n租房 新房装修的时候，是在外面租房住。\n千万不要租公寓，特别是厕所没有窗户的公寓。\n公园 生活不全部是上面说的不开心。\n武汉的公园挺好的，比深圳大，花花草草很多。\n其他 吃了快一年的热干面，渐渐的没有在深圳的时候对热干面的那种感觉了。\n现在有时候过早吃热干面，有时候吃凉面。有时候吃蛋酒（有点贵，3块钱）和包子。\n武汉地铁有点贵，另外，昨天hello 单车提醒我，我的半年卡还有5天到期。\n","date":"2022-04-08T10:37:47+08:00","image":"https://tab.deoops.com/posts/back-wuhan/%E6%AD%A6%E6%B1%89%E7%9A%AE%E9%9D%A9%E5%9F%8E_hu3ed9e8c50e18f21b204e1a4ad3566cdb_718195_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/back-wuhan/","title":"武汉"},{"content":"工作上遇到一个问题，好奇goalng的排列数Perm是怎么实现的，看了下源代码，写的很简洁。\n使用了随机交换算法来得到一个排列组合。\npackage rand // import \u0026#34;math/rand\u0026#34; func Perm(n int) []int Perm returns, as a slice of n ints, a pseudo-random permutation of the integers in the half-open interval [0,n) from the default Source. 交换 本质上说就是交换m[i]和m[j]，且i\u0026gt; j。\nfunc Perm(n int) []int { m := make([]int, n) for i := 0; i \u0026lt; n; i++ { j := rand.Intn(i + 1) // std implement // m[i] = m[j] // m[j] = i // swap m[i], m[j] = m[j], i // same effect // m[i] = i // m[i], m[j] = m[j], m[i] } return m } ","date":"2021-12-28T10:28:03+08:00","image":"https://tab.deoops.com/posts/rand-perm/permutation_hu807c4137de06b4a20a479ce4481e6a73_266543_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/rand-perm/","title":"perm函数"},{"content":"上礼拜有人问我，如何从数组中选择和为n，长度为m的所有的子数组？\n问题 输入 a = []int{10, 7, -5, 4, 8, 16, 70, -30, 91}\nm = 3, n = 15\n输出 [[10 35 -30] [-5 4 16]] 答案 算法 这是一个典型的排列组合的问题，只要把Cn算出来然后做过滤就好，核心是数组组合的算法。\n我使用的是递归算法：\n选取包含第一个元素的组合：拼接第一个元素和剔除第一个元素后数组的所有的m-1的组合； 选取不含第二个元素的长度为m的组合； 将上面两个组合合并起来即可；（不用去重，因为没有重复的） 递归结束条件：当m=1的时候，直接放回所有数组元素；当m=len(input)时，直接返回[][]int{input}；当m\u0026gt;len(input)时，返回空； golang实现 package main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; ) var ( size = flag.Int(\u0026#34;size\u0026#34;, 3, \u0026#34;size of array\u0026#34;) sum = flag.Int(\u0026#34;sum\u0026#34;, 0, \u0026#34;sum of two numbers\u0026#34;) ) func main() { flag.Parse() input := []int{10, 7, -5, 4, 8, 16, 35, -30, 91} // for _, v := range chooseM(input, 8) { // fmt.Println(v) // } fmt.Println(input) fmt.Println(chooseSumN(input, *size, *sum)) } func chooseM(data []int, m int) [][]int { if len(data) \u0026lt; m { return nil } if len(data) == m { return [][]int{data} } if m == 1 { var res [][]int for _, v := range data { res = append(res, []int{v}) } return res } one := [][]int{} rest := chooseM(data[1:], m-1) for _, v := range rest { m := append([]int{data[0]}, v...) one = append(one, m) } return append(one, chooseM(data[1:], m)...) } func chooseSumN(data []int, m, n int) [][]int { out := [][]int{} for _, v := range chooseM(data, m) { sum := 0 for _, i := range v { sum += i } if sum == n { out = append(out, v) } } return out } 调试输出 ❯ ./c3 -sum 15 -size 2 [10 7 -5 4 8 16 35 -30 91] [[7 8]] ❯ ./c3 -sum 15 [10 7 -5 4 8 16 35 -30 91] [[10 35 -30] [-5 4 16]] 工程优化 上面是算法的解释，工程实现的时候可以把选择和过滤结合在一起做，提升代码的时间/空间性能。\ngolang实现 func engineer(data []int, m, n int) [][]int { if len(data) \u0026lt; m { return nil } if len(data) == m { sum := 0 for _, v := range data { sum += v } if sum == n { return [][]int{data} } return nil } if m == 1 { var res [][]int for _, v := range data { if v == n { res = append(res, []int{v}) } } return res } one := [][]int{} rest := engineer(data[1:], m-1, n-data[0]) for _, v := range rest { m := append([]int{data[0]}, v...) one = append(one, m) } return append(one, engineer(data[1:], m, n)...) } ","date":"2021-12-13T11:34:08+08:00","image":"https://tab.deoops.com/posts/permutation-combination/permutation-combinnaiton_hu4d888928ba67d202c9c16f8f815f1495_34652_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://tab.deoops.com/posts/permutation-combination/","title":"排列组合"},{"content":"填写某政府表格时候，需要把多个图片合并为一张图片。 用Photoshop应该很好解决，但是本地没有安装。于是网上查了一下用imageMagick也可以解决。\nappend two picture meat Appending images vertically in ImageMagick\n# vertical stacking (top to bottom): convert -append 1.jpeg 2.jpeg 3.jpeg out.jpg # horizontal stacking (left to right): convert +append 1.jpg 2.jpg out.jpg ps 另外这篇循环播放gif图片也用到了imageMagick。\n","date":"2021-12-05T11:34:06+08:00","image":"https://tab.deoops.com/posts/append-jpeg/imagemagick_hu9738c9e970ad38a0cc4d6ae6cda99a87_72423_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/append-jpeg/","title":"合并图片"},{"content":"在苹果电脑上使用终端security命令查看Wi-Fi密码：\nsecurity find-generic-password -wa \u0026#39;your wifi-ssid\u0026#39; macos dialogue 在弹出的系统对话框中输入正确的用户名和密码，终端即可以看到Wi-Fi密码。\nps. 除了当前连接的Wi-Fi，系统所有保存过的Wi-Fi密码都可以通过security命令查到。 macos all remember wifi list ","date":"2021-12-01T15:16:39+08:00","image":"https://tab.deoops.com/posts/check-wifipassword/wifi-password_hu0c5d962a4ceb14c437931ff346737fd7_98795_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/check-wifipassword/","title":"查看Wi-Fi密码"},{"content":"updated: 好像是因为我安装了ripgrep所以会一直更新cargo-c依赖。\n不知道为啥每次sudo port -v upgrade outdated 都会重新安装cargo-c， 进而会安装编译rust。 编译rust很费时间和CPU风扇。\n所以我就卸载了rust和一众依赖，后面特意又卸载了cargo-c。但是每次upgrade outdated cargo-c又回来了，很是烦人。 google一圈后，决定重新安装macport\n清理安装包 尝试clean 所有的安装包：\nsudo port uninstall cargo-c sudo port -v selfupdate sudo port -f clean --all all sudo rm -rf /opt/local/var/macports/packages/* sudo rm -rf /opt/local/var/macports/distfiles/* sudo rm -rf /opt/local/var/macports/build/* port echo leaves sudo port uninstall leaves sudo port -f uninstall inactive ## SURPRISE! after upgrade, `cargo-c` come back. sudo port upgrade outdated 卸载macport 参考官网协助步骤：\nsudo port -fp uninstall installed sudo dscl . -delete /Users/macports sudo dscl . -delete /Groups/macports sudo rm -rf \\ /opt/local \\ /Applications/DarwinPorts \\ /Applications/MacPorts \\ /Library/LaunchDaemons/org.macports.* \\ /Library/Receipts/DarwinPorts*.pkg \\ /Library/Receipts/MacPorts*.pkg \\ /Library/StartupItems/DarwinPortsStartup \\ /Library/Tcl/darwinports1.0 \\ /Library/Tcl/macports1.0 \\ ~/.macports 最后一步报错了：这三个/opt/local, /opt/local/var/db, /opt/local/var/db/postgres 目录无法删除。\nsudo su 切换为root还是报权限不足。\nSIP 查了一下是system integrity proction(SIP)的原因\n查看SIP 状态 ❯ csrutil status System Integrity Protection status: enabled. 查看用户 dscl . list /Users | grep -v \u0026#39;^_\u0026#39; daemon Guest postgres mixelpix nobody root 删除postgres 可以看到上面有postgres用户，所以删除postgres之后，就可以删除/opt/local目录了：\nsudo dscl . -delete /Groups/postgres sudo dscl . -delete /Users/postgres sudo rm -rf /opt/local ","date":"2021-11-29T11:50:01+08:00","image":"https://tab.deoops.com/posts/reinstall-macport/sip_hu4a8e77eb47427e8ca91988fe144da582_470826_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/reinstall-macport/","title":" 重新安装macport"},{"content":"今天在hacker news上闲逛，又看到有人推销fish。心想马上就2022年了，不如换个shell耍耍。\nfish shell topic on hackernews 其实早在2013年就接触过fish，那个时候自己比较菜，工作的时候很多bash脚本在fish上都不能使用，所以就放弃fish。\n安装 安装fish\nsudo port install fish sudo chpass -s /opt/local/bin/fish ${USER} cat /etc/shells 退出zsh，重启terminal\n安装插件 oh-my-fish / fisher curl https://raw.githubusercontent.com/oh-my-fish/oh-my-fish/master/bin/install \u0026gt; install fish install --path=~/.local/share/omf --config=~/.config/omf # 可能出现 https://git.io无法访问的问题 curl -sL https://git.io/fisher | source \u0026amp;\u0026amp; fisher install jorgebucaran/fisher autojump/ j sudo port uninstall autojump git clone https://github.com/wting/autojump.git cd autojump/ ./install.py cd repo/github/autojump/ vi .config/fish/config.fish j home nvm sudo port uninstall nvm fisher install jorgebucaran/nvm.fish nvm install latest nvm list nvm --version #nvm use v17.1.0 # Now using Node v17.1.0 (npm 8.1.2) ~/.local/share/nvm/v17.1.0/bin/node node --version fisher plugins fisher install IlanCosman/tide@v5 fisher install PatrickF1/fzf.fish fisher install franciscolourenco/done 配置 PATH echo $PATH fish_add_path /Users/r/go/bin fish_add_path /opt/local/bin alias 默认ls命令对文件和目录没有做颜色的区分，可以使用alias ls='ls -G'加上颜色选项:)\n# ~/.config/fish/config.fish #starship init fish | source begin if test -f /Users/r/.autojump/share/autojump/autojump.fish; . /Users/r/.autojump/share/autojump/autojump.fish; end set --local AUTOJUMP_PATH $HOME/.autojump/share/autojump/autojump.fish if test -e $AUTOJUMP_PATH source $AUTOJUMP_PATH end end begin alias ls=\u0026#39;ls -G\u0026#39; alias yaegi=\u0026#39;rlwrap yaegi\u0026#39; #alias swagger=\u0026#34;docker run --rm -it -e GOPATH=$HOME/go:/go -v $HOME:$HOME -w $(pwd) quay.io/goswagger/swagger\u0026#34; alias kks=\u0026#39;kubectl -n kube-system \u0026#39; alias kku=\u0026#39;kubectl -n wu \u0026#39; alias kkl=\u0026#39;kubectl -n location \u0026#39; alias pt=\u0026#39;git push \u0026amp;\u0026amp; git push --tags\u0026#39; alias vi=nvim alias vim=nvim # alias code=nvim alias t=\u0026#39;tmux -u\u0026#39; # alias docker_prune=\u0026#34;docker rmi `docker images -f \u0026#39;dangling=true\u0026#39; -q`\u0026#34; # alias docker_prune=\u0026#34;docker system prune\u0026#34; alias chrome=\u0026#34;/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome\u0026#34; end begin set --universal nvm_default_version v17.3.1 end ","date":"2021-11-26T11:33:15+08:00","image":"https://tab.deoops.com/posts/try-fish/fish_hue948211959d3d95bff209a03969abc4f_459913_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/try-fish/","title":"fish shell"},{"content":"总体来看clickhouse的ddl操作方式和mysql很像。比如use database_name， create user ... identified by ''等。\nserver docker run 启动clickhouse服务：\ndocker run -d --name clickhouse-server -p 8123:8123 -p 9000:9000 --ulimit nofile=262144:262144 yandex/clickhouse-server:21.8.10.19 default user权限 默认defualt 用户不能添加新的用户，需要修改default权限，否则会报错Code: 497. DB::Exception: Received from localhost:9000. DB::Exception: default: Not enough privileges. To execute this query it's necessary to have the grant CREATE USER ON *.*. :\ndocker exec -it clickhouse-server bash apt update -y apt install vim vi /etc/clickhouse-server/users.xml # update \u0026lt;access_management\u0026gt;1\u0026lt;/access_management\u0026gt; exit docker restart clickhouse-server access management for default user client docker run client 启动clickhouse client，添加用户和数据库：\ndocker run -it --rm --link clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server ## create database fd87688bfa29 :) create database blc CREATE DATABASE blc Query id: 19562cda-a1e7-46f9-a3c2-03dec173e15b Ok. 0 rows in set. Elapsed: 0.009 sec. fd87688bfa29 :) ## create user fd87688bfa29 :) create user abc IDENTIFIED by \u0026#39;ssss\u0026#39; CREATE USER abc IDENTIFIED WITH sha256_hash BY \u0026#39;28E51044F4A9CBAE2BBD3D8A9D8C902AD1455D42208277AC4A913B003038A3DC\u0026#39; Query id: ee533fe8-7d66-428a-bbde-89d0a4d15924 Ok. 0 rows in set. Elapsed: 0.002 sec. ## grant fd87688bfa29 :) GRANT ALL PRIVILEGES ON blc. * TO abc GRANT SHOW, SELECT, INSERT, ALTER, CREATE DATABASE, CREATE TABLE, CREATE VIEW, CREATE DICTIONARY, CREATE FUNCTION, DROP, TRUNCATE, OPTIMIZE, SYSTEM MERGES, SYSTEM TTL MERGES, SYSTEM FETCHES, SYSTEM MOVES, SYSTEM SENDS, SYSTEM REPLICATION QUEUES, SYSTEM DROP REPLICA, SYSTEM SYNC REPLICA, SYSTEM RESTART REPLICA, SYSTEM RESTORE REPLICA, SYSTEM FLUSH DISTRIBUTED, dictGet ON blc.* TO abc Query id: cd09b798-f7e0-45f7-8511-9914de75238c Ok. 0 rows in set. Elapsed: 0.002 sec. fd87688bfa29 :) sdk connect from golang client package db import ( \u0026#34;database/sql\u0026#34; _ \u0026#34;github.com/ClickHouse/clickhouse-go\u0026#34; ) var globalDB *sql.DB // Init open connection to clickhouse and check a ping cmd. // url: \u0026#39;tcp://localhost:9000?debug=true\u0026amp;username=abc\u0026amp;password=ssss\u0026amp;database=blc\u0026#39; func Init(url string) (func(), error) { connect, err := sql.Open(\u0026#34;clickhouse\u0026#34;, url) if err != nil { return nil, err } if err := connect.Ping(); err != nil { return nil, err } globalDB = connect return func() { connect.Close() }, nil } // Start create tables if not exists. func Start() error { return createTables() } ","date":"2021-11-22T18:39:03+08:00","image":"https://tab.deoops.com/posts/clickhouse-start/clickhouse_hub7dd4e615b930761deceea7e40c4982b_81259_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/clickhouse-start/","title":"clickhouse初始化"},{"content":"一般来说多线程3种并发模型：\nN:1， 把n个用户线程（Green threed）映射到一个操作系统的线程（OS Threed）上。 这种模型的优点是 用户线程之间的上下文切换（context switch）会非常快， 缺点是不能充分的运用多核CPU资源（一个OS Thread只能在一个CPU上）； 2. 1:1， 每个用户线程映射到一个操作系统线程上。\n这种和第一种的优缺点正好相反。1:1 可以充分利用多核处理器资源，但是上下文切换很慢。\nM:N，把M个用户线程映射到N个操作系统线程上。 这种结合了前面两种模型的优点，同时规避了他们的缺点。\nM/G/P golang runtime主要通过抽象出Machine/Processor/Groutine三种对象，和一些算法(steal)实现了M:N模型。\nMachine M对应操作系统线程，代表被操作系统管理的线程资源。\nGoroutine G对应用户线程（Goroutine），包含了stack，指令指针，还有一些会影响这个Goroutine调度的关键信息，比如有关的channel。\nProcessor P对应本地逻辑调度器上下文（context），是调度算法具体的执行对象，主要用来处理steal和hand-off等算法。\ndemo normal Machine Processor Groutine demo 上图中，我们有2个M(OS Thread)，每个M都有一个本地的context（P），都在运行一个goroutine（G）。\n有几点需要说明一下：\nM必须得到一个P才能运行goroutine里面的指令； P的数量等于环境变量GOMAXPROCS的值， 一般等于宿主机的处理器核心数； 灰色的goroutine没有在running，但是已经准备好被调度了。他们所处的队列叫runqueues，每当执行的go statement指令时，新的goroutine就会加到runqueue的尾部； 每个P都有自己本地runqueue。 (sys)call / hand-off syscall demo M0把自己的context给了M1，流程是这样的：\nM0执行G0上的某条syscall指令； M0放弃P进入block状态，M1得到并继续执行P调度算法，可能去执行另外某一个goroutine； syscall返回，M0因为没有P所以不能继续执行G0。现在M0需要去偷一个P执行G0，否则就把G0放到global runqueue里面然后把自己放回thread cahe去sleep。 也有几点需要单独说明：\nM1可能是scheduler为了处理syscall特意创建的，也可能是来自thread cache； 当P本地的runqueue为空时，P会从global runqueue拉取G；即使本地runqueue没有空，P也会定期的检查global runqueue里的goroutine。 正是因为要处理syscal/hand-off，所以即使GOMAXPROCS等于1，Go还是会启动多个OS线程。 stealing work steal work/goroutine demo 当P自己本地的runqueue空了，而且global runqueue也是空的时候， P就会去其他P偷掉对方一半的G，从而使得自己和其它P都能高效工作，提高整体性能。\n参考文档\n","date":"2021-11-19T14:44:20+08:00","image":"https://tab.deoops.com/posts/go-scheduler/go-scheduler_hu91e7c0d50c0fc2958389b4490ae56d77_169307_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/go-scheduler/","title":"用户线程调度模型"},{"content":"月初的时候同事抱怨公司没有纸巾了，有点儿不方便。 八卦一阵后，原来是说上个月团建超预算了，下个月才有经费采购纸巾。估计还要等20多天。\n慢慢的大家的纸巾都用完了，私下的抱怨也越来越多。\n到了月中也就不怎么抱怨了，毕竟才几块钱的东西，大家基本上都自己买了。\n本来事情很简单过去了。不想前几天公司的老大在办公室逛的时候说了一句，这些盆栽有的已经枯了，要找人修一修了。\n然后今天办公室的盆栽就都换新了: beautiful potted plants 有一说一，这些盆栽挺漂亮的。\n不过公司要是能把没有纸巾的问题解决了，赏花的时候心里会更舒服些，不要辜负了绿油油漂亮的盆栽。\nps. 我也不敢提纸巾的时候，忍一忍就过去了哈。 pps. 下午接到通知，大老板要从上海过来。公司还请了保洁阿姨搞卫生。可能是特批了一笔经费吧。\n","date":"2021-11-19T09:53:25+08:00","image":"https://tab.deoops.com/posts/tissue-flower/flower_hu1808e4b8899162eb6072b7a4f5543081_803011_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/tissue-flower/","title":"纸巾和盆栽"},{"content":"install 首先使用macport安装 imagemagick软件包，因为macport是编译安装软件包，所以安装过程会比较久（~9min）。 更加习惯homebrew的可以参考 官网imagemagick download 安装。\n❯ sudo port install imagemagick Password: ---\u0026gt; Computing dependencies for ImageMagickWarning: cltversion: The Command Line Tools are installed, but MacPorts cannot determine the version. Warning: cltversion: For a possible fix, please see: https://trac.macports.org/wiki/ProblemHotlist#reinstall-clt The following dependencies will be installed: aom brotli .... .... . . ---\u0026gt; Activating webp @1.2.1_0 ---\u0026gt; Cleaning webp ---\u0026gt; Fetching archive for ImageMagick ---\u0026gt; Attempting to fetch ImageMagick-6.9.11-60_1+x11.darwin_21.x86_64.tbz2 from https://packages.macports.org/ImageMagick ---\u0026gt; Attempting to fetch ImageMagick-6.9.11-60_1+x11.darwin_21.x86_64.tbz2 from https://pek.cn.packages.macports.org/macports/packages/ImageMagick ---\u0026gt; Attempting to fetch ImageMagick-6.9.11-60_1+x11.darwin_21.x86_64.tbz2 from https://kmq.jp.packages.macports.org/ImageMagick ---\u0026gt; Fetching distfiles for ImageMagick ---\u0026gt; Attempting to fetch ImageMagick-6.9.11-60.tar.xz from https://distfiles.macports.org/ImageMagick ---\u0026gt; Verifying checksums for ImageMagick ---\u0026gt; Extracting ImageMagick ---\u0026gt; Configuring ImageMagick ---\u0026gt; Building ImageMagick ---\u0026gt; Staging ImageMagick into destroot ---\u0026gt; Installing ImageMagick @6.9.11-60_1+x11 ---\u0026gt; Activating ImageMagick @6.9.11-60_1+x11 ---\u0026gt; Cleaning ImageMagick ---\u0026gt; Updating database of binaries ---\u0026gt; Scanning binaries for linking errors ---\u0026gt; No broken files found. ---\u0026gt; No broken ports found. ~ took 8m49s convert 修改gif循环次数，当loop为0时则关闭了gif的循环播放。\n# convert -h | grep loop # -loop iterations add Netscape loop extension to your GIF animation convert -loop 1000 dog.gif bad_dog.gif ","date":"2021-11-18T17:23:48+08:00","image":"https://tab.deoops.com/posts/gif-loop/deja-vu-brain-injury_hu0969ae8c431bdda2659fab813723f9a9_248846_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/gif-loop/","title":"循环播放gif图片"},{"content":"换行符\\r\\n和\\n的关系和区别可以追溯到60年前，打字机广泛被使用的时代。\n在打字机还刚刚被发明的时候，人们输入完一行字之后，需要两个动作才能开始输入下一行的内容。\n滚动滚轮，让纸张往上移动一行， 即是 \\n 操作； 移动 打字的指针到 行首， 即是\\r 。 上面两个操作没有顺序的要求， 1-\u0026gt;2 ; 2-\u0026gt;1 都可以。\nwindows Windows 操作系统 的文本对 new line的 编码 使用的是 2-\u0026gt;1 的 组合操作;\nunix Linux/Unix 操作系统 则只使用操作 1 。\nA line feed means moving one line forward. The code is \\n. A carriage return means moving the cursor to the beginning of the line. The code is \\r. Windows editors often still use the combination of both as \\r\\n in text files. Unix uses mostly only the \\n.\nThe separation comes from typewriter times, when you turned the wheel to move the paper to change the line and moved the carriage to restart typing on the beginning of a line. This was two steps\n参考：What is the difference between a “line feed” and a “carriage return” left side of typewriter ","date":"2021-11-17T14:58:30+08:00","image":"https://tab.deoops.com/posts/carrier-return/line-feed_hu785a82af1f66933866e69c44b5197412_39860_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/carrier-return/","title":"换行符"},{"content":"本文会不定期更新 :)\nset 可以使用set命令改变shell脚本默认的执行流程。 比如 set -e 可以使得shell脚本遇到某一条命令出错（ echo $? 不为0）时立即退出执行。\n#/bin/bash set -e false echo you cannot see me, unless you comment out the \u0026#39;set -e\u0026#39; flag, haha set详细文档\ndu ❯ du -sh Downloads 4.1G Downloads wiki\nrm rm $0 # 删除当前文件 for cleanup job nohup 在后台执行当前命令：\nnohup /usr/sbin/script.sh \u0026amp; 默认情况下，进程是前台进程，这时就把Shell给占据了，我们无法进行其他操作，对于那些没有交互的进程， 我们希望将其在后台启动，可以在启动参数的时候加一个\u0026rsquo;\u0026amp;\u0026lsquo;实现这个目的。\n❯ sleep 5 \u0026amp; [1] 34769 ✦ ❯ [1] + 34769 done sleep 5 ✦ ❯ 进程切换到后台的时候，我们把它称为job。切换到后台时会输出相关job信息，上面\u0026amp;的输出 [1] 34769 ： [1]表示job ID 是1,11319表示进程ID是34769。切换到后台的进程，仍然可以用ps命令查看。\nfg/bg suspended 可以通过bg （background）和fg（foreground）命令将其在前后台间状态切换。 可以使用+z 组合键把当前fg的进程挂起:\nSource changed \u0026#34;/Users/r/datewu.github.io/content/posts/bash-command/index.md\u0026#34;: WRITE Total in 15 ms ^Z [1] + 3595 suspended hugo server 注意，挂起的进程会暂停运行。需要再执行bg/fg命令才能继续运行：\ndatewu.github.io on  main [!?] took 7m41s ✦ ❯ bg [1] + 3595 continued hugo server 同样的，bg之后执行fg命令就可以把job重新分配到当前的shell了。\ndatewu.github.io on  main [!?] ✦ ❯ ls Makefile Readme.md archetypes config.yaml content data layouts resources static themes datewu.github.io on  main [!?] ✦ ❯ fg [1] + 3595 running hugo server 守护进程 如果一个进程永远都是以后台方式启动，并且不能受到Shell退出影响而退出，一个正统的做法是将其创建为守护进程。 守护进程值得是系统长期运行的后台进程，类似Windows服务。守护进程信息通过ps –a无法查看到，需要用到–x参数。 当查看守护进程时，往往还附上-j参数以查看作业控制信息，其中TPGID一栏为-1就是守护进程。\nroot ~ ps xj PPID PID PGID SID TTY TPGID STAT UID TIME COMMAND 953 1190 1190 1190 ? -1 Ss 1000 0:00 /bin/sh /usr/bin/startkde 1 1490 1482 1482 ? -1 Sl 1000 0:00 /usr/bin/VBoxClient –seamless 1 1491 1477 1477 ? -1 Sl 1000 0:00 /usr/bin/VBoxClient –display 创建守护进程最关键的一步是调用setsid函数创建一个新的Session，并成为Session Leader。 成功创建守护进程的流程为：\n创建一个新的Session，当前进程成为Seesion Leader， 当前进程的id就是Session id； 创建一个新的进程组，当前进程成为进程组的Leader， 当前进程的id就是进程组的id； 如果当前进程原本有一个控制终端，则它会失去这个shell，成为一个没有shell的进程（即守护进程）。 可以使用的命令有 disown setsid nohup\n❯ tldr disown Command disown does not exist for the host platform. Displaying the page from linux platform disown Allow sub-processes to live beyond the shell that they are attached to. See also the jobs command. More information: https://www.gnu.org/software/bash/manual/bash.html#index-disown. - Disown the current job: disown - Disown a specific job: disown %job_number - Disown all jobs: disown -a - Keep job (do not disown it), but mark it so that no future SIGHUP is received on shell exit: disown -h %job_number See also: jobs ~ ❯ man setsid | head SETSID(2) System Calls Manual SETSID(2) NAME setsid – create session and set process group ID SYNOPSIS #include \u0026lt;unistd.h\u0026gt; pid_t setsid(void); exit #/bin/bash exit(0) #脚本/命令正常退出 exit(1) # 脚本/命令异常退出 $? \u0026amp;\u0026amp; || shell 在执行某个命令的时候，会返回一个返回值，该返回值保存在 shell 变量 $? 中。 当 $? == 0 时，表示执行成功；当 $? == 1 时（非0返回值，一般在0-255间），表示执行失败。 shell 提供了 \u0026amp;\u0026amp; 和 || 来实现命令执行flow控制的功能，shell 将根据 \u0026amp;\u0026amp; 或 || 前面命令的返回值来选择执行后续命令。\n\u0026amp;\u0026amp; command1 \u0026amp;\u0026amp; command2 [\u0026amp;\u0026amp; command3 ...] 命令之间使用\u0026amp;\u0026amp;连接，实现逻辑与的功能； 只有在 \u0026amp;\u0026amp; 左边的命令返回真（命令返回值 $? == 0），\u0026amp;\u0026amp; 右边的命令才会被执行； 只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。 即是短路的功能。 || command1 || command2 [|| command3 ...] 命令之间使用 || 连接，实现 逻辑或的功能； 只有在||左边的命令返回假（命令返回值 $? == 1），|| 右边的命令才会被执行。这和 c 语言中的逻辑或语法功能相同，即实现defult赋值操作； 只要有一个命令返回真（命令返回值 $? == 0），后面的命令就不会被执行。 $ shell语句变量无需声明，需要引用变量指时， 在变量名前添加$符号即可（有时需要配合括号消除歧义）:\nvar=hellhttp://sdf.com?uid=233\u0026amp;pwd=ls echo var # var echo $var # hellhttp://sdf.com?uid=233\u0026amp;pwd=ls va=he echo $valloworld # 无输出，因为没有valloworld变量 echo ${va}lloworld # hellworld $() 先执行括号里面的shell命令，然后把括号里面shell命令的输出做为新的命令去执行。 也可以使用泛引号``。\n❯ echo ls \u0026gt; abc ~/codebase ❯ $(less abc) # same as `less abc` abc gobookIread jsbook lol playaround ~/codebase ❯ $(cat abc) abc gobookIread jsbook lol playaround ~/codebase ❯ $(ls) # same as `ls` zsh: command not found: abc ❯ echo `date` 2021年11月17日 星期三 11时58分02秒 CST ~ ❯ echo $(date) 2021年11月17日 星期三 11时58分10秒 CST wget http basic auth\nwget --no-check-certificate --user user --password pass https://server_address/ curl curl -I google.com curl -i google.com # Post json file curl -vX POST http://localhost:9095/post -d @t.json -H \u0026#34;Content-Type: application/json\u0026#34; -H \u0026#39;Authorization: Bearer eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiIxIiwiaWF0IjoxNTUxNDEyODY2LCJleHAiOjE1NTIwMTc2NjZ9.V6ZT8L_r_arIlxtRHul-FxKt4exErTkvNdYy7O_cR5A\u0026#39; argument default argument #!/usr/local/bin/bash echo \u0026#39;begin\u0026#39; date duration=${1-\u0026#34;3\u0026#34;} # default 3 sleep $duration echo \u0026#39;finish\u0026#39; date #begin #2021年11月17日 星期三 12时05分52秒 CST #finish #2021年11月17日 星期三 12时05分55秒 CST #~ took 3s loop bash 提供 for和while两种循环控制\nfor # for in for i in abc.s{1..1000}; do echo $i done # for arguments ./for.sh x y z sdf{1..4} 1 9 #for a; do #\techo $a #done while (read) while read -r h; do echo -n $h done #❯ bash read-pwds.sh #abcde #abcdexyz #xyz% macos osascript macos osascript notification #!/bin/sh # # must use sh for `at` command #!/usr/bin/osascript #display notification \u0026#34;Lorem ipsum dolor sit amet\u0026#34; with title \u0026#34;Title\u0026#34; # https://apple.stackexchange.com/questions/57412/how-can-i-trigger-a-notification-center-notification-from-an-applescript-or-shel/115373#115373 osascript -e \u0026#39;display notification \u0026#34;吃武汉热干面啦\u0026#34; with title \u0026#34;米西米西\u0026#34; \u0026#39; ","date":"2021-11-17T10:06:54+08:00","image":"https://tab.deoops.com/posts/bash-command/terminal_hue2114e7452ea31417cca47ffc195f951_34424_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/bash-command/","title":"bash简介"},{"content":"当我们本地对git的submodule目录下的文件做了改动时，会发现不论是用git checkout . 还是 git clean -df都无法丢弃修改。使用git status命令查看工作树的状态时会有如下 报错信息git submodule modified content\n错误 以hugo为例，当使用hugo server本地预览博客文章时， hugo会修改主题目录的内容。从而出现 git submodule modified content的问题。\n❯ git status 位于分支 main 您的分支领先 \u0026#39;origin/main\u0026#39; 共 1 个提交。 （使用 \u0026#34;git push\u0026#34; 来发布您的本地提交） 尚未暂存以备提交的变更： （使用 \u0026#34;git add \u0026lt;文件\u0026gt;...\u0026#34; 更新要提交的内容） （使用 \u0026#34;git restore \u0026lt;文件\u0026gt;...\u0026#34; 丢弃工作区的改动） （提交或丢弃子模组中未跟踪或修改的内容） 修改： themes/stack (修改的内容) 修改尚未加入提交（使用 \u0026#34;git add\u0026#34; 和/或 \u0026#34;git commit -a\u0026#34;） 这个问题挺常见的，Google后使用下面两条命令即可清理submodule：\nmeat git submodule foreach --recursive git reset --hard git submodule update --recursive --init How do I revert my changes to a git submodule?\nmake 建议在根目录下编写内容如下的Makefile， 以节省输入命令的时间和加强记忆。\n.PHONY: clean clean: -@git submodule deinit -f . -@git submodule update --init --recursive #git submodule deinit -f . #git submodule update --init 执行make命令即可清除本地对git submodule的改动。\n❯ make 已清除目录 \u0026#39;themes/stack\u0026#39; 子模组 \u0026#39;themes/stack\u0026#39;（https://github.com/datewu/hugo-theme-stack.git）未对路径 \u0026#39;themes/stack\u0026#39; 注册 子模组 \u0026#39;themes/stack\u0026#39;（https://github.com/datewu/hugo-theme-stack.git）已对路径 \u0026#39;themes/stack\u0026#39; 注册 子模组路径 \u0026#39;themes/stack\u0026#39;：检出 \u0026#39;aeb077874a7de9ce71304c990ad5cfdc72664f38\u0026#39; ","date":"2021-11-16T16:13:57+08:00","image":"https://tab.deoops.com/posts/git-submodule/git-submodule_hubf47deaeadbbffe799a99e90ac214cb8_24021_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/git-submodule/","title":"清理git submodule"},{"content":"上个周末江江回家看小宝发了好多脾气。\n表面上是和妈妈吵架，实际上还是怪我没工作。\n“你爸妈其实很想你去上班，应该把你赶出去，却想让我来当坏人。让我来和你吵架。”\n工作找了3个月了，确实很对不住家人们的期待。\n这周四去武汉吧，打工或者做生意，不在家里呆着了。\n","date":"2021-07-01T10:54:49+08:00","image":"https://tab.deoops.com/posts/jia-you/jiayou_hu774117d347ae584e8f355912c62e10a3_91406_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/jia-you/","title":"加油"},{"content":"凌晨5点不到，二楼的房门被打开。\n老妈下来翻箱倒柜找东西，我睡眼朦胧，不情不愿的问了下在找什么。\n老妈说有没有看到装钱的包包。\n几句话交流下来，原来是今天准备去买菜，发现4000多块人民币连钱带包都不见了，最后一次拿钱是在3楼。\n结合以往的经验，我说应该是又被小孩子偷，老妈想了想去3楼了。\n十多分钟后，我去3楼去看看情况，果然是被家里全托的4个小孩偷了。\n每天偷100块，主要是买烧烤，平均每次吃80多块钱的烧烤。\n4个小孩，全部知情，无一人上报/揭发。\n其中一个小女孩，读2年级，经常被另外3给小男孩欺负，她虽然没有分赃但也是啥也不说。\n3个男孩子，两个读2年级，一个读4年级。\n3个男孩子吃不完的烧烤就带给学校别的小伙伴吃（但是不给另外全托的那个女孩子）。\n我把他们一个个单独拉出来问话。\n结果有问题的是二年级的a男生和四年级的A男生。\nA有前科，在我家偷过好几次，只是数额比较小，最多20。\n另外，b男生的2000多块钱的儿童手表不见过好几次，有一次是A帮着从a的书包里搜出来了，手表用袋子包了好几层。\n回到这次偷钱事件上。\na虽然承认每次事自己拿的钱，但是一口咬定说：“是A叫他拿的，不拿的话A就打死他。\u0026quot;\nA很狡猾，啥也不承认。\n综合所有人的口供，总的说来，就是罗生门，所有人都说自己没问题，或者是被胁迫的。\n我心中暗想，去吃烧烤的时候咋没说是被胁迫的（小女孩除外，他们不带她去吃）。\n只是没想到9，10岁年纪的小孩子已经可以演绎罗生门了。\n人之初性本善，人到底是啥时候忘记的本心呢？3岁？ 5岁？\n救救孩子吧\n","date":"2021-06-01T11:09:29+08:00","image":"https://tab.deoops.com/posts/little-child/child_huc415f0c9abc48b509b81646876ab4331_8285_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/little-child/","title":"'小'偷"},{"content":"昨天在武汉打了个的士，19公里，80块，路况基本一路通畅，3到4个红绿灯。\n在2021年网约车这么多的情况，的士师傅还是这么狂的也是少见。\n经过 本来是打开滴滴，弹出来青菜拼车，只要40来块钱两个人，但是一直拼不到。\n看了下滴滴快车是69块。\n但是等了15来分钟的拼车，还要赶着去客运站，感觉被滴滴耍了，怒退单。\n同行的伙伴说打的士吧，应该不贵。\n这个武汉的的士师傅，还是个女师傅，真的是太心黑了。\n打表，跑了16来公里就上80多块钱了。我们提出抗议后，大方的说“行，反正快到了了，那就不打表了，给80就行”。\n惹不起她，达到目的地 给了80块。\n后记 最后我们几十赶到客运站了，但是直达老家的车去保养了，我们转了两次车回家了。\n另外在的士上接到了个约我面试的电话:\u0026gt;\n","date":"2021-05-20T10:37:55+08:00","image":"https://tab.deoops.com/posts/19-80/taxi_hu7ea228d815ac6efb23a84e506d592faf_16498_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/19-80/","title":"19公里80块"},{"content":"今天发现vscodevim插件不能连续输入方向键j， 以为是插件的问题，关闭了插件。\n发现在vscode里按住j不放，编辑器并不会连续输入j。\n需要调整系统的dafaults关闭PressAndHold选项：\nenable key-repeating\nTo enable key-repeating, execute the following in your Terminal, log out and back in, and then restart VS Code:\ndefaults write com.microsoft.VSCode ApplePressAndHoldEnabled -bool false # Enable key-repeating for vs code ","date":"2021-05-09T19:25:25+08:00","image":"https://tab.deoops.com/posts/vscode-hold/vim-jkhl_hu82495191c6c5575719d500aa4784b4a7_107933_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/vscode-hold/","title":"vscode按键调整"},{"content":"4月份腾讯面试的时候被问到如何在空间复杂度为O（1）前提下检查连表是否为闭环：\n当时没想出来，面试官提醒用快慢指针也没写出来。\n回到家里想了下，其实当时已经想出来了，没敢写出来:\nfunc circular(head *ListNode) bool { slow, fast := head, head for fast != nil \u0026amp;\u0026amp; fast.Next != nil { slow = slow.Next fast = fast.Next.Next if slow == fast { return true } } return false } Floyd’s slow and fast pointers approach ","date":"2021-04-19T19:56:53+08:00","image":"https://tab.deoops.com/posts/two-points/Image-3_hu1cf7d8b8efc5a2b157c3acd03bb293db_51553_120x120_fill_box_smart1_1.gif","permalink":"https://tab.deoops.com/posts/two-points/","title":"快慢指针"},{"content":"同事问我，把企业的harbor镜像仓库服务暴露到公网之后，有人暴力登陆怎么办？\nharbor默认是没有captcha的，google了很久也没看到。\n咋办呢？\n所以还是自研的产品好哦，随随便便就可以加个手机验证码。\nps：今天是在招商金科的最后一天了。\n","date":"2021-03-31T17:24:52+08:00","image":"https://tab.deoops.com/posts/zi-yan/open-source_hu3adf11e8af7981b881d662db7ec05d58_162335_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/zi-yan/","title":"自研"},{"content":"今天看到react 17.0.2 发布一个礼拜了(快有1年没大的更新了），想升级试一试，在项目下执行 npm update指令，结果报错如下：\n✗ npm update npm ERR! code ERESOLVE npm ERR! ERESOLVE unable to resolve dependency tree npm ERR! npm ERR! Found: @babel/core@7.12.3 npm ERR! node_modules/@babel/core npm ERR! @babel/core@\u0026#34;7.12.3\u0026#34; from the root project npm ERR! @babel/core@\u0026#34;^7.9.0\u0026#34; from @svgr/webpack@5.4.0 npm ERR! node_modules/@svgr/webpack npm ERR! @svgr/webpack@\u0026#34;5.4.0\u0026#34; from the root project npm ERR! 9 more (babel-jest, babel-loader, ...) npm ERR! npm ERR! Could not resolve dependency: npm ERR! peer @babel/core@\u0026#34;^7.13.0\u0026#34; from @babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining@7.13.12 npm ERR! node_modules/@babel/preset-env/node_modules/@babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining npm ERR! @babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining@\u0026#34;^7.13.12\u0026#34; from @babel/preset-env@7.13.12 npm ERR! node_modules/@babel/preset-env npm ERR! @babel/preset-env@\u0026#34;^7.9.5\u0026#34; from @svgr/webpack@5.4.0 npm ERR! node_modules/@svgr/webpack npm ERR! @svgr/webpack@\u0026#34;5.4.0\u0026#34; from the root project npm ERR! npm ERR! Fix the upstream dependency conflict, or retry npm ERR! this command with --force, or --legacy-peer-deps npm ERR! to accept an incorrect (and potentially broken) dependency resolution. npm ERR! npm ERR! See /Users/r/.npm/eresolve-report.txt for a full report. npm ERR! A complete log of this run can be found in: npm ERR! /Users/r/.npm/_logs/2021-03-30T08_44_22_160Z-debug.log 这个项目运行过 npm run eject，所以担心是eject造成的。\n于是在另外一个目录执行npx create-react-app abc命令做来一个对照组。\n把abc项目eject后发现npm install依然会失败。\ngoogle发现有类似的遭遇\nHumm, if I see things correctly, the resolving dependency error occurs because create-react-app has the \u0026ldquo;@babel/core\u0026rdquo;: dependency fixed to \u0026ldquo;7.12.3\u0026rdquo; in its \u0026ldquo;packages.json\u0026rdquo; file. 3 days ago, @babel/plugin-bugfix-v8-spread-parameters-in-optional-chaining was merged to Babe master branch that requires \u0026ldquo;@babel/core\u0026rdquo; dependency to be \u0026ldquo;7.13.0\u0026rdquo; or greater. I don\u0026rsquo;t know create-react-app too well, but would perhaps relaxing the \u0026ldquo;@babel/core\u0026rdquo; dependency to \u0026ldquo;^7.12.3\u0026rdquo; be a proper way to fix this issue?\n修改package.json文件，添加\u0026quot;@babel/core\u0026quot;: \u0026ldquo;7.13.14\u0026rdquo;, 依赖，重新npm install 恢复正常。\n一下是完整的package.json文件，供参考:\n{ \u0026#34;name\u0026#34;: \u0026#34;abc\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;0.1.0\u0026#34;, \u0026#34;private\u0026#34;: true, \u0026#34;proxy\u0026#34;: \u0026#34;http://localhost:8080\u0026#34;, \u0026#34;dependencies\u0026#34;: { \u0026#34;@babel/core\u0026#34;: \u0026#34;7.13.14\u0026#34;, \u0026#34;@material-ui/core\u0026#34;: \u0026#34;^4.11.3\u0026#34;, \u0026#34;@material-ui/icons\u0026#34;: \u0026#34;^4.11.2\u0026#34;, \u0026#34;@pmmmwh/react-refresh-webpack-plugin\u0026#34;: \u0026#34;0.4.3\u0026#34;, \u0026#34;@svgr/webpack\u0026#34;: \u0026#34;5.5.0\u0026#34;, \u0026#34;@testing-library/jest-dom\u0026#34;: \u0026#34;^5.11.10\u0026#34;, \u0026#34;@testing-library/react\u0026#34;: \u0026#34;^11.2.5\u0026#34;, \u0026#34;@testing-library/user-event\u0026#34;: \u0026#34;^12.8.3\u0026#34;, \u0026#34;@tinymce/tinymce-react\u0026#34;: \u0026#34;^3.12.1\u0026#34;, \u0026#34;@typescript-eslint/eslint-plugin\u0026#34;: \u0026#34;^4.5.0\u0026#34;, \u0026#34;@typescript-eslint/parser\u0026#34;: \u0026#34;^4.5.0\u0026#34;, \u0026#34;babel-eslint\u0026#34;: \u0026#34;^10.1.0\u0026#34;, \u0026#34;babel-jest\u0026#34;: \u0026#34;^26.6.0\u0026#34;, \u0026#34;babel-loader\u0026#34;: \u0026#34;8.1.0\u0026#34;, \u0026#34;babel-plugin-named-asset-import\u0026#34;: \u0026#34;^0.3.7\u0026#34;, \u0026#34;babel-preset-react-app\u0026#34;: \u0026#34;^10.0.0\u0026#34;, \u0026#34;bfj\u0026#34;: \u0026#34;^7.0.2\u0026#34;, \u0026#34;camelcase\u0026#34;: \u0026#34;^6.1.0\u0026#34;, \u0026#34;case-sensitive-paths-webpack-plugin\u0026#34;: \u0026#34;2.3.0\u0026#34;, \u0026#34;css-loader\u0026#34;: \u0026#34;4.3.0\u0026#34;, \u0026#34;dotenv\u0026#34;: \u0026#34;8.2.0\u0026#34;, \u0026#34;dotenv-expand\u0026#34;: \u0026#34;5.1.0\u0026#34;, \u0026#34;eslint\u0026#34;: \u0026#34;^7.11.0\u0026#34;, \u0026#34;eslint-config-react-app\u0026#34;: \u0026#34;^6.0.0\u0026#34;, \u0026#34;eslint-plugin-flowtype\u0026#34;: \u0026#34;^5.2.0\u0026#34;, \u0026#34;eslint-plugin-import\u0026#34;: \u0026#34;^2.22.1\u0026#34;, \u0026#34;eslint-plugin-jest\u0026#34;: \u0026#34;^24.1.0\u0026#34;, \u0026#34;eslint-plugin-jsx-a11y\u0026#34;: \u0026#34;^6.3.1\u0026#34;, \u0026#34;eslint-plugin-react\u0026#34;: \u0026#34;^7.21.5\u0026#34;, \u0026#34;eslint-plugin-react-hooks\u0026#34;: \u0026#34;^4.2.0\u0026#34;, \u0026#34;eslint-plugin-testing-library\u0026#34;: \u0026#34;^3.9.2\u0026#34;, \u0026#34;eslint-webpack-plugin\u0026#34;: \u0026#34;^2.5.2\u0026#34;, \u0026#34;file-loader\u0026#34;: \u0026#34;6.1.1\u0026#34;, \u0026#34;fs-extra\u0026#34;: \u0026#34;^9.0.1\u0026#34;, \u0026#34;html-react-parser\u0026#34;: \u0026#34;^1.2.4\u0026#34;, \u0026#34;html-webpack-plugin\u0026#34;: \u0026#34;4.5.0\u0026#34;, \u0026#34;identity-obj-proxy\u0026#34;: \u0026#34;3.0.0\u0026#34;, \u0026#34;jest\u0026#34;: \u0026#34;26.6.0\u0026#34;, \u0026#34;jest-circus\u0026#34;: \u0026#34;26.6.0\u0026#34;, \u0026#34;jest-resolve\u0026#34;: \u0026#34;26.6.0\u0026#34;, \u0026#34;jest-watch-typeahead\u0026#34;: \u0026#34;0.6.1\u0026#34;, \u0026#34;js-base64\u0026#34;: \u0026#34;^3.6.0\u0026#34;, \u0026#34;mini-css-extract-plugin\u0026#34;: \u0026#34;0.11.3\u0026#34;, \u0026#34;optimize-css-assets-webpack-plugin\u0026#34;: \u0026#34;5.0.4\u0026#34;, \u0026#34;pnp-webpack-plugin\u0026#34;: \u0026#34;1.6.4\u0026#34;, \u0026#34;postcss-flexbugs-fixes\u0026#34;: \u0026#34;4.2.1\u0026#34;, \u0026#34;postcss-loader\u0026#34;: \u0026#34;3.0.0\u0026#34;, \u0026#34;postcss-normalize\u0026#34;: \u0026#34;8.0.1\u0026#34;, \u0026#34;postcss-preset-env\u0026#34;: \u0026#34;6.7.0\u0026#34;, \u0026#34;postcss-safe-parser\u0026#34;: \u0026#34;5.0.2\u0026#34;, \u0026#34;prismjs\u0026#34;: \u0026#34;^1.23.0\u0026#34;, \u0026#34;prompts\u0026#34;: \u0026#34;2.4.0\u0026#34;, \u0026#34;react\u0026#34;: \u0026#34;^17.0.2\u0026#34;, \u0026#34;react-app-polyfill\u0026#34;: \u0026#34;^2.0.0\u0026#34;, \u0026#34;react-dev-utils\u0026#34;: \u0026#34;^11.0.3\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^17.0.2\u0026#34;, \u0026#34;react-hook-form\u0026#34;: \u0026#34;^6.15.5\u0026#34;, \u0026#34;react-refresh\u0026#34;: \u0026#34;^0.8.3\u0026#34;, \u0026#34;react-router-dom\u0026#34;: \u0026#34;^5.2.0\u0026#34;, \u0026#34;resolve\u0026#34;: \u0026#34;1.18.1\u0026#34;, \u0026#34;resolve-url-loader\u0026#34;: \u0026#34;^3.1.2\u0026#34;, \u0026#34;sass-loader\u0026#34;: \u0026#34;^10.0.5\u0026#34;, \u0026#34;semver\u0026#34;: \u0026#34;7.3.2\u0026#34;, \u0026#34;style-loader\u0026#34;: \u0026#34;1.3.0\u0026#34;, \u0026#34;terser-webpack-plugin\u0026#34;: \u0026#34;4.2.3\u0026#34;, \u0026#34;tinymce\u0026#34;: \u0026#34;^5.7.1\u0026#34;, \u0026#34;ts-pnp\u0026#34;: \u0026#34;1.2.0\u0026#34;, \u0026#34;url-loader\u0026#34;: \u0026#34;4.1.1\u0026#34;, \u0026#34;web-vitals\u0026#34;: \u0026#34;^1.1.1\u0026#34;, \u0026#34;webpack\u0026#34;: \u0026#34;4.44.2\u0026#34;, \u0026#34;webpack-dev-server\u0026#34;: \u0026#34;3.11.1\u0026#34;, \u0026#34;webpack-manifest-plugin\u0026#34;: \u0026#34;2.2.0\u0026#34;, \u0026#34;workbox-webpack-plugin\u0026#34;: \u0026#34;5.1.4\u0026#34; }, \u0026#34;scripts\u0026#34;: { \u0026#34;start\u0026#34;: \u0026#34;node scripts/start.js\u0026#34;, \u0026#34;build\u0026#34;: \u0026#34;node scripts/build.js\u0026#34;, \u0026#34;test\u0026#34;: \u0026#34;node scripts/test.js\u0026#34; }, \u0026#34;eslintConfig\u0026#34;: { \u0026#34;extends\u0026#34;: [ \u0026#34;react-app\u0026#34;, \u0026#34;react-app/jest\u0026#34; ] }, \u0026#34;browserslist\u0026#34;: { \u0026#34;production\u0026#34;: [ \u0026#34;\u0026gt;0.2%\u0026#34;, \u0026#34;not dead\u0026#34;, \u0026#34;not op_mini all\u0026#34; ], \u0026#34;development\u0026#34;: [ \u0026#34;last 1 chrome version\u0026#34;, \u0026#34;last 1 firefox version\u0026#34;, \u0026#34;last 1 safari version\u0026#34; ] }, \u0026#34;jest\u0026#34;: { \u0026#34;roots\u0026#34;: [ \u0026#34;\u0026lt;rootDir\u0026gt;/src\u0026#34; ], \u0026#34;collectCoverageFrom\u0026#34;: [ \u0026#34;src/**/*.{js,jsx,ts,tsx}\u0026#34;, \u0026#34;!src/**/*.d.ts\u0026#34; ], \u0026#34;setupFiles\u0026#34;: [ \u0026#34;react-app-polyfill/jsdom\u0026#34; ], \u0026#34;setupFilesAfterEnv\u0026#34;: [ \u0026#34;\u0026lt;rootDir\u0026gt;/src/setupTests.js\u0026#34; ], \u0026#34;testMatch\u0026#34;: [ \u0026#34;\u0026lt;rootDir\u0026gt;/src/**/__tests__/**/*.{js,jsx,ts,tsx}\u0026#34;, \u0026#34;\u0026lt;rootDir\u0026gt;/src/**/*.{spec,test}.{js,jsx,ts,tsx}\u0026#34; ], \u0026#34;testEnvironment\u0026#34;: \u0026#34;jsdom\u0026#34;, \u0026#34;testRunner\u0026#34;: \u0026#34;/Users/r/repo/github/blog-fn/node_modules/jest-circus/runner.js\u0026#34;, \u0026#34;transform\u0026#34;: { \u0026#34;^.+\\\\.(js|jsx|mjs|cjs|ts|tsx)$\u0026#34;: \u0026#34;\u0026lt;rootDir\u0026gt;/config/jest/babelTransform.js\u0026#34;, \u0026#34;^.+\\\\.css$\u0026#34;: \u0026#34;\u0026lt;rootDir\u0026gt;/config/jest/cssTransform.js\u0026#34;, \u0026#34;^(?!.*\\\\.(js|jsx|mjs|cjs|ts|tsx|css|json)$)\u0026#34;: \u0026#34;\u0026lt;rootDir\u0026gt;/config/jest/fileTransform.js\u0026#34; }, \u0026#34;transformIgnorePatterns\u0026#34;: [ \u0026#34;[/\\\\\\\\]node_modules[/\\\\\\\\].+\\\\.(js|jsx|mjs|cjs|ts|tsx)$\u0026#34;, \u0026#34;^.+\\\\.module\\\\.(css|sass|scss)$\u0026#34; ], \u0026#34;modulePaths\u0026#34;: [], \u0026#34;moduleNameMapper\u0026#34;: { \u0026#34;^react-native$\u0026#34;: \u0026#34;react-native-web\u0026#34;, \u0026#34;^.+\\\\.module\\\\.(css|sass|scss)$\u0026#34;: \u0026#34;identity-obj-proxy\u0026#34; }, \u0026#34;moduleFileExtensions\u0026#34;: [ \u0026#34;web.js\u0026#34;, \u0026#34;js\u0026#34;, \u0026#34;web.ts\u0026#34;, \u0026#34;ts\u0026#34;, \u0026#34;web.tsx\u0026#34;, \u0026#34;tsx\u0026#34;, \u0026#34;json\u0026#34;, \u0026#34;web.jsx\u0026#34;, \u0026#34;jsx\u0026#34;, \u0026#34;node\u0026#34; ], \u0026#34;watchPlugins\u0026#34;: [ \u0026#34;jest-watch-typeahead/filename\u0026#34;, \u0026#34;jest-watch-typeahead/testname\u0026#34; ], \u0026#34;resetMocks\u0026#34;: true }, \u0026#34;babel\u0026#34;: { \u0026#34;presets\u0026#34;: [ \u0026#34;react-app\u0026#34; ] }, \u0026#34;devDependencies\u0026#34;: { \u0026#34;babel-plugin-prismjs\u0026#34;: \u0026#34;^2.0.1\u0026#34; } } ","date":"2021-03-29T17:02:01+08:00","image":"https://tab.deoops.com/posts/react-17.0.2/react-17_hu425bf1317c9ffe0d093e8348c0548d14_154119_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/react-17.0.2/","title":"React 17.0.2"},{"content":"快从深圳回老家了，东西丢的差不多了，剩下的东西直接顺丰寄回家。\n一共寄了3次顺丰包裹。\n前两次的那个工作人员，6块钱一个纸箱，而且按照体积算。\n第一次算了69kg：4个包裹，288+21元，\n第二次算了51kg： 3个包裹加一个小包裹 216 + 21元。\n第三次换了个工作人员算了38kg：两个纸箱和我自己煤气灶的纸箱不算 173 + 21元。\n第一次算了4个纸箱 4 * 6 = 24 块，第二次算了3个纸箱 3 * 6 = 18 元， 第三次没有收纸箱的钱。\n算下来，寄的越重越省钱。\nupdate 今天（3月31号）寄了最后一次快递，33kg。两个纸箱，一大一小，再加一个帐篷的包。一共 148 +12 = 169元。\n纸箱没有收费。\n","date":"2021-03-20T17:11:26+08:00","image":"https://tab.deoops.com/posts/sf-sz/sf_hu3bd39de258c459f3a5f8c55ffd61be08_51014_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/sf-sz/","title":"顺丰包裹"},{"content":"调试某个go test程序的时候，需要实现confirm/approve功能:\n测试完testCase1之后（或者说是某个断点），用户输入yes，执行下一个case，输入no， 则退出整个测试。\n分析 下意识的觉得这个很好实现，调用fmt.Scan应该就OK了。\n但是写的时候才发现，go test 会强制重定向os.Stdin = /dev/null忽略所有的 stdin输入，\n所以没法使用 fmt.Scan来等待键盘(用户)的输入：\n// fmt.Scan reads from os.Stdin, which is set to /dev/null when testing. // Tests should be automated and self-contained anyway, so you don\u0026#39;t want // to read from stdin. // Either use an external file, a bytes.Buffer, or a strings.Reader and // call scan.Fscan if you want to test **the literal** `scan` Function. 解决方案 可以参考事件驱动，使用named pipes来实现confirm/approve功能：\n消费者 先写一个消费者的函数readPipe：\npackage main import ( \u0026#34;bufio\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { for i := 0; i \u0026lt; 6; i++ { readPipe(i) } } func readPipe(i int) { f, err := os.Open(\u0026#34;pipe\u0026#34;) if err != nil { log.Fatalln(err) } defer f.Close() reader := bufio.NewScanner(f) for reader.Scan() { fmt.Println(i, reader.Text()) } } 然后在测试的case里调用readPipe函数：\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;testing\u0026#34; ) func TestCase1(t *testing.T) { fmt.Println(\u0026#34;testing test case 1\u0026#34;) fmt.Println(\u0026#34;test case 1 tested\u0026#34;) readPipe(1) } func TestCase2(t *testing.T) { fmt.Println(\u0026#34;testing test case 2\u0026#34;) fmt.Println(\u0026#34;test case 2 tested\u0026#34;) readPipe(2) } func TestCase3(t *testing.T) { fmt.Println(\u0026#34;testing test case 3\u0026#34;) fmt.Println(\u0026#34;test case 3 tested\u0026#34;) readPipe(3) } 生成者 创建pipe，然后往pipe里写数据，就可以触发(unblock)消费者进程。\n➜ ~ mkfifo pipe ➜ ~ ls -alh pipe prw-r--r-- 1 r staff 0B 4 24 16:32 pipe 写入数据：\n➜ ~ date \u0026gt; pipe ➜ ~ date \u0026gt; pipe ➜ ~ date \u0026gt; pipe 测试 调试的结果如下：\n➜ ~ go test testing test case 1 test case 1 tested 1 2020年 4月24日 星期五 16时31分50秒 CST testing test case 2 test case 2 tested 2 2020年 4月24日 星期五 16时31分59秒 CST testing test case 3 test case 3 tested 3 2020年 4月24日 星期五 16时32分08秒 CST PASS ok abc 36.016s ","date":"2020-10-14T18:38:01+08:00","image":"https://tab.deoops.com/posts/go-test/ta_moderationworkflow_huf37469d72d7cd32aea4957f9e4060393_25339_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/go-test/","title":"调试golang测试"},{"content":"国内网络环境日益恶劣，执行brew update/upgrade花费的时间够我泡好一壶普洱茶。\n不过好景不长，谁能想到那么大的普洱茶饼，日积月累一点点的被我喝完了。\n哎，怀恋普洱茶呀。 没了普洱茶，我决定换了brew的官方源，给自己节约节约生命。\n解决方案 挑挑拣拣一圈之后，我决定使用清华大学开源软件镜像站]的brew源。\n按照官网的指引很快就换好了repo，效果很好。\n和设置macos DNS servers一样我也整理了个shell脚本：\n#!/usr/local/bin/bash set_qh() { git -C \u0026#34;$(brew --repo)\u0026#34; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/brew.git git -C \u0026#34;$(brew --repo homebrew/core)\u0026#34; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-core.git git -C \u0026#34;$(brew --repo homebrew/cask)\u0026#34; remote set-url origin https://mirrors.tuna.tsinghua.edu.cn/git/homebrew/homebrew-cask.git brew update } # revocer recover() { git -C \u0026#34;$(brew --repo)\u0026#34; remote set-url origin https://github.com/Homebrew/brew.git git -C \u0026#34;$(brew --repo homebrew/core)\u0026#34; remote set-url origin https://github.com/Homebrew/homebrew-core.git git -C \u0026#34;$(brew --repo homebrew/cask)\u0026#34; remote set-url origin https://github.com/Homebrew/homebrew-cask.git brew update } a=${1-\u0026#34;check\u0026#34;} # default to check if [ $a = \u0026#34;r\u0026#34; ]; then recover fi if [ $a = \u0026#34;set\u0026#34; ]; then set_qh fi if [ $a = \u0026#34;check\u0026#34; ]; then echo \u0026#34;goping to EXPORT HOMEBREW_BOTTLE_DOMAIN\u0026#34; export HOMEBREW_BOTTLE_DOMAIN=https://mirrors.tuna.tsinghua.edu.cn/homebrew-bottles echo $HOMEBREW_BOTTLE_DOMAIN brew config | grep ORIGIN brew update brew upgrade brew cleanup fi echo \u0026#34;\u0026#34; echo $a successed! 上面的bash脚本支持3个参数 check,set和 recover，默认使用 check参数。\n保存为set-brew-repo.sh文件，再加上可执行权限即可：\n➜ ~ chmod +x set-brew-repo.sh ➜ ~ ./set-brew-repo.sh ","date":"2020-08-14T11:33:00+08:00","image":"https://tab.deoops.com/posts/brew-repo/homebrew-social-card_hud11338d999649aa10468d459dd1e6032_48303_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/brew-repo/","title":"更换brew源"},{"content":"众所周知，修改/etc/resolv.conf配置文件可以配置Linux的dns 解析服务器。\n[root@VM-8-3-centos ~]# cat /etc/resolv.conf # Generated by NetworkManager nameserver 183.60.83.19 nameserver 183.60.82.98 那么苹果系统的dns 解析服务器应该在哪里配置呢，也是配置/etc/resolv.conf文件吗？\n➜ ~ cat /etc/resolv.conf # macOS Notice # # This file is not consulted for DNS hostname resolution, address # resolution, or the DNS query routing mechanism used by most # processes on this system. # # To view the DNS configuration used by this system, use: # scutil --dns # # SEE ALSO # dns-sd(1), scutil(8) # # This file is automatically generated. # nameserver 127.0.0.1 nameserver 192.168.1.1 故事 因为新冠的爆发，公司下放了VPN权限，可以在家接公司办公网络办公。\n这段时间我在家享受丝滑顺畅的办公内网网络，基本没遇到啥问题，每天都美滋滋的。\n昨天手机运营商联通突然打电话给我说，我的手机是5G手机，设置一下网络模式可以在原有4G的套餐上使用5G网络啦，诚邀我体验。\n那么联通是怎么知道我使用的是5G手机的呢？？？这个先按下不表。\n我稍稍体验了一会，5G网络的速度和时延果然和宣传中的一样牛逼。\n我不由的感叹：华为果然🐂🍺。\n这么好的网络质量，不应该让笔记本也享受享受吗？\n话不多说我立马把苹果电脑连接上了手机热点，用了不多久就发现遇到了DNS的问题。\n问题 苹果笔记本连接手机热点的Wi-Fi后，手机热点 不会使用 VPN指定的DNS server去解析 办公网络的内网地址，导致无妨访问内网域名。\n解决方案 解决的办法也很简单：打开 设置-\u0026gt; 网络 -\u0026gt; WI-Fi -\u0026gt; 高级 -\u0026gt; DNS -\u0026gt; 修改DNS 服务器 即可。\n鼓捣一会发现，每次操作dns server都需要用鼠标点击UI，再点击6次子菜单，复制两次DNS server 的IP 地址，才能完成设置，这种操作很不程序员。\n于是搜索了macos设置dns servers的命令，把上面的UI操作脚本化了。整理了一份bash脚本如下：\n#!/usr/local/bin/bash a=${1:-check} echo ${a}ing ... #dns=\u0026#34;100.67.174.10 100.67.174.11\u0026#34; dns=\u0026#34;100.67.174.10 100.67.174.11\u0026#34; if [ $a = \u0026#34;check\u0026#34; ] then networksetup -getinfo wi-fi echo \u0026#34;\u0026#34; networksetup -getdnsservers wi-fi fi if [ $a = \u0026#34;set\u0026#34; ] then networksetup -setdnsservers wi-fi $dns echo \u0026#34;\u0026#34; networksetup -getdnsservers wi-fi fi if [ $a = \u0026#34;clear\u0026#34; ] then networksetup -setdnsservers wi-fi Empty echo \u0026#34;\u0026#34; networksetup -getdnsservers wi-fi fi echo \u0026#34;\u0026#34; echo $a successed! 脚本支持3个参数 check,set和 clear，默认使用 check参数。\n保存为set-dns.sh文件，再加上可执行权限即可：\n➜ ~ chmod +x set-dns.sh ➜ ~ ./set-dns.sh ","date":"2020-06-10T22:20:03+08:00","image":"https://tab.deoops.com/posts/macos-dns/dns-resolver_hue9933fca20fb9e9b208b55df1c447433_56249_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/macos-dns/","title":"设置dns"},{"content":"使用了一段时间Expect自动化工具简介里面的expect脚本，发现少了一些功能：\n怎么给expect脚本传参数呢？ expect怎么调用 bash/sh外部命令呢？ expect怎么操作字符串呢？ tcl 前面的文章Expect命令里面提到过，expect 使用的是Tcl/tk的语法。\n所以 大家Google 一下 tcl tutorial就可以解决上面三个问题了。\ntcl argc argc\nTcl tutorial\nTcl regexp\nnew expect script 结合上面的3篇教程我把上篇文章自动化ssh 登陆的脚本优化如下:\n#!/usr/bin/expect -f # for anyone not familar with expect # should read this awesome article # https://www.pantz.org/software/expect/expect_examples_and_tips.html if { $argc != 1} { puts \u0026#34;must set one argument for server_A host ip\u0026#34; puts \u0026#34;./login.tcl 1.1.2.2\u0026#34; exit 1 } set timeout 15 #set info [gets stdin] set info [exec privateRESTfullAPI2GetPwd -host $argv] set result [regexp {\\\u0026#34;([^\\\u0026#34;]*)\\\u0026#34;[^\\\u0026#34;]*\\\u0026#34;([^\\\u0026#34;]*)\\\u0026#34;[^\\\u0026#34;]*\\\u0026#34;([^\\\u0026#34;]*)\\\u0026#34;} $info match host pw1 pw2] send_user \u0026#34;going to connected to server_A\\n\u0026#34; spawn ssh -q -o StrictHostKeyChecking=no nobody@host expect { timeout { send_user \u0026#34;\\ntimeout Failed to get password prompt, is VPN on?\\n\u0026#34;; exit 1 } eof { send_user \u0026#34;\\nSSH failure for server_A\\n\u0026#34;; exit 1 } \u0026#34;*assword:\u0026#34; } send \u0026#34;$pwd1\\r\u0026#34; expect { timeout {send_user \u0026#34;\\nSSH failure for server_B\\n\u0026#34;; exit 1 } \u0026#34;Last login:*\u0026#34; } send \u0026#34;su -\\r\u0026#34; expect { eof { send_user \u0026#34;\\nSSH failure check your password \\n\u0026#34;; exit 1 } \u0026#34;密码\u0026#34; } send \u0026#34;$pw2\\r\u0026#34; interact 简单解释上面的脚本：\n$argc 命令行参数的个数； $argv 命令行参数（不含命令本身）; puts 相当于 bash 的 echo命令 ; exec 相当于 bash的 $(); privateRESTfullAPI2GetPwd -host $argv 是我本地的一个用来查询主机密码的工具，返回值的格式为：Passworod of \u0026quot;1.1.1.1\u0026quot; are \u0026quot;pwd for noby\u0026quot; and \u0026quot;pwd for root\u0026quot; 正则表达式 {\\\u0026quot;([^\\\u0026quot;]*)\\\u0026quot;[^\\\u0026quot;]*\\\u0026quot;([^\\\u0026quot;]*)\\\u0026quot;[^\\\u0026quot;]*\\\u0026quot;([^\\\u0026quot;]*)\\\u0026quot;}用来提取上面privateAPI2GetPwd返回值中3个 双引号中的内容， 即： host, pwd1, pwd2 命令set result [regexp {...第五条..} $info match host pw1 pw2]把上面正则表达式的的3个 group 分别赋值给 host, pw1, pw2三个变量。 丢弃了result 和 match 两个变量。 ","date":"2020-05-19T16:41:43+08:00","image":"https://tab.deoops.com/posts/tcl-101/tcl-tk_huac390069c03572eabeb0f3110525b1ed_71932_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/tcl-101/","title":"tcl入门"},{"content":"众所周知kubenetest的端口转发功能kubectl port-forward非常实用，可以提高开发人员的debug效率。\n其实kubectl port-forward的底层脏活累活都是socat命令在做，kubectl只能算是一个代理商。\nsocat 端口转发 socat tcp-listen:58812,reuseaddr,fork tcp:localhost:8000把58812端口的流量转发到 8000上。\nbash-5.1# python3 -m http.server \u0026amp; bash-5.1# socat tcp-listen:58812,reuseaddr,fork tcp:localhost:8000 \u0026amp; bash-5.1# curl -I localhost:58812 HTTP/1.0 200 OK Server: SimpleHTTP/0.6 Python/3.10.3 Date: Thu, 14 Apr 2022 03:19:15 GMT Content-type: text/html; charset=utf-8 Content-Length: 336 从而使得原来无法访问的 localhost:8000端口的服务，现在可以通过*:58812访问到了。\nbash-5.1# netstat -nlp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:58812 0.0.0.0:* LISTEN 10/socat tcp 0 0 0.0.0.0:8000 0.0.0.0:* LISTEN 31/python3 Active UNIX domain sockets (only servers) Proto RefCnt Flags Type State I-Node PID/Program name Path 其他 ➜ ~ tldr socat socat Multipurpose relay (SOcket CAT). - Listen to a port, wait for an incoming connection and transfer data to STDIO: socat - TCP-LISTEN:8080,fork - Create a connection to a host and port, transfer data in STDIO to connected host: socat - TCP4:www.example.com:80 - Forward incoming data of a local port to another host and port: socat TCP-LISTEN:80,fork TCP4:www.example.com:80 语法 man 手册\nSocat is a command line based utility that establishes two bidirectional byte streams and transfers data between them. Because the streams can be constructed from a large set of different types of data sinks and sources (see address types), and because lots of address options may be applied to the streams, socat can be used for many different purposes.\nFilan is a utility that prints information about its active file descriptors to stdout. It has been written for debugging socat, but might be useful for other purposes too. Use the -h option to find more infos.\nProcan is a utility that prints information about process parameters to stdout. It has been written to better understand some UNIX process properties and for debugging socat, but might be useful for other purposes too.\nThe life cycle of a socat instance typically consists of four phases.\nIn the init phase, the command line options are parsed and logging is initialized.\nDuring the open phase, socat opens the first address and afterwards the second address. These steps are usually blocking; thus, especially for complex address types like socks, connection requests or authentication dialogs must be completed before the next step is started.\nIn the transfer phase, socat watches both streamscq read and write file descriptors via CWselect() , and, when data is available on one side and can be written to the other side, socat reads it, performs newline character conversions if required, and writes the data to the write file descriptor of the other stream, then continues waiting for more data in both directions.\nWhen one of the streams effectively reaches EOF, the closing phase begins. Socat transfers the EOF condition to the other stream, i.e. tries to shutdown only its write stream, giving it a chance to terminate gracefully. For a defined time socat continues to transfer data in the other direction, but then closes all remaining channels and terminates.\n","date":"2020-04-14T09:50:07+08:00","image":"https://tab.deoops.com/posts/socat-usage/socat_hu03f89a710279d4dc3ef9b7f10baa691c_127416_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/socat-usage/","title":"socat工具"},{"content":"本文不定期更新 :)\n上个礼拜逛Hacker News看到推荐了一份写于1991年介绍TCP/IP协议的文章A TCP/IP Tutorial。\n初略的扫了几眼，发现不错，加入了收藏夹。\n昨天晚上抽出时间来细读了一遍觉得很有翻译的价值，于是试着翻译一下：\nIntroduction This tutorial contains only one view of the salient points of TCP/IP, and therefore it is the \u0026ldquo;bare bones\u0026rdquo; of TCP/IP technology. It omits the history of development and funding, the business case for its use, and its future as compared to ISO OSI. Indeed, a great deal of technical information is also omitted. What remains is a minimum of information that must be understood by the professional working in a TCP/IP environment. These professionals include the systems administrator, the systems programmer, and the network manager.\nThis tutorial uses examples from the UNIX TCP/IP environment, however the main points apply across all implementations of TCP/IP.\nNote that the purpose of this memo is explanation, not definition. If any question arises about the correct specification of a protocol, please refer to the actual standards defining RFC.\nThe next section is an overview of TCP/IP, followed by detailed descriptions of individual components.\n序言 这篇教程只是对 TCP/IP 协议中一些最重要的要点做描述性的介绍，因此可以把这篇教程看作是TCP/IP 技术的骨架。 这篇教程不涉及 TCP/IP 的发展历史，资金/资助来源，商业用途以及和ISO OSI的对比来看TCP/IP的未来会怎样。而且很多详细的技术细节也不会在本文中得到介绍。 这篇教程所涉及的内容，恰恰是一个日常工作在TCP/IP 环境的专业人士必须理解的最少TCP/IP技术信息量，不能再少了。这些专业人士包括了系统管理员，系统程序员，和网络管理员。 这篇教程使用的例子都是在 UNIX TCP/IP 环境，但是这些例子所表达的主要意思对所有实现了TCP/IP协议的环境来说都是同样试用的。\n注意⚠️ 这篇教程的目的是解释/介绍 TCP/IP技术，并不是给TCP/IP 技术下定义。\n如果读者对这篇教程描述的某个协议的规格/规范产生了疑问，请参考这个协议的RFC具体的规范定义。 下个小节是对 TCP/IP技术的概览，然后是单独的对每个TCP/IP组件详细的描述。\nTCP/IP Overview The generic term \u0026ldquo;TCP/IP\u0026rdquo; usually means anything and everything related to the specific protocols of TCP and IP. It can include other protocols, applications, and even the network medium. A sample of these protocols are: UDP, ARP, and ICMP. A sample of these applications are: TELNET, FTP, and rcp. A more accurate term is \u0026ldquo;internet technology\u0026rdquo;. A network that uses internet technology is called an \u0026ldquo;internet\u0026rdquo;.\nTCP/IP 概览 术语“TCP/IP”一般意义上来说，通常是指 任何以及所有和 ‘TCP 协议’，与 ‘IP 协议’有关的技术。 “TCP/IP”术语可以指 其它的网络协议，网络应用，甚至是物理网络媒介。这些网络协议可以是：UDP, ARP, 和 ICMP； 网络应用可以是：TELNET，FTP，和 RCP。\n“TCP/IP”更准确的术语应该是“互联网技术”。当一个网络使用了互联网技术，我们就可以把这个网络叫“互联网”。\nBasic Structure To understand this technology you must first understand the following logical structure:\n---------------------------- | network applications | | | |... \\ | / .. \\ | / ...| | ----- ----- | | |TCP| |UDP| | | ----- ----- | | \\ / | | -------- | | | IP | | | ----- -*------ | | |ARP| | | | ----- | | | \\ | | | ------ | | |ENET| | | ---@-- | ----------|----------------- | ----------------------o--------- Ethernet Cable Figure 1. Basic TCP/IP Network Node This is the logical structure of the layered protocols inside a computer on an internet. Each computer that can communicate using internet technology has such a logical structure. It is this logical structure that determines the behavior of the computer on the internet. The boxes represent processing of the data as it passes through the computer, and the lines connecting boxes show the path of data. The horizontal line at the bottom represents the Ethernet cable; the \u0026ldquo;o\u0026rdquo; is the transceiver. The \u0026ldquo;*\u0026rdquo; is the IP address and the \u0026ldquo;@\u0026rdquo; is the Ethernet address. Understanding this logical structure is essential to understanding internet technology; it is referred to throughout this tutorial.\n2.1 基础结构\n要想理解TCP/IP技术，首先你必须要理解下面图表展示的网络节点逻辑结构：\n---------------------------- | 网络应用 | | | |... \\ | / .. \\ | / ...| | ----- ----- | | |TCP| |UDP| | | ----- ----- | | \\ / | | -------- | | | IP | | | ----- -*------ | | |ARP| | | | ----- | | | \\ | | | ------ | | |ENET| | | ---@-- | ----------|----------------- | ----------------------o--------- 以太网网线 图1. 基础TCP/IP网络节点逻辑结构 上图（图1）描绘的是互联网上某台计算机节点内部网络协议的逻辑分层架构。每一台能使用互联网技术和其它计算机通信的计算机都有图1描述的逻辑结构。图1描述的逻辑结构也决定了互联网上计算机的行为。\n图1中的 虚线框 表示当数据在计算机内部传输时 对数据的某种处理， 连接虚线框的虚线表示数据在计算机内部传输的路径。最下面的那条虚线表示以太网线；“o”表示接收发送器， “*”表示IP 地址，“@”表示 以太（MAC）地址。\n理解图1所描述的逻辑结构的对理解整个互联网技术起着关键性作用；而且对图1的引用贯穿了整篇教程。\nTerminology The name of a unit of data that flows through an internet is dependent upon where it exists in the protocol stack. In summary: if it is on an Ethernet it is called an Ethernet frame; if it is between the Ethernet driver and the IP module it is called a IP packet; if it is between the IP module and the UDP module it is called a UDP datagram; if it is between the IP module and the TCP module it is called a TCP segment (more generally, a transport message); and if it is in a network application it is called a application message.\nThese definitions are imperfect. Actual definitions vary from one publication to the next. More specific definitions can be found in RFC 1122, section 1.3.3.\nA driver is software that communicates directly with the network interface hardware. A module is software that communicates with a driver, with network applications, or with another module.\nThe terms driver, module, Ethernet frame, IP packet, UDP datagram, TCP message, and application message are used where appropriate throughout this tutorial\n2.2 术语\n根据数据在图1中被哪一层逻辑层处理，我们给每层的数据块起了不同的名字加以区分。\n总的来说，如果数据块在以太网上，那么这个数据块就被称为 \u0026lsquo;Ethernet frame\u0026rsquo;； 如果数据块\n","date":"2020-04-13T22:25:56+08:00","image":"https://tab.deoops.com/posts/tcp-ip-tutorial/_hucbca32dcf96b8048fcbe36f855742c0a_81541_b8a26a82b3d2640f14fb51e730ee4ea7.png","permalink":"https://tab.deoops.com/posts/tcp-ip-tutorial/","title":"TCP/IP教程"},{"content":"问题 已知本地local可以ssh到server A和B，\n问如何从server A ssh到server B，以及如何在server A 和B之间使用scp互传文件 ？\n解决方案 配置ssh_config然后使用 ssh-add命令指定使用ssh agent可以很安全和高效的解决上面的两个问题。\n配置ssh_config 注意看注释，下面脚本所有的操作和配置都是在 本地local上执行的:\nvi .ssh/config ## 修改配置 添加forward agent HOST server-A HostName 1.2.3.4 User root IdentityFile ~/.ssh/serverA ForwardAgent yes # 现在 serverA可以和local本机的agent通信啦 HOST server-B HostName 6.7.8.9 User root IdentityFile ~/.ssh/serverB ForwardAgent yes # 现在 serverB可以和local本机的ssh agent 通信啦 rg Forward /etc/ssh/ssh_config ## 检查全局的forwordagent配置有没有被覆盖 echo \u0026#34;$SSH_AUTH_SOCK\u0026#34; ## 查看本地ssh agent是否在运行 ssh-add -L ## 查看/操作/删除 ssh agent hold的keys ssh-add -K li ssh-add -K ~/.ssh/serverA ## 给agent serverA 密钥，使得serverB 可以“使用” 这个密钥 ssh-add -K ~/.ssh/serverB ## 给agent serverB密钥，使得serverA可以“使用”这个密钥 ssh-add -K ~/.ssh/github ## 给阿根廷 github密钥，使得serverA， serverB都可以ssh github，方便测试 ssh-add -L 验证github 也是在本机local操作，以github作为测试目标：\nssh -T git@github.com ## 本地基准测试 ssh serverA ## 测试serverA ssh -T git@github.com ## 测试/验证 小结 注意看注释，注意ForwardAgent配置应用到哪个server上了。\n当 server 可以和local本机的agent通信的时候，就表示 server 好像拥有了local本地的private keys了。\n具体说： 当server A 的 ForwardAgent 开启时， server A就可以直接ssh到server B了，\n如果这个时候 server B 的ForwordAgent没有开启，server B是不可以ssh 到server A的\n","date":"2020-04-12T11:34:44+08:00","image":"https://tab.deoops.com/posts/ssh-agent/ssh-agent-typical_hu35990bd695fd81eac7c516b5a65eda1e_35136_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/ssh-agent/","title":"SSH代理（Agent）"},{"content":"公司服务器使用了两层跳板机，外面的一台我们管它叫 server A， 另外一台 叫它 server B。\n虽然我不知道这种双保险给公司带来了多少安全感，但是我知道我的运维效率降低了差不多90%吧 :\u0026gt;。\nserver A被直接暴露在公网上， 我们不能使用 ssh key 只能使用 password认证ssh。\n这还不算完，server A每3小时改一次自己的root密码。\n后面的server B跳板机器的自我安全感就强多了，server B可以直接免密使用ssh key登陆所有的内网服务器，而且允许server A免密登陆到自己。\n我决得这样两次登录很浪费时间，于是写了个脚本从外网一次性登陆到server B服务器上。\nexpect 脚本 #!/usr/bin/expect -f # for anyone not familar with expect # should read this awesome post # https://www.pantz.org/software/expect/expect_examples_and_tips.html set timeout 15 ### CHANGE pwd every 3h set pwd \u0026#34;mySuperSecretpwd123\u0026#34; set nested_ssh \u0026#34;ssh server_B\u0026#34; ## for debug # log_user 0 # exp_internal 1 send_user \u0026#34;going to connected to server A\\n\u0026#34; spawn ssh -q -o StrictHostKeyChecking=no server_A expect { timeout { send_user \u0026#34;\\ntimeout Failed to get password prompt, is VPN on?\\n\u0026#34;; exit 1 } eof { send_user \u0026#34;\\nSSH failure for server A\\n\u0026#34;; exit 1 } \u0026#34;*assword:\u0026#34; } send \u0026#34;$pwd\\r\u0026#34; expect { timeout {send_user \u0026#34;\\nSSH failure for server B\\n\u0026#34;; exit 1 } \u0026#34;Last login:*\u0026#34; } send \u0026#34;$nested_ssh\\r\u0026#34; interact 基本语法 简单说下基本流程如下：\nset: 设置变量； spawn: 给对象一个进程空间，让对象可以运行起来 expect: 模拟用户等待，期待对象输出字符串； send: 模拟用户输入，给对象发送字符串； send_user: 提示用户; interact: 用户直接和对象通信，expect不再在中间传话了。 因为expect语法继承自Tcl，所以变量赋值/初始化的方式和 Bash不同，需要使用set关键字。\n注意第5点send_user命令是为了更好的用户交互，主要是给用户一下提示信息/反馈信息。\n最后注意：send \u0026quot;$pwd\\r\u0026quot;语句的转以符是 \\r 而不是\\r\\n。\n详细语法 expect详细的语法和参数解释参考Expect examples and tips\nExpect is an automation and testing tool used to automate a process that receives interactive commands. If you can connect to any machine using ssh, telnet, ftp, etc then you can automate the process with an expect script. This works even with local programs where you would have to interact with a script or program on the command line.\n","date":"2020-03-19T16:07:45+08:00","image":"https://tab.deoops.com/posts/expect-interactive/automate_hu0267170b9c80213022522221d5593617_95938_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/expect-interactive/","title":"Expect自动化工具简介"},{"content":"架构部的同事问了我两个问题:\n宿主机上有个进程很耗 cpu，怎么判断它是不是某个容器的进程; 如果它是跑在容器里，怎么查到是哪个容器(container id); 解决方案 有两种方法来解决上面两个问题：\ncgroup 通过查看pid的cgroup是否含有 slice信息来判断是否是容器进程:\n主机进程 cgroup 直接运行在宿主机主的进程的cgroup是没有slice信息的：\n[root@deoops ~]# cat /proc/1/cgroup 11:perf_event:/ 10:memory:/ 9:devices:/ 8:cpuacct,cpu:/ 7:cpuset:/ 6:hugetlb:/ 5:blkio:/ 4:net_prio,net_cls:/ 3:freezer:/ 2:pids:/ 1:name=systemd:/ [root@deoops ~]# cat /proc/19/cgroup 11:perf_event:/ 10:memory:/ 9:devices:/ 8:cpuacct,cpu:/ 7:cpuset:/ 6:hugetlb:/ 5:blkio:/ 4:net_prio,net_cls:/ 3:freezer:/ 2:pids:/ 1:name=systemd:/ k8s pod进程 cgroup 在宿主机上看跑在容器进程的cgroup是可以看到slice信息的：\n[root@deoops ~]# cat /proc/20397/cgroup 11:perf_event:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 10:memory:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 9:devices:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 8:cpuacct,cpu:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 7:cpuset:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 6:hugetlb:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 5:blkio:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 4:net_prio,net_cls:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 3:freezer:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 2:pids:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 1:name=systemd:/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-pod6d62265d_cd4d_4807_8d2f_386714d5bbdc.slice/docker-c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc.scope 从上面的输出可以看到 pid 20397是 kubernets pod拉起来的进程， 而且pod ID 是： 6d62265d_cd4d_4807_8d2f_386714d5bbdc, 对应的 contariner ID 是 c44a782ba5482052e66dc3f5ed3811e2a699db09b6715e26102d582a51ac52cc\ndocker container进程 cgroup docker containre 和 k8s 一样，也会设置cgroup slice：\n[root@deoops ~]# docker run --name mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=super123456secret -d mysql:5.7.28 Unable to find image \u0026#39;mysql:5.7.28\u0026#39; locally 5.7.28: Pulling from library/mysql 804555ee0376: Pull complete c53bab458734: Pull complete ca9d72777f90: Pull complete 2d7aad6cb96e: Pull complete 8d6ca35c7908: Pull complete 6ddae009e760: Pull complete 327ae67bbe7b: Pull complete 31f1f8385b27: Pull complete a5a3ad97e819: Pull complete 48bede7828ac: Pull complete 380afa2e6973: Pull complete Digest: sha256:b38555e593300df225daea22aeb104eed79fc80d2f064fde1e16e1804d00d0fc Status: Downloaded newer image for mysql:5.7.28 771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633 [root@deoops ~]# ps aux | grep mysql polkitd 23855 3.0 1.1 1007636 190588 ? Ssl 09:39 0:00 mysqld root 24176 0.0 0.0 112724 984 pts/0 S+ 09:39 0:00 grep --color=auto mysql [root@deoops ~]# cat /proc/23855/cgroup 11:perf_event:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 10:memory:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 9:devices:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 8:cpuacct,cpu:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 7:cpuset:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 6:hugetlb:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 5:blkio:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 4:net_prio,net_cls:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 3:freezer:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 2:pids:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope 1:name=systemd:/system.slice/docker-771aa2fdc8cae66a0cb05a4585beff44a69f0f1c5b2a21fd159cbeea0a86d633.scope [root@deoops ~]# docker ps | grep 771aa2fdc 771aa2fdc8ca mysql:5.7.28 \u0026#34;docker-entrypoint.s…\u0026#34; 58 seconds ago Up 57 seconds 0.0.0.0:3306-\u0026gt;3306/tcp, 33060/tcp mysql 从上面bash脚本的输出可以清楚的看到，cgroup配置的container ID 和 docker ps 命令输出的 container ID是一样的，都是 771aa2fdc8ca\nppid 下面两种方法和docker强相关，不推荐。\npstree 老老实实的一步步排查主机上所有的container ppid： 用 pstree命令查看宿主机上的进程树，看\u0026lt;pid\u0026gt; 是不是docker-current创建的子进程;\ndocker inspect 使用docker inspect命令查询所有容器的pid，然后做过滤，找到该进程对应的容器名，id，容器的详细信息中，包含了它在宿主机上的进程号。\ndocker inspect -f \u0026#34;{{.Id}} {{.State.Pid}} {{.Config.Hostname}}\u0026#34; $(docker ps -q) | grep \u0026lt;pid\u0026gt; 发散思维 MATRIX 那么进程自己如何判断自己是否在container中呢？\n答案很简单，如果 pid 1 的cgroup有slice信息，说明进程现在在容器中，否则则说明不在容器里。\n[root@deoops ~]# kubectl exec -it binary-controller-7dfd4bbd6f-6hsvd sh / # ps PID USER TIME COMMAND 1 root 0:30 /binary-controller -kubeconfig=inCluster -mode=prodution 196 root 0:00 sh 202 root 0:00 ps / # cat /proc/1/cgroup 11:hugetlb:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 10:pids:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 9:perf_event:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 8:freezer:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 7:memory:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 6:net_prio,net_cls:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 5:blkio:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 4:cpuacct,cpu:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 3:devices:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 2:cpuset:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope 1:name=systemd:/kubepods.slice/kubepods-burstable.slice/kubepods-burstable-pod482f0a01_7b3e_48cd_adb6_61c9097b43a4.slice/docker-b3079dba7a1cf909631c2f9e8ad38cc7f193e6ba8fe649d4d6f590a7d11b0509.scope ## OH NO, I\u0026#39;M IN THE MATRIX ","date":"2020-03-13T22:43:32+08:00","image":"https://tab.deoops.com/posts/container-pid/the-matrix_huca438541ad881ab92ee8c66601a20268_121541_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/container-pid/","title":"容器PID"},{"content":"昨天同事调试前端页面分页功能时， 发现了一个分页的问题。\n问题简要描述如下：\n前端选择一些过滤条件（a\u0026amp;\u0026amp;b\u0026amp;\u0026amp;c||d \u0026hellip;）向后端请求数据，过一会发现用同样的过滤条件去查询，数据变少了，前端看上去第一页和最后一页是一样的。\n初步怀疑是分页出了问题。\n这个分页的问题比较麻烦，不能稳定复现，一会出现一会又不出现。\n分析了很长一段时间后，发现是后台的定时任务更新了db数据使得很多数据不再符合前面的过滤条件，后端框架返回的总页码数，和data的数量不符。\n也就是说后端返回的总页码数是脏/旧数据。\n脏数据 查看后端框架代码时候，发现后端查询db的执行了count 和 select 两条query：\nselect count(*) form table_name where condition_a -- meamwhile other workers update table_a -- A LOT in short time -- or they(the workers) MIGHT lock -- the WHOLE table for READ select * form table_name where condition_a 这就是是造成脏数据的原因。\n解决方案 window function 可以用 window function 做到上述两条query的同时查询（其实就是一条查询）：\nSELECT *, count(*) OVER() AS full_count FROM tbl WHERE -- /* whatever */ ORDER BY col1 LIMIT ? OFFSET ? Common Table Expressions (cte) However, as Dani pointed out, when OFFSET is at least as great as the number of rows returned from the base query, no rows are returned. So we also don\u0026rsquo;t get full_count.\nIf that\u0026rsquo;s not acceptable, a possible workaround to always return the full count would be with a CTE and an OUTER JOIN:\n参考Run a query with a LIMIT/OFFSET and also get the total number of rows\nWITH cte AS ( SELECT * FROM tbl WHERE -- /* whatever */ ) SELECT * FROM ( TABLE cte ORDER BY col1 LIMIT ? OFFSET ? ) sub RIGHT JOIN (SELECT count(*) FROM cte) c(full_count) ON true; ","date":"2020-03-02T11:00:52+08:00","image":"https://tab.deoops.com/posts/pagination-wf/available-paginations_hu8c8c5c52595cd1807af80028fe6ed5c0_344112_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/pagination-wf/","title":"准确的分页"},{"content":"今天在hacker news上看到 wireguard macos client 发布了，决定试用一下。\n和所有的vpn安装一样，wireguard的安装也是分两步，一是安装vpn server，二是安装 vpn的client。 安装不分先后，配置先配置server，然后在配置client。\n服务端 安装 服务器为 RHEL 7.6 (Maipo)， 服务端的安装流程:\n#!/bin/bash sudo -i [root@deoops ~]# cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.6 (Maipo) [root@deoops ~]# echo \u0026#34;net.ipv4.ip_forward = 1\u0026#34; \u0026gt;\u0026gt; /etc/sysctl.conf [root@deoops ~]# sysctl -p ### install packages [root@deoops ~]# curl -Lo /etc/yum.repos.d/wireguard.repo https://copr.fedorainfracloud.org/coprs/jdoss/wireguard/repo/epel-7/jdoss-wireguard-epel-7.repo [root@deoops ~]# yum install -y epel-release wireguard-dkms wireguard-tools [root@deoops ~]# yum install -y epel-release [root@deoops ~]# rpm -ivh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm [root@deoops ~]# yum update -y [root@deoops ~]# yum install -y epel-release wireguard-dkms wireguard-tools [root@deoops ~]# init 6 配置 ### wireguard server conf [root@deoops ~]# cat wg.conf [Interface] ListenPort = 58855 PrivateKey = private_key [Peer] PublicKey = public_key_one #AllowedIPs = 0.0.0.0/0 AllowedIPs = 10.0.0.7/32 [Peer] PublicKey = public_key_two #AllowedIPs = 0.0.0.0/0 AllowedIPs = 10.0.0.9/32 启动wg0 设备 记得加上iptables设置：\n### start wg0 device [root@deoops ~]# cat start-wireguard.sh ip l a dev wg0 type wireguard ip a a dev wg0 10.0.0.1/24 wg setconf wg0 wg.conf ip l set up dev wg0 iptables -A FORWARD -i wg0 -j ACCEPT; iptables -A FORWARD -o wg0 -j ACCEPT; iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE 客户端 下载 在app store可以直接下载wireguard 客户端。\n如果是中国大陆的app store， 则需要修改Apple ID的国家和地区才能下载wireguard客户端。\n配置 注意 [Peer]的 Endpoint 和服务器端的[Interface]对上（都是58855端口)：\n[Interface] PrivateKey = private_key ListenPort = 54123 Address = 10.0.0.9/32 DNS = 8.8.8.8, 1.1.1.1, 1.0.0.1, 8.8.4.4 [Peer] PublicKey = server_public_key AllowedIPs = 0.0.0.0/0, ::/0 Endpoint = server_address:58855 PersistentKeepalive = 30 折腾调试 安装完成后，可能会遇到vpn不通的问题，可以安装下面介绍的 udp port troubleshoot方法调试调试：\n# nc on client to scan, and tcpdump on server side ### client side ➜ ~ nc -vz -u server 58885 ## the -z option to perform a scan instead of attempting to initiate a connection. ### server side [root@deoops ~]# yum install tcpdump [root@deoops ~]# tcpdump -i eth0 udp port 58855 -vv -X 后记 体验一天，大体说来，wireguard比shadowsocks 速度快上6到8倍。\nkernel update 当我们对主机执行升级Linux kernel操作之后，需要重新load wireguard mod，否则 ip link add ...wg0的时候会报错。\n这个时候 删除旧的 dkms mod， 然后add新的wireguard mod即可：\ndkms status ls /var/lib/dkms/wireguard/0.0.20191206/ ls -alh /var/lib/dkms/wireguard/0.0.20191206/ ls -alh /var/lib/dkms/wireguard/0.0.20200105/ rm -rf /var/lib/dkms/wireguard/0.0.20191206 ls -alh /var/lib/dkms/wireguard/ rm -rf /var/lib/dkms/wireguard/kernel-4.18.0-80.11.2.el8_0.x86_64-x86_64 dkms status dkms add -m wireguard/0.0.20200105 dkms status vi /etc/wireguard/wg0.conf wg-quick up /etc/wireguard/wg0.conf ","date":"2020-01-06T16:50:26+08:00","image":"https://tab.deoops.com/posts/try-wireguard/site-to-site-complex.svg","permalink":"https://tab.deoops.com/posts/try-wireguard/","title":"安装wireguard"},{"content":"update: more user friendly #bin/bash mustBeRoot() { if [ \u0026#34;$(id -u)\u0026#34; != \u0026#34;0\u0026#34; ]; then echo \u0026#34;只有root用户才能运行\u0026#34; 1\u0026gt;\u0026amp;2 echo \u0026#34;当前登录用户`whoami`\u0026#34; exit 1 fi } # 数据盘挂载 checkAndMountDataDisk() { echo \u0026#34;选择数据盘/分区 ：\u0026#34; fdisk -l | grep /dev | grep G | cut -f 1 -d , echo \u0026#34; \u0026#34; read -p \u0026#34;请输入硬盘/分区 名：/dev/\u0026#34; -r disk_name disk_id=`blkid | grep $disk_name` if [ $? -ne 0 ];then echo \u0026#34;获取硬盘/分区 $disk_name uuid失败，请检查名称是否准确\u0026#34; exit 1 fi disk_path=`echo $disk_id | cut -f 1 -d \u0026#39;:\u0026#39; ` echo \u0026#34;已选择 $disk_path\u0026#34; disk_uuid=`blkid $disk_path | cut -f 2 -d \u0026#39;\u0026#34;\u0026#39; ` disk_info=`lsblk -f $disk_path | grep $disk_uuid ` if [ $? -ne 0 ];then echo \u0026#34;获取硬盘/分区 $disk_path 详细信息失败，请检查名称是否正确\u0026#34; exit 1 fi fs_type=`echo $disk_info | cut -f 2 -d \u0026#39; \u0026#39;` if [ $fs_type != \u0026#39;ext4\u0026#39; ];then echo \u0026#34;硬盘文件系统格式不是ext4。\u0026#34; read -p \u0026#34;是否格式化为 ext4？ 输入 y 同意格式化\u0026#34; -r format_ext4 if [ $format_ext4 != \u0026#39;y\u0026#39; ];then echo \u0026#34;未格式化 $disk_path 文件系统格式，退出安装脚本。\u0026#34; exit 1 fi echo \u0026#34;即将格式化 $disk_path 文件系统格式....\u0026#34; mkfs.ext4 $disk_path fi if [ -d \u0026#34;$1\u0026#34; ];then echo \u0026#34;数据盘挂载点/data目录已存在\u0026#34; else echo \u0026#34;创建数据盘挂载点/data目录\u0026#34; mkdir $1 fi mount UUID=$disk_uuid $1 echo \u0026#34;UUID=$disk_uuid $1 ext4 defaults 0 0\u0026#34; \u0026gt;\u0026gt; /etc/fstab echo \u0026#34;硬盘挂载成功\u0026#34; } checkDataMountpoint() { echo \u0026#39;挂载数据盘\u0026#39; grep \u0026#34;$1\u0026#34; /etc/fstab | grep ext4 if [ $? -ne 0 ];then checkAndMountDataDisk else echo \u0026#39;数据盘已挂载\u0026#39; mount -a fi cpAndUntar } mustBeRoot checkDataMountpoint ${1-/data} lsblk 首先使用lsblk查看当前系统硬盘挂载的情况\n[root@dev-7 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT sr0 11:0 1 1024M 0 rom vda 253:0 0 40G 0 disk └─vda1 253:1 0 40G 0 part / vdb 253:16 0 200G 0 disk └─vdb1 253:17 0 200G 0 part format and mount 格式化硬盘然后挂载到 /media/newdrive 目录\n### disk_add_new.sh function set_vars() { fs_ty=ext4 mount_point=/media/newdrive #check=`fdisk -l | grep GB | cut -d \u0026#39;:\u0026#39; -f1 | cut -c 6-` #new_disk=`echo $check | cut -d \u0026#39; \u0026#39; -f 2` new_disk=$1 # new_disk=${check##*\\ } } function make_file_system(){ mkfs -t $fs_ty $new_disk } function get_uuid() { uuid_info=`blkid $new_disk` uuid=${uuid_info#*UUID} id=`echo $uuid | cut -d \u0026#39;\u0026#34;\u0026#39; -f 2` } function mount_and_auto_mount() { mkdir -p $mount_point mount -t $fs_ty $new_disk $mount_point grep $id /etc/fstab || echo UUID=$id $mount_point $fs_ty defaults 0 0 \u0026gt;\u0026gt; /etc/fstab } function run() { set_vars $1 make_file_system get_uuid [ ${#id} -gt 30 ] || exit 1 mount_and_auto_mount } run $1 扩容 mount_point=/media/newdrive # check=`fdisk -l | grep GB | cut -d \u0026#39;:\u0026#39; -f1 | cut -c 6-` # disk=`echo $check | cut -d \u0026#39; \u0026#39; -f 2` disk=$1 # /dev/sdb sdc fs_ty=ext4 e2fsck -f $disk resize2fs $disk mount -t $fs_ty $disk $mount_point 逻辑卷 创建逻辑卷 pvcreate /dev/vdb vgcreate vg02 /dev/vdb lvcreate -l 90%free -n kubelet vg02 mkfs.xfs -n ftype=1 /dev/vg02/kubelet mount /dev/vg02/kubelet /var/lib/kubelet echo \u0026#34;/dev/mapper/vg02-kubelet /var/lib/kubelet xfs defaults 0 0 \u0026#34; \u0026gt;\u0026gt; /etc/fstab 扩容逻辑卷 # 1. 删除未挂载的逻辑卷 lvremove vg02/kubelet # lvremove vg02 # 2. 删除kubelet的磁盘 vgreduce vg02 /dev/vdb # vgremove vg02 # 3. 将原来的磁盘加到别的vg vgextend vg01 /dev/vdb # 4. 扩容逻辑卷 lvextend -L +200G /dev/vg01/var # 5. 生效 xfs_groups /dev/vg01/var # 6. 删除/etc/fstab 中kubelet的挂载点 ## vim /etc/fstab +5dd troubleshoot 有时候删除逻辑卷会遇到报错Logical volume vg01/LVgdb contains a filesystem in use\n[root@cnsz92vl14311 ~]# umount /dev/vg01/LVgdb [root@cnsz92vl14311 ~]# lvremove /dev/vg01 Do you really want to remove active logical volume LVgdb? [y/n]: y Logical volume \u0026#34;LVgdb\u0026#34; successfully removed [root@cnsz92vl14311 ~]# lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT vda 252:0 0 60G 0 disk ├─vda1 252:1 0 476M 0 part /boot └─vda2 252:2 0 59.5G 0 part ├─vg00-root 253:0 0 10G 0 lvm / ├─vg00-home 253:1 0 500M 0 lvm /home ├─vg00-var 253:2 0 10G 0 lvm /var ├─vg00-tmp 253:3 0 10G 0 lvm /tmp └─vg00-app 253:4 0 29G 0 lvm /app vdb 252:16 0 300G 0 disk [root@cnsz92vl14311 ~]# ","date":"2019-10-18T10:03:07+08:00","image":"https://tab.deoops.com/posts/load-disk/disk_huc96de7ee3c2e159b1393298e71eecb80_142875_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/load-disk/","title":"挂载硬盘"},{"content":"docker run调试某个container报如下所示x509证书错误，一开始怀疑是容器网络（--network host) 的问题 :\n[deoops@dev-3 ~]# docker run --network host datewu/controller:v0.0.2 {\u0026#34;level\u0026#34;:\u0026#34;panic\u0026#34;,\u0026#34;error\u0026#34;:\u0026#34;Get https://google.com: x509: certificate signed by unknown authority\u0026#34;,\u0026#34;time\u0026#34;:1555498448,\u0026#34;message\u0026#34;:\u0026#34;get max item failed\u0026#34;} panic: get max item failed goroutine 26 [running]: github.com/rs/zerolog.(*Logger).Panic.func1(0x7773e9, 0x13) /Users/deoops/go/pkg/mod/github.com/rs/zerolog@v1.13.0/log.go:307 +0x4f github.com/rs/zerolog.(*Event).msg(0xc00012e8a0, 0x7773e9, 0x13) /Users/deoops/go/pkg/mod/github.com/rs/zerolog@v1.13.0/event.go:141 +0x1c1 github.com/rs/zerolog.(*Event).Msg(...) /Users/deoops/go/pkg/mod/github.com/rs/zerolog@v1.13.0/event.go:105 main.catchUp() /Users/deoops/github/controller/work.go:69 +0x326 main.populate(0xc000114000) /Users/deoops/github/controller/worker.go:10 +0x26 created by main.initWork /Users/deoops/github/controller/work.go:84 +0x7f 错误信息大概是说 client 不能识别google的https 证书， 可能是base image alpine的问题。\n将base image改为 scratch ，结果还是会报x509错。\n解决方案 给 alpine镜像 加上ca-certificates解决了问题:\nFROM alpine # add Common CA certificates PEM files RUN apk --no-cache add ca-certificates # .... # docker build -t datewu/alpine-ca . 以后用到alpine的Dockerfile 直接 FROM datewu/alpine-ca 问题解决啦😄\n总结一下：当容器里的进程访问外部tls server时，如果容器内没有配置Common CA certificates，客户端就会出现无法识别server证书的问题。\nps: 今天在写字楼二楼快餐店吃晚饭的时候，遇到两个建筑工人，50-60岁的样子，像是夫妻。\n两个人一起打才吃了10块钱：一人一个素菜，豆芽菜和黑油白。\npps：快餐店下午的汤是免费的，所以他们一人又拿了一碗汤，\n回想起今天我过早才就吃了10块钱，嗟乎唏嘘。\n","date":"2019-09-12T10:11:32+08:00","image":"https://tab.deoops.com/posts/container-ca/common-ca_hu249b34a0b606f672af7bc796fcfc189a_82371_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/container-ca/","title":"证书问题"},{"content":"今天准备管理某一个kubernetes 集群时发现master主机22端口因为管理的需要被禁用了，无法登陆服务器。\n问了一下运维人员，原来是基于安全原因，公司决定禁用所有服务器的root ssh登陆权限，\n平时我都是ssh 登陆到master node，在服务器上直接使用kubectl命令 查看/部署/debug deployment/service等资源，\n现在只好修改下本地 kubeconfig 文件，用自己本地的 kubectl 管理/操作kubernetes集群。\n操作了一段时间后，发现用本地kubectl操作kubernetes体验蛮好的，特别是服务器缺少本地editor(vim) kubectl edit ... 的语法高亮支持。\n配置kubeconfig过程分享如下，大体上说过就两步：\n添加 context； use context。 vim .kube/config kubectl config use-context dev-8-admin@kubernetes 除了使用vim 编辑 .kube/config 文件，对于一些简单的配置也可以使用kubectl config command 快速配置kubeconfig：\n## create new cluster kubectl config set-cluster NAME [--server=server] [--certificate-authority=path/to/certificate/authority] [--insecure-skip-tls-verify=true] ## create new user kubectl config set-credentials NAME [--client-certificate=path/to/certfile] [--client-key=path/to/keyfile] [--token=bearer_token] [--username=basic_user] [--password=basic_password] [--auth-provider=provider_name] [--auth-provider-arg=key=value] [options] ## create new context kubectl config set-context [NAME | --current] [--cluster=cluster_nickname] [--user=user_nickname] [--namespace=namespace] [options] ## use context kubectl config use-context CONTEXT_NAME [options] 另外，kubectl config set 不支持对 certificate-authority-data字段的设置，只支持指定data文件的路径， 所以推荐用vim 编辑kubeconfig文件。\nkuebconfig demo 下面是一份配置好的 kubeconfig demo文件，供参考: （略去了client 的证书, private key等敏感信息）\napiVersion: v1 clusters: - cluster: certificate-authority-data: YOUR kubernete CA BASE64 DATA server: https://test-8:9443 name: dev-8 - cluster: certificate-authority: /Users/r/.minikube/ca.crt server: https://192.168.99.117:8443 name: minikube contexts: - context: cluster: dev-8 user: dev-8-admin name: dev-8-admin@kubernetes - context: cluster: minikube user: minikube name: minikube current-context: test-8-admin@kubernetes kind: Config preferences: {} users: - name: dev-8-admin user: client-certificate-data: YOUR client CERT BASE64 DATA client-key-data: YOUR client PRIVATE KEY DATA - name: minikube user: client-certificate: /Users/r/.minikube/client.crt client-key: /Users/r/.minikube/client.key kubecofig 字段解释 kubeconfig是什么，以及上面的编辑过程到底做了什么：\nkubeconfig 是一个配置文件，这个配置文件使用了 yaml 文件的语法，主要记录了kubernetes 集群的认证信息。\nkubectl 或者是其它的kubernetes client通过解析kubeconfig文件得到kubernetes cluster的认证信息，从而使用/管理 kubernetes 集群。\n安装kubectl的时候，~/.kube/config文件就是一个kubeconfig，而且可以配置管理多个kubernetes集群。\nkubeconfig的可读性很高，建议仔细通读下。\n一个kubeconfig文件包含了3个对象， 一种关系:\n一个 kubeconfg 可以包含 多个 context，多个 cluster， 多个 user。 每个 context 由一个cluster 和 user 组成。\ncluster由 apiserver地址， server ca 信息，和cluster name 组成。\nuser (user 的定义其实就是 kubernetes client 的 auth 定义) 的定义相对灵活些：\n可以是我上面贴出来的 certificate 认证信息； 也可以是 username/password 的认证信息(k3s 的kubeconfig 就是用的这种)； 或者是用serviceAccount 的 token认证也可以， 总的说来一份auth 信息 和 一个name 对应了一个user 的定义。\n大家应该注意到了 cluster 和 user 的组合可以很灵活，从而产生多种 context。\nclient-go操作kubecofig marshal 把 restclient.Config 编码保存到文件：\npackage main import ( \u0026#34;clientcmdapi \u0026#34;k8s.io/client-go/tools/clientcmd/api\u0026#34; \u0026#34;k8s.io/client-go/tools/clientcmd\u0026#34; ) // 节选，有删减 func snipedDumpKubeconfig() error { // ... globalKubeCfg := clientcmdapi.NewConfig() globalKubeCfg.Clusters[clusterName] = clientcmdapi.NewCluster() globalKubeCfg.Clusters[clusterName].Server = config.Host globalKubeCfg.Clusters[clusterName].CertificateAuthorityData = config.CAData globalKubeCfg.AuthInfos[authName] = clientcmdapi.NewAuthInfo() globalKubeCfg.AuthInfos[authName].ClientCertificateData = config.CertData globalKubeCfg.AuthInfos[authName].ClientKeyData = config.KeyData globalKubeCfg.Contexts[contextName] = clientcmdapi.NewContext() globalKubeCfg.Contexts[contextName].Cluster = clusterName globalKubeCfg.Contexts[contextName].AuthInfo = authName return clientcmd.WriteToFile(*globalKubeCfg, globalKubeconfFile) } unmarshal 由 kubeconfig文件生成 restclient.Config： 在官方的 k8s.io/client-go/tools/clientcmd 包中可以找到example：\n// RESTConfigFromKubeConfig is a convenience method to give back a restconfig from your kubeconfig bytes. // For programmatic access, this is what you want 80% of the time func RESTConfigFromKubeConfig(configBytes []byte) (*restclient.Config, error) { clientConfig, err := NewClientConfigFromBytes(configBytes) if err != nil { return nil, err } return clientConfig.ClientConfig() } // BuildConfigFromFlags is a helper function that builds configs from a master // url or a kubeconfig filepath. These are passed in as command line flags for cluster // components. Warnings should reflect this usage. If neither masterUrl or kubeconfigPath // are passed in we fallback to inClusterConfig. If inClusterConfig fails, we fallback // to the default config. func BuildConfigFromFlags(masterUrl, kubeconfigPath string) (*restclient.Config, error) { if kubeconfigPath == \u0026#34;\u0026#34; \u0026amp;\u0026amp; masterUrl == \u0026#34;\u0026#34; { klog.Warningf(\u0026#34;Neither --kubeconfig nor --master was specified. Using the inClusterConfig. This might not work.\u0026#34;) kubeconfig, err := restclient.InClusterConfig() if err == nil { return kubeconfig, nil } klog.Warning(\u0026#34;error creating inClusterConfig, falling back to default config: \u0026#34;, err) } return NewNonInteractiveDeferredLoadingClientConfig( \u0026amp;ClientConfigLoadingRules{ExplicitPath: kubeconfigPath}, \u0026amp;ConfigOverrides{ClusterInfo: clientcmdapi.Cluster{Server: masterUrl}}).ClientConfig() } ","date":"2019-08-11T22:09:21+08:00","image":"https://tab.deoops.com/posts/explain-kubeconfig/kubeconfig_hu6b4ad2ec2bfdd2b8d356f680088de3f4_179870_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/explain-kubeconfig/","title":"细说kubeconfig"},{"content":"问题 用 kubeadm 部署 kubernetes 集群，启动kubelet服务后，kubelet daemon 会认为 /etc/sysconfig/kubelet内容的优先级更高，\n覆盖KUBELET_EXTRA_ARGS环境变量--pod-infra-container-image的配置内容。\n分析 kubelet.service文件 首先查看/etc/systemd/system/kubelet.service文件:\n# cat /etc/systemd/system/kubelet.service [Unit] Description=kubelet: The Kubernetes Node Agent Documentation=https://kubernetes.io/docs/ [Service] ExecStart=/usr/bin/kubelet Restart=always StartLimitInterval=0 RestartSec=10 [Install] WantedBy=multi-user.target 注意第5行的ExecStart指令为/usr/bin/kubelet\nkubelet.service.d 目录 然后查看kubelet服务的配置目录/etc/systemd/system/kubelet.service.d/，注意 kubeadm会在这里创建一个10-kubeadm.conf文件来配置kubelet启动方式（参数）。\n# cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf # Note: This dropin only works with kubeadm and kubelet v1.11+ [Service] Environment=\u0026#34;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf\u0026#34; Environment=\u0026#34;KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml\u0026#34; # This is a file that \u0026#34;kubeadm init\u0026#34; and \u0026#34;kubeadm join\u0026#34; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env # This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use # the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file. EnvironmentFile=-/etc/sysconfig/kubelet ExecStart= ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS 配置文件的load的顺序是以字母顺序排列的， 如果我们也在这里创建了kubelet的配置文件，比如20-my-kubelet.conf，10-*会先被读取，20-*其次。\n注意10-kubeadm.conf文件的倒数第二行ExecStart=空行，这一行等效于kubeadm覆盖了上面kubelet.service 的ExecStart指令！\n结论 [Service]覆盖和充值特性。\nExecStart参数特性：如果已经设置ExecStart指令参数， 后续ExecStart指令会叠加所有的参数，顺序apply所有指令。\n如果想要清空前面的ExecStart指令，则需要写一行空的ExecStart=,然后写新的ExecStart=\u0026hellip;\u0026hellip;指令。\n同理KUBELET_EXTRA_ARGS也会有覆盖和清空的语法。\n目前我们/etc/systemd/system/kubelet.service.d/20-my-kubelet.conf文件是这样的：\n[Service] Environment=\u0026#34;KUBELET_EXTRA_ARGS=--pod-infra-container-image=172.20.8.4/library/pause:3.1 --feature-gates=LocalStorageCapacityIsolation=true \\ --kube-reserved-cgroup=/kubepods.slice --kube-reserved=cpu=500m,memory=500Mi,ephemeral-storage=1Gi \\ --system-reserved-cgroup=/system.slice --system-reserved=cpu=500m,memory=500Mi,ephemeral-storage=1Gi \\ --eviction-hard=memory.available\u0026lt;500Mi,nodefs.available\u0026lt;10%\u0026#34; 表面上看 20-my-kubelet.conf追加了KUBELET_EXTRA_ARGS环境变量，应该是成功的配置了--pod-infra-container-image参数，\n但是在10-kubeadm.conf文件里已经定义EnvironmentFile=-/etc/sysconfig/kubelet，然后 在/etc/sysconfig/kubelet里有一句KUBELET_EXTRA_ARGS=，等于号后面无值，清空了KUBELET_EXTRA_ARGS环境变量。\n解决方案 删除20-my-kubelet.conf文件，直接在/etc/systemconf/kubelet中定义KUBELET_EXTRA_ARGS参数定义：\n# cat /etc/systemconf/kubelet #KUBELET_EXTRA_ARGS= KUBELET_EXTRA_ARGS=--pod-infra-container-image=192.168.9.20/library/pause:3.1 --feature-gates=LocalStorageCapacityIsolation=true \\ --kube-reserved-cgroup=/kubepods.slice --kube-reserved=cpu=500m,memory=500Mi,ephemeral-storage=1Gi \\ --system-reserved-cgroup=/system.slice --system-reserved=cpu=500m,memory=500Mi,ephemeral-storage=1Gi \\ --eviction-hard=memory.available\u0026lt;500Mi,nodefs.available\u0026lt;10% 修改完配置文件后，执行kubeadm init可以看到 pod-infra-contrainer-image配置生效了。\n参考systemd config\n","date":"2019-05-11T16:02:39+08:00","image":"https://tab.deoops.com/posts/systemd-conf/Systemd_components.svg","permalink":"https://tab.deoops.com/posts/systemd-conf/","title":"Systemd配置问题"},{"content":"前段时间的花了很多功夫对接k8s和openstack的kuryr-kubernetes网路组件。 学到了很多openstack的知识，今天抽出时间来整理下。\nclient 首先是 install openstack-cli neutron client：\n#!/bin/bash [root@deoops ~]# cat /etc/redhat-release Red Hat Enterprise Linux Server release 7.5 (Maipo) #### add openstack yum repo source [root@deoops ~]# vi /etc/yum.repos.d/openstack.repo [root@deoops ~]# yum install -y python2-openstackclient openstack-neutron [root@deoops shells]# cat source export OS_PROJECT_DOMAIN_NAME=default export OS_USER_DOMAIN_NAME=default export OS_PROJECT_NAME=your_project_name export OS_USERNAME=your_use_name export OS_PASSWORD=your_pwd export OS_AUTH_URL=http://10.8.1.3:35357/v3 export OS_IDENTITY_API_VERSION=3 export OS_IMAGE_API_VERSION=2 vip 我们来创建一个virtual IP验证上一步配置的openstack source对不对 ：\n创建 vip 对应的port; 把 上一步创建好的port加入到 vm ip对应port 的 allow-address-pairs 属性中; [root@deoops shells]# cat vip.sh . ./source network=your_network_id ### i\u0026#39;ve comment out the create Port operation #subnet=you_subnet_id #for i in {62..64} #do # echo $i # neutron port-create --fixed-ip subnet_id=$subnet,ip_address=10.0.1.$i --device-owner \u0026#39;Virtual IP\u0026#39; --no-security-groups --name \u0026#39;Virtual IP\u0026#39; $network #done # openstack port list | grep -E \u0026#39;10.0.1.6(2|3|4)\u0026#39; | cut -d \u0026#39;|\u0026#39; -f 4,5 #p1=mac_address=fa:16:3e:aa:6b:68,ip_address=\u0026#39;10.0.1.64\u0026#39; #p2=mac_address=fa:16:3e:1d:11:2d,ip_address=\u0026#39;10.0.1.62\u0026#39; #p3=mac_address=fa:16:3e:b8:47:f9,ip_address=\u0026#39;10.0.1.63\u0026#39; p1=ip_address=\u0026#39;10.0.1.64\u0026#39; p2=ip_address=\u0026#39;10.0.1.62\u0026#39; p3=ip_address=\u0026#39;10.0.1.63\u0026#39; p4=ip_address=\u0026#39;10.0.1.80\u0026#39; #for p in `neutron port-list --device-owner compute:nova -f value | grep -E \u0026#39;10.0.1.5(1|2|3)\u0026#39; | cut -d \u0026#39; \u0026#39; -f 1 `; for p in `neutron port-list --device-owner compute:nova -f value | grep -E \u0026#39;10.0.1\u0026#39; | cut -d \u0026#39; \u0026#39; -f 1 `; do echo $p $p1 $p2 $p3 $p4 ### must NOT set virtual ip Port macaddress ### leave it empty to use host port macaddress neutron port-update $p --allowed-address-pair $p1 --allowed-address-pair $p2 --allowed-address-pair $p3 --allowed-address-pair $p4 done create port 申请创建port\n[root@deoops shells]# cat create-port.sh #!/bin/bash . ./source network=your_network_id subnet=your_subnetwork_id function setup { #i=$1 #ip_addr=10.0.1.$((i+1)) echo \u0026#34;going to create ip ...\u0026#34; #neutron port-create --fixed-ip subnet_id=$subnet,ip_address=$ip_addr --device-owner \u0026#39;compute:kuryr\u0026#39; --no-security-groups --name \u0026#39;concurrent load test IP\u0026#39; $network neutron port-create subnet_id=$subnet --device-owner \u0026#39;compute:kuryr\u0026#39; --no-security-groups --name \u0026#39;concurrent load test IP lll\u0026#39; $network } for i in `seq $1`; do time setup $i \u0026amp; done update allow-address-pairs 批量update allow-address-pairs:\n[root@deoops shells]# cat allow-address-pairs-load-test.sh . ./source vm_port=vm_port_id function setup { i=$1 mac=$(echo $[RANDOM%10]$[RANDOM%10]:$[RANDOM%10]$[RANDOM%10]:$[RANDOM%10]$[RANDOM%10]) ip_addr=10.0.1.$((i+1)) param+=\u0026#34; --allowed-address-pair ip_address=${ip_addr},mac_address=${mac}\u0026#34; #echo $vm_port $param neutron port-update $vm_port $param } echo starting for i in `seq $1`; do ## for i in `jot $1`; do echo \u0026#34;$i...\u0026#34; time setup $i done delete port \u0026amp; allowed-address-pairs 清理 Port 和 Port allowed-address-pairs\n#!/bin/bash . ./source #neutron port-list --device-owner compute:kuryr neutron port-delete `neutron port-list --device-owner compute:kuryr -c id -f value` neutron port-list --device-owner compute:nova | grep 10.0.1 | grep -vE \u0026#39;10.0.1.5(1|2|3)\u0026#39; for p in `neutron port-list --device-owner compute:nova -f value | grep 10.0.1 | cut -d \u0026#39; \u0026#39; -f 1 `; do neutron port-update $p --allowed-address-pairs action=clear ; done trunk port 创建trunk 子port\nfor i in `openstack port list | grep ACTIVE |grep -vE \u0026#39;(22.1.104|1.106|132|117|129)\u0026#39; | awk \u0026#39;{print $2}\u0026#39;`; do openstack network trunk unset --subport $i trunktest; done for i in `openstack port list | grep DOWN | awk \u0026#39;{print $2}\u0026#39;`; do openstack port delete $i ; done ############################################ openstack network create —share —external —provider-physical-network provider —provider-network-type vlan —provider-segment 162 podlan162 —transparent-vlan openstack subnet create —no-dhcp —subnet-range 172.23.0.0/16 —gateway 172.23.0.254 —network podlan162 —allocation-pool start=172.23.1.101,end=172.23.1.201 —dns-nameserver 114.114.114.114 jyvlan162sub2 openstack port create —network podlan162 —fixed-ip subnet=jyvlan162sub2,ip-address=172.23.1.129 —project admin v162port # openstack trunk 端口加入 openstack network trunk create —parent-port v162port trunktest openstack network create —share —external —provider-physical-network provider —provider-network-type vlan —provider-segment 163 podlan163 —transparent-vlan openstack subnet create —no-dhcp —subnet-range 172.24.0.0/16 —gateway 172.24.0.254 —network podlan163 —allocation-pool start=172.24.1.101,end=172.24.1.201 —dns-nameserver 114.114.114.114 jyvlan164sub2 openstack port create —network podlan163 —fixed-ip subnet=jyvlan164sub2,ip-address=172.24.1.129 —project admin v163port openstack network trunk set trunktest —subport port=v163port,segmentation-type=vlan,segmentation-id=163 # 测试子网，测试163 tag sudo ip link add link eth0 name eth0.163 type vlan id 163 sudo ip link set dev eth0.163 address fa:16:3e:26:a8:08 sudo ip link set dev eth0.163 up ","date":"2019-04-21T18:39:27+08:00","image":"https://tab.deoops.com/posts/neutron-port/Neutron-Networking-CompNet-v1_huba0c25ee44e1cb025e6838f746b91615_235376_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/neutron-port/","title":"Neutron小记"},{"content":"update: ioutil逐渐被io 取代。\npackage ioutil // import \u0026#34;io/ioutil\u0026#34; func NopCloser(r io.Reader) io.ReadCloser NopCloser returns a ReadCloser with a no-op Close method wrapping the provided Reader r. As of Go 1.16, this function simply calls io.NopCloser. package io // import \u0026#34;io\u0026#34; func NopCloser(r Reader) ReadCloser NopCloser returns a ReadCloser with a no-op Close method wrapping the provided Reader r. 最近使用baloo写集成测试，遇到了个需求， 在unmarshalrespones之后（或者之前）还要再输出一次response的纯文本格式供debug参考。\n即需要多次读http.Resp.Body。\n问题 response.Body 只能读一次，读完之后再进行read操作就会遇到EOF error。\n分析问题 模糊记得baloo在一次请求中能多次(JSON() 和 String())读取response.Body内容。\n仔细去看了下baloo的源代码，发现baloo自己在内部 封装了一个对象 http.RawResonse ，使用了 iouti.NopCloser函数重新填充了res.Body:\nfunc readBodyJSON(res *http.Response) ([]byte, error) { body, err := ioutil.ReadAll(res.Body) if err != nil { return []byte{}, err } // Re-fill body reader stream after reading it res.Body = ioutil.NopCloser(bytes.NewBuffer(body)) return body, err } 解决方案 有了 ioutil.NopCloser函数，可以很快速的写出debugPlugin：\nfunc debugPlugin() (p plugin.Plugin) { f := func(ctx *context.Context, h context.Handler) { res, err := ioutil.ReadAll(ctx.Response.Body) fmt.Println(\u0026#34;response:\u0026#34;, string(res), err, \u0026#34;response Type:\u0026#34;, ctx.Response.Header.Get(\u0026#34;Content-Type\u0026#34;)) // should CLOSE, due to next Close method will be no-op ctx.Response.Body.Close() ctx.Response.Body = ioutil.NopCloser(bytes.NewBuffer(res)) h.Next(ctx) } p = plugin.NewResponsePlugin(f) return } 注意：应该先 resp.Body.Close() 掉，然后重新填充reader。\n因为新的Response.Body.Close()是 no-op操作，不会去close 之前的Body，可能会造成资源泄漏。\n","date":"2019-04-17T21:39:31+08:00","image":"https://tab.deoops.com/posts/read-again/nopcloser_hu50a7be3079bf16d477751bec8b519605_85339_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/read-again/","title":"nopCloser函数"},{"content":"客户反馈我们的产品有个很奇怪的问题。\n添加完商品后，可以看到商品，但是一刷新页面，刚才添加的商品就消失啦。\n以前没碰到过，一直都用的好好的为什么今天就不行了呢？\n分析问题 既然一刷新即消失了，就证明没有写入到数据库。\n没写入到数据库是什么原因呢？ API 也没有报错呀？\n更加奇怪的是，有的页面有这个问题，有的没有这个问题。\n后端的API 有的是golang写的，有的是Java写的。\ngolang orm对mysql 数据库的写操作存在上面说的刷新就丢失的问题，Java的orm对mysql的写操作没有问题。\n这是为什么呢？\nDBA 排查了很久发现原来是客户那边的DBA把 mysql的 session autocommit的配置关闭啦。\nautocommit\n翻了下文档，确定　Java的orm框架会忽略mysql server的配置默认自己commit，golang的orm则没有这个优化（也许是有但我们没有启用？）。\n所以会出现 java的后端是正常的，golang的后端写不了mysql。 解决方案 客户DBA开启autocommit配置项后，产品业务恢复正常。\n","date":"2019-04-12T10:37:04+08:00","image":"https://tab.deoops.com/posts/mysql-autocommit/Start-ad-hoc-Transaction_hu404d58f8b5b266b7abe493c2dcfba32a_66141_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/mysql-autocommit/","title":"Mysql Autocommit问题"},{"content":"对macvlan 不熟悉的同学，可以先看下这篇macvlan virtual network简介\n默认情况下Linux kernel会阻止(drop)宿主机（host eth0）虚拟出来的 macvlan network（bridge mode） 和宿主机host eth0）之间网络数据包。\n调试了一段时间后，我们发现了可以通过路由表来绕过这个限制。 具体实施的方法如下：\n在host network namesapces下新增 一个macvlan device，然后添加路由规则即可。\n通信的两个方向简单解释如下：\neth0(host) -\u0026gt; pod(macvlan) 宿主机host eth0 通过break0 设备 和route table的路由规则 可以访问到pod（在macvlan中）\nshell调试脚本如下：\nip link add break0 link eth0 type macvlan mode bridge # NOTE: if use /24 CIDR will auto add a route rule # (100.75.30.0/24 dev break0 proto kernel scope link src 100.75.30.1) # which we don\u0026#39;t need ifconfig break0 100.75.30.7/32 up ip r a 100.75.30.71 dev break0 # 100.75.30.71 is a pod ip for test 因为kuryr是用python配置网络的，所以也提供对应的python脚本如下：\nfrom pyroute2 import IPDB def _create_break0(self): with IPDB() as ipdb: i = ipdb.interfaces.get(IF_NAME) if i: i.up().commit() return i.index try: with ipdb.create( ifname=IF_NAME, kind=MACVLAN_KIND, link=ipdb.interfaces[self.parent], macvlan_mode=MACVLAN_MODE_BRIDGE, ) as i: i.add_ip(self.addr + \u0026#34;/32\u0026#34;) i.up() return i.index except ipdb_exceptions.CreateException as e: LOG.info(\u0026#34;Exception when create break0 device %s\u0026#34;, e) return 0 def _route_spec(self, ip): return {\u0026#34;dst\u0026#34;: ip + \u0026#34;/32\u0026#34;, \u0026#34;oif\u0026#34;: self.idx} def add_route(self, ip): spec = self._route_spec(ip) with IPDB() as ipdb: try: ipdb.routes.add(spec).commit() except (ipdb_exceptions.CommitException, link_exceptions.NetlinkError) as e: LOG.info(\u0026#34;Exception %s when add route table \u0026#34; + ip, e) pod -\u0026gt; eth0(host) 因为是2层打通的，所以只要有mac_addr就可以访问，和pod以及宿主机eth0具体的IP 地址没关系。\n在pod 里面使用 arp -n 可以看到，host IP（100.75.30.51） 和 break0 （100.75.30.7） 的IP 对应的是同一个mac addr， 都是 break0的mac addr。\n在主机上添加 路由 ip r add 100.75.30.71 dev break0 （100.75.30.71 是pod 的IP地址），\n然后在 pod 上ping 下 host IP， 宿主机eth0的hostIP 和 break0 的两个条目就会在pod的arp 表里显示出来。\n如果只是从 host ping pod， 则 pod的arp 表只显示 break0 这个条目。\n","date":"2019-03-11T11:33:48+08:00","image":"https://tab.deoops.com/posts/macvlan-route/Macvlan-network-with-traffic-flows_hu61f53e93031289750fcd2806a2b8e722_10345_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/macvlan-route/","title":"Macvlan路由规则"},{"content":"This article offers a sample of basic Markdown syntax that can be used in Hugo content files, also it shows whether basic HTML elements are decorated with CSS in a Hugo theme.\nHeadings The following HTML \u0026lt;h1\u0026gt;—\u0026lt;h6\u0026gt; elements represent six levels of section headings. \u0026lt;h1\u0026gt; is the highest section level while \u0026lt;h6\u0026gt; is the lowest.\nH1 H2 H3 H4 H5 H6 Paragraph Xerum, quo qui aut unt expliquam qui dolut labo. Aque venitatiusda cum, voluptionse latur sitiae dolessi aut parist aut dollo enim qui voluptate ma dolestendit peritin re plis aut quas inctum laceat est volestemque commosa as cus endigna tectur, offic to cor sequas etum rerum idem sintibus eiur? Quianimin porecus evelectur, cum que nis nust voloribus ratem aut omnimi, sitatur? Quiatem. Nam, omnis sum am facea corem alique molestrunt et eos evelece arcillit ut aut eos eos nus, sin conecerem erum fuga. Ri oditatquam, ad quibus unda veliamenimin cusam et facea ipsamus es exerum sitate dolores editium rerore eost, temped molorro ratiae volorro te reribus dolorer sperchicium faceata tiustia prat.\nItatur? Quiatae cullecum rem ent aut odis in re eossequodi nonsequ idebis ne sapicia is sinveli squiatum, core et que aut hariosam ex eat.\nBlockquotes The blockquote element represents content that is quoted from another source, optionally with a citation which must be within a footer or cite element, and optionally with in-line changes such as annotations and abbreviations.\nBlockquote without attribution Tiam, ad mint andaepu dandae nostion secatur sequo quae. Note that you can use Markdown syntax within a blockquote.\nBlockquote with attribution Don\u0026rsquo;t communicate by sharing memory, share memory by communicating. — Rob Pike1\nTables Tables aren\u0026rsquo;t part of the core Markdown spec, but Hugo supports supports them out-of-the-box.\nName Age Bob 27 Alice 23 Inline Markdown within tables Italics Bold Code italics bold code Code Blocks Code block with backticks \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block indented with four spaces \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026quot;en\u0026quot;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; Code block with Hugo\u0026rsquo;s internal highlight shortcode \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Example HTML5 Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;Test\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; List Types Ordered List First item Second item Third item Unordered List List item Another item And another item Nested list Fruit Apple Orange Banana Dairy Milk Cheese Other Elements — abbr, sub, sup, kbd, mark GIF is a bitmap image format.\nH2O\nXn + Yn = Zn\nPress CTRL+ALT+Delete to end the session.\nMost salamanders are nocturnal, and hunt for insects, worms, and other small creatures.\nThe above quote is excerpted from Rob Pike\u0026rsquo;s talk during Gopherfest, November 18, 2015.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"2019-03-11T00:00:00Z","image":"https://tab.deoops.com/posts/markdown-syntax/pawel-czerwinski-8uZPynIu-rQ-unsplash_hud7e36f7e20e71be184458283bdae4646_55974_120x120_fill_q75_box_smart1.jpg","permalink":"https://tab.deoops.com/posts/markdown-syntax/","title":"Markdown语法指南"},{"content":"Mathematical notation in a Hugo project can be enabled by using third party JavaScript libraries.\nIn this example we will be using KaTeX\nCreate a partial under /layouts/partials/math.html Within this partial reference the Auto-render Extension or host these scripts locally. Include the partial in your templates like so: {{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} To enable KaTex globally set the parameter math to true in a project\u0026rsquo;s configuration To enable KaTex on a per page basis include the parameter math: true in content files Note: Use the online reference of Supported TeX Functions\nExamples Block math: $$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n","date":"2019-03-08T00:00:00Z","image":"https://tab.deoops.com/posts/math-typesetting/katex_hudffa2aa6f1e028c3b6c1c9b85d2c1242_33963_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/math-typesetting/","title":"数学符号KaTeX"},{"content":"最近给客户调试 macvlan network时，遇到了Linux kernel 报错 SIOCSIFFKAGS: Device or resource busy. 无法创建网络device。\n结果长时间的debug分析， 发现问题是高并发压测 创建和释放macvlan device的时候，设备的mac address出现了重复。\nps：这个问题只出现在 macvlannetwork 的设备中。\n可以用下面的shell脚本来复现macvlan Device or resource busy的错误：\n#!/bin/bash function setup { i=$1 ip l a m$i address 00:40:2F:4D:5E:6F link eth0 type macvlan mode bridge ip netns add ns$i ip l set m$i netns ns$i sleep 1 ip netns exec ns$i ifconfig m$i 10.0.0.$((i+1))/24 up echo $? } echo cleaning up ip -all netns d echo creating netsnses for i in `seq $1`; do echo $i.... #setup $i \u0026amp; setup $i done 如果把 macvlan 类型改为 dummy (上面脚本第5行 type macvlan 改为 type dummy) ，即使 MAC address 重复也不会引发kernel 报错。\n","date":"2019-03-07T11:17:49+08:00","image":"https://tab.deoops.com/posts/macvlan-busy/maxresdefault_hu0c8b163f503b127644f51a05a3769b8d_73452_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/macvlan-busy/","title":"无法创建macvlan设备"},{"content":"PLEG 不熟悉PLEG(Pod Lifecycle Event Generator)的同学，可以先看下这篇文章What is PLEG?。\n这篇文章对pleg是什么和常见的unhealthy问题有很详细的介绍。\ncni 当k8s的 cni 插件性能较差，node上的pod 数量较多（大于 80）的时候，我们常常会遇到PLEG出错的问题:\nPLEG is not healthy: pleg was last seen active 6m55.488150776s ago; threshold is 3m0s\n调试kuryr cni的时候，发现当openstack neutron服务压力比较大的时候。\ncni这边申请和释放 port的时延会相应的增加，导致虚拟机大量堆积无效的netns，\n然后就会遇到由kueblet PLEG not healthy引起的docker hang 住问题。\ndocker 重启 docker 和 kueblet 可以暂时解决PLEG unhealthy。\nsystemctl restart docker systemctl restart kubelet # do NOT use `docker rm -vf`, # which will kill running containers docker rm -v `docker ps -qa` 建议同时修改 kubelet 启动参数 \u0026ndash;housekeeping-interval=30s\n","date":"2019-02-11T10:33:42+08:00","image":"https://tab.deoops.com/posts/pleg-unhealth/reboot_hu7bd3807bcb59ca2798687ce57e12416d_147938_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/pleg-unhealth/","title":"pod生命周期事件生成器"},{"content":"base 通过修改ibase和obase可以实现各种进制的转化，比如十进制和二进制和十六进制之间的转换；\nIf you aren’t familiar with conversion between decimal, binary, and hexadecimal formats, you can use a calculator utility such as bc or dc to convert between different radix representations. For example, in bc, you can run the command obase=2; 240 to print the number 240 in binary (base 2) form.\n#!/bin/bash ❯ bc -q ibase 10 obase 10 1+ 3 *3 10 obase=2 245 11110101 255 11111111 192 11000000 168 10101000 1 1 172 10101100 obase=16 34 22 172 AC ^D% syntax bc An arbitrary precision calculator language. More information: https://manned.org/bc. - Start bc in interactive mode using the standard math library: bc -l - Calculate the result of an expression: bc \u0026lt;\u0026lt;\u0026lt; \u0026#34;(1 + 2) * 2 ^ 2\u0026#34; - Calculate the result of an expression and force the number of decimal places to 10: bc \u0026lt;\u0026lt;\u0026lt; \u0026#34;scale=10; 5 / 3\u0026#34; - Calculate the result of an expression with sine and cosine using mathlib: bc -l \u0026lt;\u0026lt;\u0026lt; \u0026#34;s(1) + c(1)\u0026#34; ","date":"2018-10-22T18:16:25+08:00","image":"https://tab.deoops.com/posts/bc-language/calculator_hu7fe9f2c05e9afb6d43e72c846db96c5b_49911_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/bc-language/","title":"bc语言"},{"content":"今天测试给我反馈了一个pod问题，测试给出的 use case 描述如下：\n配置一个nginx的web服务；\n在生命周期中选择http协议，端口配置80，路径配置/errorpath； 服务中pod能正常启动； 预期在pod的事件中应该有一个“FailedPostStartHook”错误信息。 测试人员发现，第4点的预期没有达到，pod 正常启动了，却没有 FailedPostStarHook事件出现。 简单分析了一下，我发现是测试人员把pod的lifecycle和 pod的liveness/readiness 诺混淆了，从而写出了错的test case。\nLifecycle handlers 首先回顾下pod lifecycle的作用：\nkubectl explain pod.spec.containers.lifecycle.postStart RESOURCE: postStart \u0026lt;Object\u0026gt; DESCRIPTION: PostStart is called immediately after a container is created. If the handler fails, the container is terminated and restarted according to its restart policy. Other management of the container blocks until the hook completes. More info: https://kubernetes.io/docs/concepts/containers/ container-lifecycle-hooks/#container-hooks/ Handler defines a specific action that should be taken 简单翻译下就是说， kubernetes 提供的pod start和exit的 lifecycle hooks 方便开发人员hooked到\n下面两个生命周期：\n通知外部世界pod已经启动完成 (postStart) pod优雅的退出 (preStop) ps：init container可以解决依赖问题，不要和postStart hooks混淆了哦。\n举例子 当postStart hook的handler发送http请求到指定的port和path时， 如果exit code 不为0时，\n会触发 FailedPostStartHook 事件，然后执行用户给定的hook/handler。\n注意：lifecycle hooks不会理会http 自身的 response code健康与否，\n只要handler exit code 为0 pod就会正常启动，不触发任何事件。\n可以配置一个错误的端口使产生FailedPostStartHook event：\napiVersion: v1 kind: Pod metadata: name: lifecycle90 spec: containers: - image: nginx:latest name: ng imagePullPolicy: Never readinessProbe: httpGet: port: 80 path: / lifecycle: postStart: httpGet: port: 80 ## change this to 8080 or 9090 to get the FailedPostStartHook event path: /teststart preStop: httpGet: port: 80 path: /teststop 相关代码 talk is cheap.\nhandler只关心getHTTPRespBody 获取body的过程是否有err产生， 并不关心http respond code。\ngetHTTPRespBody Liveness/readiness probe pod liveness/readiness probe 则会检查 http code 是否为 2xx来确定probe 是否成功。\n相关代码 talk is cheap.\nrunProbe 调用http.Probe，DoHTTPProbe会检查http response code。\ndoHTTPProbe 从源码可以清楚的看到， lifecyce 的handler 只发送了http 请求，并没有检查 http response code, liveness/readiness 的prober 则做了http response code 的检查。\n","date":"2018-09-18T23:33:25+08:00","image":"https://tab.deoops.com/posts/pod-life-live/hook_hu272076145e79d12d10d979ff0bdeaf35_73849_120x120_fill_box_smart1_1.gif","permalink":"https://tab.deoops.com/posts/pod-life-live/","title":"Pod生命周期和Liveness的区别"},{"content":"之前写了一篇post 适配腾讯云backend 的文章，从代码的角度简单记录了flannel vpc backend实现过程。\n这篇文章是对前面文章的补充，全局鸟瞰描绘了flannel vpc backend网络数据包的流动过程。\n总体来看vpc 和 host-gw 模式是很类似的，理解host-gateway模式 对理解vpc 模式很有帮助。\nhost gw host gateway 模式：\nhost-gw adds route table entries on hosts, so that host know how to traffic container network packets.\nThis works on L2, because it only concerns hosts, switches and containers. switches does not care IP and route, hosts know containers exists, and how to route to them, containers just send and receive data.\nIf hosts are at different networks, L3 is introduced, and routers are involved. routers have no idea that containers exists, and any containers packet will be dropped, making communication impossible.\nOf course, you can add route table entries on routers, but that is out of control flannel.\nL2 为什么host-gw模式下，二层网络一定要打通:\nstackoverflow 上面已经有很好的回答，我摘抄过来：\nhost-gw adds route table entires on each host. And the entries are as following:\nDestination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.110.110.1 0.0.0.0 UG 100 0 0 eth0 10.100.14.0 10.110.110.21 255.255.255.0 UG 0 0 0 eth0 10.100.38.0 0.0.0.0 255.255.255.0 U 0 0 0 docker0 10.110.110.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0 169.254.169.254 10.110.110.1 255.255.255.255 UGH 100 0 0 eth0 The most import item is the value of Gateway(10.110.110.21).\nThe route table will change the destination mac address to the mac_address of node(10.110.110.21) which is connected L2 directly to 10.110.110.22(current node).\nIf not L2 connected, the packet can not be delivered to nodes(next-hop)\nvpc 数据包 vpc backend模式下网络数据包的流动过程如下：\nFlanneld 在每个node 节点创建一个 cni0 的网桥bridge设备，node里面的pod之间的通信走cni0网桥； Node 和 在本机上的pod 通信通过node的route table 转发到cni0； vpc backend适配器调用vpc sdk在vpc网络中维护了一个vpc route table； Node 和其它node里面的pod通信，是先通过 vpc route table 找到 pod 子网 所对应node，然后再通过这个node把网络流量转发到pod； 跨node 的pod 通信通过这个route table找到pod网段所在的node，然后载通过 node 节点上的route table 转发到对应node 上cni0设备上然后找到对应的pod； VPC上的普通vm(即，该vm不是k8s node)和 k8s pod之间的通信与 上面第4，5点 通信是一样的。 L2区别 host-gw mode 因为是在node 节点上 添加的 route 规则，所以所有的节点要二层switch打通，否则 下一跳(next-hop)跳不过去，没人帮忙转发二层数据包(frame)。\nvpc backend mode 因为是在云厂商提供的外部router上添加的 route 规则，对vm做到了无感知，所以 vm之间无需打通 二层 switch。\n","date":"2018-09-11T10:07:04+08:00","image":"https://tab.deoops.com/posts/flannel-vpc/flannel-vpc-mode_hu2599b793831e4d2e2f689a3c7eb97256_31225_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/flannel-vpc/","title":"VPC模式"},{"content":"最近开发的遇到一个需求是在判断 两个目录是否相互包含。\n想着用正则表达式或者递归去解决，捣鼓一段时间后发现总有些edge case 不能cover到，\n后来发现用 python 的pathlib 可以很好的解决。\nfrom pathlib import Path def overlapping(a, b): if a == b: return True a_path = Path(a) b_path = Path(b) return a_path in b_path.parents or b_path in a_path.parents ","date":"2018-09-10T21:20:33+08:00","image":"https://tab.deoops.com/posts/path-overlap/Half-overlapping-paths-of-AB_hu442aa320a58d7e682a982b572c7211d3_21904_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/path-overlap/","title":"子目录父目录"},{"content":"有一个app 跑在pod里面，这个app 默认会输出自己的运行日志到syslogd，\n请问如何让host主机上运行的syslogd日志收集器收集到上面app输出的运行日志呢？\n/dev/log 答案：把 主机的 /dev/log目录挂载到 pod 里面的 /dev/log即可。\nSome of these messages need to be brought to a system administrator’s attention immediately. And it may not be just any system administrator – there may be a particular system administrator who deals with a particular kind of message. Other messages just need to be recorded for future reference if there is a problem. Still others may need to have information extracted from them by an automated process that generates monthly reports.\nTo deal with these messages, most Unix systems have a facility called \u0026ldquo;Syslog.\u0026rdquo; It is generally based on a daemon called “Syslogd” Syslogd listens for messages on a Unix domain socket named /dev/log.\n参考syslog overview\nmountPath 具体来说就在 pod 的yaml 定义中，添加 hostPath volume，然后挂载到pod里：\n- name: syslog mountPath: /dev/log volumes: - name: syslog hostPath: path: /dev/log 在client-go等sdk中这样写即可：\napiv1.Volume{ Name: \u0026#34;syslog\u0026#34;, VolumeSource: apiv1.VolumeSource{ HostPath: \u0026amp;apiv1.HostPathVolumeSource{ Path: \u0026#34;/dev/log\u0026#34;, }, }, }, }, Containers: []apiv1.Container{ apiv1.Container{ apiv1.VolumeMount{ Name: \u0026#34;syslog\u0026#34;, MountPath: \u0026#34;/dev/log\u0026#34;, }, }, }, ","date":"2018-09-03T20:46:00+08:00","image":"https://tab.deoops.com/posts/pod-syslog/local-domain-socket_huc790e5f02f9d94a1836f453997902818_137334_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/pod-syslog/","title":"收集容器syslog"},{"content":"一般来说，kubernetes 的pod是不在master 节点上运行的。\n如果要求pod 必须被调度到master 节点上运行，可以修改pod 的 toleration 和 affinity。\ntoleration和affinity： 在pod加上toleration和affinity配置\nyaml spec: tolerations: - key: \u0026#34;node-role.kubernetes.io/master\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;true\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: node-role.kubernetes.io/master operator: Exists go + Operator: apiv1.TolerationOpExists, + Effect: apiv1.TaintEffectNoSchedule, + }, + }, + Affinity: \u0026amp;apiv1.Affinity{ + NodeAffinity: \u0026amp;apiv1.NodeAffinity{ + RequiredDuringSchedulingIgnoredDuringExecution: \u0026amp;apiv1.NodeSelector{ + NodeSelectorTerms: []apiv1.NodeSelectorTerm{ + apiv1.NodeSelectorTerm{ + MatchExpressions: []apiv1.NodeSelectorRequirement{ + apiv1.NodeSelectorRequirement{ + Key: \u0026#34;node-role.kubernetes.io/master\u0026#34;, + Operator: apiv1.NodeSelectorOpExists, + }, + }, + }, + }, + }, ","date":"2018-08-18T20:57:52+08:00","image":"https://tab.deoops.com/posts/master-affinity/toleration_hu89d476b51a49cae5cfc73cf97d1c4cfc_30538_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://tab.deoops.com/posts/master-affinity/","title":"调度到master节点"},{"content":"客户需要把kubernetes apiserver/etcd/kubelet/kubectl 等所有的证书有效期修改为100年。\n很明显这是一个不合理的需求，不过客户说什么就是什么。\n于是经几天的调试有了下面的这个 Makefile批量生成所有(FILES变量)的证书。\n如果对makefile的语法不熟悉，可以看看Makefile简介\nmakefile FILES = ca.crt ca.key sa.key sa.pub front-proxy-ca.crt front-proxy-ca.key etcd_ca.crt etcd_ca.key CONFS = admin.conf controller-manager.conf kubelet.conf scheduler.conf SELFS = kubelet.crt.self kubelet.crt.key #KEYs = ca.key front-proxy-ca.key etcd_ca.key sa.key #CAs = ca.crt front-proxy-ca.crt etcd_ca.crt #PUBs = sa.pub ## kubernetes will sign certificate ## automatically, so below ## csr/cert is for test purpose #CSR = apiserver.csr apiserver-kubelet-client.csr CERT_KEYS = apiserver.key apiserver-kubelet-client.key front-proxy-client.key CERTS = apiserver.cert apiserver-kubelet-client.cert front-proxy-client.cert # openssl genrsa -des3 -out rootCA.key 4096 CMD_CREATE_PRIVATE_KEY = openssl genrsa -out $@ 2048 CMD_CREATE_PUBLIC_KEY = openssl rsa -in $\u0026lt; -pubout -out $@ # openssl req -x509 -new -nodes -key rootCA.key -sha256 -days 1024 -out rootCA.crt CMD_CREATE_CA = openssl req -x509 -new -nodes -key $\u0026lt; -sha256 -days 36500 -out $@ -subj \u0026#39;/CN=kubernetes\u0026#39; # openssl req -new -key mydomain.com.key -out mydomain.com.csr CMD_CREATE_CSR = openssl req -new -key $\u0026lt; -out $@ -config $(word 2,$^) # openssl x509 -req -in mydomain.com.csr -CA rootCA.crt -CAkey rootCA.key -CAcreateserial -out mydomain.com.crt -days 500 -sha256 CMD_SIGN_CERT = openssl x509 -req -in $\u0026lt; -CA $(word 2,$^) -CAkey $(word 3,$^) -CAcreateserial -out $@ -days 36500 -sha256 -extfile $(word 4,$^) -extensions my_extensions # generata self sign certificate CMD_CREATE_CERT = openssl req -x509 -new -nodes -key $\u0026lt; -sha256 -days 36500 -out $@ -subj \u0026#39;/CN=nodeXXX@timestamp1531732165\u0026#39; CMD_MSG = @echo generating $@ ... MASTER_IP := 192.168.1.200 ## REMEMBER CHANGE ME .PHONY: all clean check self_sign rename all: ${FILES} ${CONFS} ${CERT_KEYS} ${CERTS} clean: -rm ${FILES} ${CONFS} ${CERT_KEYS} ${CERTS} self_sign: ${SELFS} check: for f in *.cert *.crt; do echo $$f; openssl x509 -noout -dates -in $$f; echo \u0026#39;===\u0026#39;; done rename: for f in *.cert; do echo $$f; mv $$f $${f%.*}.crt; echo \u0026#39;=====\u0026#39;; done %.key: ${CMD_MSG} ${CMD_CREATE_PRIVATE_KEY} %.pub: %.key ${CMD_MSG} ${CMD_CREATE_PUBLIC_KEY} %.self: %.key ${CMD_MSG} ${CMD_CREATE_CERT} %.crt: %.key ${CMD_MSG} ${CMD_CREATE_CA} %.csr: %.key %.csr.cnf ${CMD_MSG} ${CMD_CREATE_CSR} %.cert: %.csr ca.crt ca.key %.csr.cnf #%.cert: %.csr front-proxy-ca.crt front-proxy-ca.key %.csr.cnf ${CMD_MSG} ${CMD_SIGN_CERT} %.conf: %.cert %-conf.sh sh $(word 2,$^) ${MASTER_IP} 上面的Makefile还需要对应的csr和 conffiles。\nadmin.csr.cnf cat \u0026lt;\u0026lt;EOF \u0026gt; admin.conf apiVersion: v1 clusters: - cluster: certificate-authority-data: $(cat ca.crt | base64 | tr -d \u0026#39;\\n\u0026#39;) server: https://$1:6443 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: $(cat admin.cert | base64 | tr -d \u0026#39;\\n\u0026#39;) client-key-data: $(cat admin.key | base64 | tr -d \u0026#39;\\n\u0026#39;) EOF admin-conf.sh [ req ] prompt = no utf8 = yes distinguished_name = my_req_distinguished_name req_extensions = my_extensions [ my_req_distinguished_name ] C = dn ST = lol L = dota O = system:masters # change this CN = kubernetes-admin # change this [ my_extensions ] basicConstraints=CA:FALSE subjectKeyIdentifier = hash # subjectAltName=@my_subject_alt_names # # [ my_subject_alt_names ] # DNS.1 = *.oats.org # DNS.2 = *.oats.net # DNS.3 = *.oats.in # DNS.4 = oats.org # DNS.5 = oats.net # DNS.6 = oats.in kubeadm 安装k8s集群的时候，使用kubeadm phase certs命令更新证书即可：\nkubeadm alpha phase certs all --config kubeadm-config.yaml kubeadm alpha phase kubelet config write-to-disk --config kubeadm-config.yaml kubeadm alpha phase kubelet write-env-file --config kubeadm-config.yaml kubeadm alpha phase kubeconfig kubelet --config kubeadm-config.yaml kubeadm alpha phase kubeconfig all --config kubeadm-config.yaml ## --experimental-cluster-signing-duration=187600h0m0s kubeadm alpha phase controlplane all --config kubeadm-config.yaml systemctl start kubelet kubeadm alpha phase mark-master --config kubeadm-config.yaml ## kubeadm join 172.16.0.5:6443 --token xvrmo8.lk0ec7ifzn8mdflu2 --discovery-token-unsafe-skip-ca-verification ","date":"2018-08-10T20:27:10+08:00","image":"https://tab.deoops.com/posts/k8s-certs/k8s_hucdb5e2014b72ce09271c5c17f97648be_22078_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://tab.deoops.com/posts/k8s-certs/","title":"替换k8s所有证书"},{"content":"update: flannel从v0.14.0(2021/05/27)开始已经支持腾讯云的vpc backend了。\n客户需要在腾讯云上部署kubernetes集群而且选用的网络插件是flannel，所以我们需要为flannel 添加 腾讯云 vpc 的 backend 适配。\n我大致看了下github上 阿里云 和 aws 适配器的代码，发现并不复杂，flannel已经把所有的dirty work flannel 都包装好API了。\n稍稍了解一些网络设备或者Linux网络相关的命令（比如route table）就可以比较轻松的写出flannel适配器。\n整个适配过程可以分为下面4个步骤：\n定义 TxVpcBackend struct, 实现New func 在init func中注册; 调用腾讯云SDK 实现 RegisterNetwork method; 最后在main.go中 注册腾讯云backend 即可； 部署deployment 的时候选择 tx-vpc 的backend 即可. 下面结合部分代码具体的说下实现过程：\n开发 定义结构体 只是搭一个架子，方便注册到flannel backend上，不含具体适配器的逻辑：\ntype TxVpcBackend struct { sm subnet.Manager extIface *backend.ExternalInterface } func New(sm subnet.Manager, extIface *backend.ExternalInterface) (backend.Backend, error) { be := TxVpcBackend{ sm: sm, extIface: extIface, } return \u0026amp;be, nil } func init() { backend.Register(\u0026#34;tx-vpc\u0026#34;, New) } 实现RegisterNetwork func (be *TxVpcBackend) RegisterNetwork(ctx context.Context, config *subnet.Config) (backend.Network, error) { // 1. Parse our configuration cfg := struct { AccessKeyID string AccessKeySecret string }{} if len(config.Backend) \u0026gt; 0 { if err := json.Unmarshal(config.Backend, \u0026amp;cfg); err != nil { return nil, fmt.Errorf(\u0026#34;error decoding VPC backend config: %v\u0026#34;, err) } } log.Infof(\u0026#34;Unmarshal Configure : %v\\n\u0026#34;, cfg) // 2. Acquire the lease form subnet manager attrs := subnet.LeaseAttrs{ PublicIP: ip.FromIP(be.extIface.ExtAddr), } l, err := be.sm.AcquireLease(ctx, \u0026amp;attrs) switch err { case nil: case context.Canceled, context.DeadlineExceeded: return nil, err default: return nil, fmt.Errorf(\u0026#34;failed to acquire lease: %v\u0026#34;, err) } if cfg.AccessKeyID == \u0026#34;\u0026#34; || cfg.AccessKeySecret == \u0026#34;\u0026#34; { cfg.AccessKeyID = os.Getenv(\u0026#34;ACCESS_KEY_ID\u0026#34;) cfg.AccessKeySecret = os.Getenv(\u0026#34;ACCESS_KEY_SECRET\u0026#34;) if cfg.AccessKeyID == \u0026#34;\u0026#34; || cfg.AccessKeySecret == \u0026#34;\u0026#34; { return nil, fmt.Errorf(\u0026#34;ACCESS_KEY_ID and ACCESS_KEY_SECRET must be provided! \u0026#34;) } } err = createRoute(l.Subnet.String(), cfg.AccessKeyID, cfg.AccessKeySecret) if err != nil { log.Errorf(\u0026#34;Error DescribeVRouters: %s .\\n\u0026#34;, err.Error()) } return \u0026amp;backend.SimpleNetwork{ SubnetLease: l, ExtIface: be.extIface, }, nil } 主要逻辑是 使用腾讯云的SDK 在vpc 网络下创建route , 即上面的\nerr = createRoute(l.Subnet.String(), cfg.AccessKeyID, cfg.AccessKeySecret\n因为createRoute Func和 腾讯云sdk 的实现强相关，所以我就不展开了，个人可以去github上查看腾讯云的sdk文档。\ntargetRout 最后targetRoute 应该是这个样子的：\ntargetRoute := \u0026amp;vpc.Route{ DestinationCidrBlock: common.StringPtr(dest), GatewayType: common.StringPtr(\u0026#34;NORMAL_CVM\u0026#34;), GatewayId: common.StringPtr(nextHop), RouteDescription: common.StringPtr(desc + \u0026#34; flannel podCIDR\u0026#34;), } registry 在main.go文件中加如import pkg即可注册腾讯云适配器：\n_ \u0026#34;github.com/coreos/flannel/backend/txvpc\u0026#34; 部署 修改deploymnet 修改官方的deployment yaml 文件中 net-conf.json字段， 把\u0026quot;Type\u0026quot;: 改成tx-vpc即可：\nnet-conf.json: | { \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;, \u0026#34;Backend\u0026#34;: { \u0026#34;Type\u0026#34;: \u0026#34;tx-vpc\u0026#34; } 新建RAM账户 在腾讯云的dashboard新建 RAM 帐户，赋予vpc网络读写权限。\n记下 AccessKeyID 和 AccessKeySecret;\n修改deployment中填写deploy.yaml为上一步中记录的 AccessKeyID 和 AccessKeySecret值。\n最后，用kubectl 部署flannle即可：\nkubectl create -f deploy.yaml ","date":"2018-08-08T18:59:00+08:00","image":"https://tab.deoops.com/posts/tx-flannel/flannel-vpc_hue54105a7a72eb92cf36c8a151067b0c3_60319_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/tx-flannel/","title":"flannel vpc"},{"content":"网络上关于 makefile的教程有很多，由于我日常不是写c/c++的， 不常使用makefile，需要用的时候总是要重新Google搜索makefile的语法。\n索性整理出来这篇 makefile 教程，备忘。\n教程 Makefile简易教程：\n基本语法 target: dependency1 dependency2 ... [TAB] action1 [TAB] action2 ... 下面的makefile摘抄自GNU Make in Detail for Beginners，这篇入门文章把makefile的语法写的非常透彻。\n推荐大家多读几遍\n##### makefile for compile C programs # Compiler to use CC = gcc # -g for debug, -O2 for optimise and -Wall additonal messages OPTIONS = -O2 -g -Wall # Directory for header file INCLUDES = -I . # List of objects to be build OBJS = main.o module.o .PHONY: all list clean all: ${OBJS} @echo \u0026#34;Building...\u0026#34; # print \u0026#34;Building...\u0026#34; message ${CC} ${OPTIONS} ${INCLUDES} ${OBJS} -o main_bin %.o: %.c # \u0026#39;%\u0026#39; pattern wildcard matching ${CC} ${OPTIONS} ${INCLUDES} -c %.c list: @echo $(shell ls) # print output of command `ls` clean: @echo Cleaning up... -rm -rf *.0 # \u0026#39;-\u0026#39; prefix for ignoring errors and continue execution -rm main_bin #### makefile for img manage FILES = $(shell find imgs -type f -iname \u0026#34;*.jpg\u0026#34; | sed \u0026#39;s/imgs/thumb/g\u0026#39;) CONVERT_CMD = convert -resize \u0026#34;100x100\u0026#34; $\u0026lt; $@ MSG = \u0026#34;\\nUpdating thumbnail\u0026#34; $@ all_thumb: ${FILES} thumb/%.jpg: imgs/%.jpg ${MSG} ${CONVERT_CMD} thumb/%.JPG: imgs/%.JPG ${MSG} ${CONVERT_CMD} clean_all: @echo Cleaning up files... -rm -rf thumb/*.{jpg,JPG} 变量 赋值 Simple assignment (:=) We can assign values (RHS) to variables (LHS) with this operator, for example: CC := gcc. With simple assignment (:=), the value is expanded and stored to all occurrences in the Makefile when its first definition is found.\nFor example, when a CC := ${GCC} ${FLAGS} simple definition is first encountered, CC is set to gcc -W and wherever ${CC} occurs in actions, it is replaced with gcc -W.\nRecursive assignment (=) Recursive assignment (the operator used is =) involves variables and values that are not evaluated immediately on encountering their definition, but are re-evaluated every time they are encountered in an action that is being executed. As an example, say we have:\nGCC = gcc FLAGS = -W With the above lines, CC = ${GCC} {FLAGS} will be converted to gcc -W only when an action like ${CC} file.c is executed somewhere in the Makefile. With recursive assignation, if the GCC variable is changed later (for example, GCC = c++), then when it is next encountered in an action line that is being updated, it will be re-evaluated, and the new value will be used; ${CC} will now expand to c++ -W.\nWe will also have an interesting and useful application further in the article, where this feature is used to deal with varying cases of filename extensions of image files.\nConditional assignment (?=) Conditional assignment statements assign the given value to the variable only if the variable does not yet have a value.\nAppending (+=) The appending operation appends texts to an existing variable. For example:\nCC = gcc CC += -W CC now holds the value gcc -W.\naction内置变量 The % character can be used for wildcard pattern-matching, to provide generic targets. For example:\n%.o: %.c [TAB] actions When % appears in the dependency list, it is replaced with the same string that was used to perform substitution in the target. Inside actions, we can use special variables for matching filenames. Some of them are:\n$@ (full target name of the current target) $? (returns the dependencies that are newer than the current target) $* (returns the text that corresponds to % in the target) $\u0026lt; (name of the first dependency) dep.o: dep.src config1.cfg config2.cfg @echo the second preq is $(word 2,$^), the third is $(word 3,$^) $^ (name of all the dependencies with space as the delimiter) Instead of writing each of the file names in the actions and the target, we can use shorthand notations based on the above, to write more generic Makefiles. action modifiers We can change the behaviour of the actions we use by prefixing certain action modifiers to the actions. Two important action modifiers are:\n- (minus) Prefixing - to any action causes any error that occurs while executing the action to be ignored.\nBy default, execution of a Makefile stops when any command returns a non-zero (error) value. If an error occurs, a message is printed, with the status code of the command, and noting that the error has been ignored. Looking at the Makefile from our sample project: in the clean target, the rm target_bin command will produce an error if that file does not exist (this could happen if the project had never been compiled, or if make clean is run twice consecutively). To handle this, we can prefix the rm command with a minus, to ignore errors: -rm target_bin.\n@ (at) @ suppresses the standard print-action-to-standard-output behaviour of make, for the action/command that is prefixed with @. For example, to echo a custom message to standard output, we want only the output of the echo command, and don’t want to print the echo command line itself. @echo Message will print “Message” without the echo command line being printed.\n.PHONY Use PHONY to avoid file-target name conflicts. Remember the all and clean special targets in our Makefile? What happens when the project directory has files with the names all or clean? The conflicts will cause errors. Use the .PHONY directive to specify which targets are not to be treated as files — for example: .PHONY: all clean.\n其它 dry run Simulating make without actual execution. At times, maybe when developing the Makefile, we may want to trace the make execution (and view the logged messages) without actually running the actions, which is time consuming. Simply use make -n to do a “dry run”.\nshell Using the shell command output in a variable Sometimes we need to use the output from one command/action in other places in the Makefile — for example, checking versions/locations of installed libraries, or other files required for compilation. We can obtain the shell output using the shell command. For example, to return a list of files in the current directory into a variable, we would run: LS_OUT = $(shell ls).\nNested Makefiles Nested Makefiles (which are Makefiles in one or more subdirectories that are also executed by running the make command in the parent directory) can be useful for building smaller projects as part of a larger project. To do this, we set up a target whose action changes directory to the subdirectory, and invokes make again:\nsubtargets: cd subdirectory \u0026amp;\u0026amp; $(MAKE) Instead of running the make command, we used $(MAKE), an environment variable, to provide flexibility to include arguments. For example, if you were doing a “dry run” invocation: if we used the make command directly for the subdirectory, the simulation option (-n) would not be passed, and the commands in the subdirectory’s Makefile would actually be executed. To enable use of the -n argument, use the $(MAKE) variable.\n实践 在golang项目里的实践：\n# Include variables from .envrc files -include .envrc # ==================================================================================== # # HELPERS # ==================================================================================== # ## help: print this help message .PHONY: help help: @echo \u0026#34;\\t##IMPORTANT##: please run \u0026#39;echo .envrc \u0026gt;\u0026gt; .gitignore\u0026#39; at very first time\u0026#34; @echo \u0026#39;Usage:\u0026#39; @sed -n \u0026#39;s/^##//p\u0026#39; ${MAKEFILE_LIST} | column -t -s \u0026#39;:\u0026#39; | sed -e \u0026#39;s/^/ /\u0026#39; # Create the new confirm target. .PHONY: confirm confirm: @echo -n \u0026#39;Are you sure? [y/N] \u0026#39; \u0026amp;\u0026amp; read ans \u0026amp;\u0026amp; [ $${ans:-N} = y ] # ==================================================================================== # # DEVELOPMENT # ==================================================================================== # ## run/main: run the cmd/main binary file .PHONY: run/main run/main: go run ./cmd ## run/debug: debug app use dlv .PHONY: run/debug run/debug: dlv debug ./cmd --headless --listen :4040 ## run/test: runs go test with default values .PHONY: run/test run/test: go test -timeout 300s -v -count=1 -race ./... ## run/update: runs go get -u \u0026amp;\u0026amp; go mod tidy .PHONY: run/update run/update: go get -u ./... go mod tidy ## db/psql: connection to the database using psql .PHONY: db/psql db/psql: @psql ${PG_DSN} ## db/generate use sqlc generated models and queries .PHONY: db/generate db/generate: @echo \u0026#39;sqlc generate in internal/sqlc fold\u0026#39; @cd internal/sqlc \u0026amp;\u0026amp; sqlc generate \u0026amp;\u0026amp; cd ../.. ## db/migrations/new name=$1: create a new database migration .PHONY: db/migrations/new db/migrations/new: @echo \u0026#39;Creating migrate files for ${name}\u0026#39; @migrate create -seq -ext=.sql -dir=./migrations ${name} ## db/migrations/up: apply all up database migrations .PHONY: db/migrations/up db/migrations/up: confirm @echo \u0026#39;Running up migrations...\u0026#39; @migrate -path ./migrations -database ${PG_DSN} up # ==================================================================================== # # QUALITY CONTROL # ==================================================================================== # ## audit: tidy dependencies and format, vet and test all code .PHONY: audit audit: @echo \u0026#39;Tidying and verifying module dependencies...\u0026#39; go mod tidy go mod verify @echo \u0026#39;Formatting code...\u0026#39; go fmt ./... @echo \u0026#39;Vetting code...\u0026#39; go vet ./... #staticcheck ./... # go install honnef.co/go/tools/cmd/staticcheck@latest @echo \u0026#39;Running tests...\u0026#39; go test -race -vet=off ./... ## vendor: tidy and vendor dependencies .PHONY: vendor vendor: @echo \u0026#39;Tidying and verifying module dependencies...\u0026#39; go mod tidy go mod verify @echo \u0026#39;Vendoring dependencies...\u0026#39; go mod vendor # ==================================================================================== # # BUILD # ==================================================================================== # #current_time = $(shell date --iso-8601=seconds) current_time = $(shell date -u +\u0026#34;%Y-%m-%dT%H:%M:%SZ\u0026#34;) git_description = $(shell git describe --always --dirty --tags --long) linker_flags = \u0026#39;-s -X main.buildTime=${current_time} -X main.version=${git_description}\u0026#39; ## build/api: build the cmd/api application .PHONY: build/api build/main: audit @echo \u0026#39;Building cmd/...\u0026#39; go build -ldflags=${linker_flags} -o=./bin/cmd ./cmd #go tool dist list GOOS=linux GOARCH=amd64 go build -ldflags=${linker_flags} -o=./bin/linux_amd64/cmd ./cmd ## build/dlv-debug: build the application with dlv gcflags .PHONY: build/dlv-debug build/dlv-debug: @echo \u0026#34;Building for delve debug...\u0026#34; @go build \\ -ldflags ${linker_flags} \\ -ldflags=-compressdwarf=false \\ -gcflags=all=-d=checkptr \\ -gcflags=\u0026#34;all=-N -l\u0026#34; \\ -o ./bin/debug ./cmd ","date":"2018-07-27T12:38:51+08:00","image":"https://tab.deoops.com/posts/makefile-tutorial/gnu-make_hu77bd53dd3603ba1d9252afbf71dec2d7_87202_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/makefile-tutorial/","title":"Makefile简介"},{"content":"在shell脚本里使用mkfifo命令创建named pipes可以实现简单的事件驱动， 避免poll（轮询）带来的时延（not real-time）和资源消耗的问题。\nmkfifo ❯ man mkfifo | head -n 12 MKFIFO(1) General Commands Manual MKFIFO(1) NAME mkfifo – make fifos SYNOPSIS mkfifo [-m mode] fifo_name ... DESCRIPTION mkfifo creates the fifos requested, in the order specified. By default, the resulting fifos have mode 0666 (rw-rw-rw-), limited by the current umask(2). ~ consumer 消费者以blocked的状态监听事件的发生，然后handle:\n#!/bin/bash pipe=/tmp/testpipe trap \u0026#34;rm -f $pipe\u0026#34; EXIT if [[ ! -p $pipe ]]; then mkfifo $pipe fi while true do if read line \u0026lt;$pipe; then if [[ \u0026#34;$line\u0026#34; == \u0026#39;quit\u0026#39; ]]; then break fi echo $line fi done echo \u0026#34;consumer exiting\u0026#34; producer 生产者往pipe里写入内容，触发事件：\n#!/bin/bash pipe=/tmp/testpipe if [[ ! -p $pipe ]]; then echo \u0026#34;Reader not running\u0026#34; exit 1 fi msg=${1-\u0026#34;Hello from $$\u0026#34;} echo $msg \u0026gt;$pipe ","date":"2018-06-27T12:20:29+08:00","image":"https://tab.deoops.com/posts/shell-fifo/pm_hu82f0a6c526c861c02b7a3999a3e46b01_146865_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/shell-fifo/","title":"事件驱动"},{"content":"今天调试容器应用的时候发现，app运行一段时间后，容器外挂的一个volumn会偶发性的被删除。 于是需要监控下到底是谁/哪个进程把文件目录给删除了。\ngoogle一阵子后，发现可以使用auditd 服务来监控和搜索出都有那些进程操作够目标文件/目录。\n整个过程分为3步：\n开启 auditd 服务； 使用auditctl 配置 auditd服务； 一段时间之后 使用 ausearch 来查看/搜索审计的日志。 启动监控 开启auditd服务：\nsystemctl start auditd ## you may need `mkdir /var/log/audit` 添加监控规则 编辑审计规则：\n## list existing rules auditctl -l ## clean existing rules auditctl -D ## watch /var/run/yourfolder auditctl -w /var/run/yourfolder -p war -k serachkey auditctl语法 [root@ddeoops ~]# auditctl -h usage: auditctl [options] -a \u0026lt;l,a\u0026gt; Append rule to end of \u0026lt;l\u0026gt;ist with \u0026lt;a\u0026gt;ction -A \u0026lt;l,a\u0026gt; Add rule at beginning of \u0026lt;l\u0026gt;ist with \u0026lt;a\u0026gt;ction -b \u0026lt;backlog\u0026gt; Set max number of outstanding audit buffers allowed Default=64 -c Continue through errors in rules -C f=f Compare collected fields if available: Field name, operator(=,!=), field name -d \u0026lt;l,a\u0026gt; Delete rule from \u0026lt;l\u0026gt;ist with \u0026lt;a\u0026gt;ction l=task,exit,user,exclude a=never,always -D Delete all rules and watches -e [0..2] Set enabled flag -f [0..2] Set failure flag 0=silent 1=printk 2=panic -F f=v Build rule: field name, operator(=,!=,\u0026lt;,\u0026gt;,\u0026lt;=, \u0026gt;=,\u0026amp;,\u0026amp;=) value -h Help -i Ignore errors when reading rules from file -k \u0026lt;key\u0026gt; Set filter key on audit rule -l List rules -m text Send a user-space message -p [r|w|x|a] Set permissions filter on watch r=read, w=write, x=execute, a=attribute -q \u0026lt;mount,subtree\u0026gt; make subtree part of mount point\u0026#39;s dir watches -r \u0026lt;rate\u0026gt; Set limit in messages/sec (0=none) -R \u0026lt;file\u0026gt; read rules from file -s Report status -S syscall Build rule: syscall name or number -t Trim directory watches -v Version -w \u0026lt;path\u0026gt; Insert watch at \u0026lt;path\u0026gt; -W \u0026lt;path\u0026gt; Remove watch at \u0026lt;path\u0026gt; --loginuid-immutable Make loginuids unchangeable once set --reset-lost Reset the lost record counter 分析日志 查看/搜索 审计日志：\n[root@deoops ~]# ausearch -k wiserun -i type=SYSCALL msg=audit(2018年06月20日 14:22:33.669:8377) : arch=x86_64 syscall=unlinkat success=no exit=ENOTEMPTY(目录非空) a0=0xffffffffffffff9c a1=0xc42038ca80 a2=0x200 a3=0x0 items=2 ppid=20523 pid=1890 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=yes_i_changed_you exe=/root/yes_i_changed_you key=wiserun type=SYSCALL msg=audit(2018年06月20日 14:22:33.669:8376) : arch=x86_64 syscall=unlinkat success=no exit=EISDIR(是一个目录) a0=0xffffffffffffff9c a1=0xc42038ca60 a2=0x0 a3=0x0 items=2 ppid=20523 pid=1890 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=yes_i_changed_you exe=/root/yes_i_changed_you key=wiserun type=SYSCALL msg=audit(2018年06月20日 14:22:33.669:8378) : arch=x86_64 syscall=openat success=yes exit=6 a0=0xffffffffffffff9c a1=0xc42038cae0 a2=O_RDONLY|O_CLOEXEC a3=0x0 items=1 ppid=20523 pid=1890 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=yes_i_changed_you exe=/root/yes_i_changed_you key=wiserun type=SYSCALL msg=audit(2018年06月20日 14:22:33.669:8379) : arch=x86_64 syscall=unlinkat success=no exit=EISDIR(是一个目录) a0=0xffffffffffffff9c a1=0xc42003e3c0 a2=0x0 a3=0x0 items=2 ppid=20523 pid=1890 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=yes_i_changed_you exe=/root/yes_i_changed_you key=wiserun type=SYSCALL msg=audit(2018年06月20日 14:22:33.669:8380) : arch=x86_64 syscall=unlinkat success=yes exit=0 a0=0xffffffffffffff9c a1=0xc42003e410 a2=0x200 a3=0x0 items=2 ppid=20523 pid=1890 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=yes_i_changed_you exe=/root/yes_i_changed_you key=wiserun type=SYSCALL msg=audit(2018年06月20日 14:22:33.669:8381) : arch=x86_64 syscall=unlinkat success=no exit=EISDIR(是一个目录) a0=0xffffffffffffff9c a1=0xc42038cb80 a2=0x0 a3=0x0 items=2 ppid=20523 pid=1890 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=yes_i_changed_you exe=/root/yes_i_changed_you key=wiserun type=SYSCALL msg=audit(2018年06月20日 14:22:33.669:8382) : arch=x86_64 syscall=unlinkat success=yes exit=0 a0=0xffffffffffffff9c a1=0xc42038cbc0 a2=0x200 a3=0x0 items=2 ppid=20523 pid=1890 auid=unset uid=root gid=root euid=root suid=root fsuid=root egid=root sgid=root fsgid=root tty=(none) ses=unset comm=yes_i_changed_you exe=/root/yes_i_changed_you key=wiserun ausearch语法 [root@deoops ~]# ausearch -h usage: ausearch [options] -a,--event \u0026lt;Audit event id\u0026gt; search based on audit event id --arch \u0026lt;CPU\u0026gt; search based on the CPU architecture -c,--comm \u0026lt;Comm name\u0026gt; search based on command line name --checkpoint \u0026lt;checkpoint file\u0026gt; search from last complete event --debug Write malformed events that are skipped to stderr -e,--exit \u0026lt;Exit code or errno\u0026gt; search based on syscall exit code -f,--file \u0026lt;File name\u0026gt; search based on file name -ga,--gid-all \u0026lt;all Group id\u0026gt; search based on All group ids -ge,--gid-effective \u0026lt;effective Group id\u0026gt; search based on Effective group id -gi,--gid \u0026lt;Group Id\u0026gt; search based on group id -h,--help help -hn,--host \u0026lt;Host Name\u0026gt; search based on remote host name -i,--interpret Interpret results to be human readable -if,--input \u0026lt;Input File name\u0026gt; use this file instead of current logs --input-logs Use the logs even if stdin is a pipe --just-one Emit just one event -k,--key \u0026lt;key string\u0026gt; search based on key field -l, --line-buffered Flush output on every line -m,--message \u0026lt;Message type\u0026gt; search based on message type -n,--node \u0026lt;Node name\u0026gt; search based on machine\u0026#39;s name -o,--object \u0026lt;SE Linux Object context\u0026gt; search based on context of object -p,--pid \u0026lt;Process id\u0026gt; search based on process id -pp,--ppid \u0026lt;Parent Process id\u0026gt; search based on parent process id -r,--raw output is completely unformatted -sc,--syscall \u0026lt;SysCall name\u0026gt; search based on syscall name or number -se,--context \u0026lt;SE Linux context\u0026gt; search based on either subject or object --session \u0026lt;login session id\u0026gt; search based on login session id -su,--subject \u0026lt;SE Linux context\u0026gt; search based on context of the Subject -sv,--success \u0026lt;Success Value\u0026gt; search based on syscall or event success value -te,--end [end date] [end time] ending date \u0026amp; time for search -ts,--start [start date] [start time] starting data \u0026amp; time for search -tm,--terminal \u0026lt;TerMinal\u0026gt; search based on terminal -ua,--uid-all \u0026lt;all User id\u0026gt; search based on All user id\u0026#39;s -ue,--uid-effective \u0026lt;effective User id\u0026gt; search based on Effective user id -ui,--uid \u0026lt;User Id\u0026gt; search based on user id -ul,--loginuid \u0026lt;login id\u0026gt; search based on the User\u0026#39;s Login id -uu,--uuid \u0026lt;guest UUID\u0026gt; search for events related to the virtual machine with the given UUID. -v,--version version -vm,--vm-name \u0026lt;guest name\u0026gt; search for events related to the virtual machine with the name. -w,--word string matches are whole word -x,--executable \u0026lt;executable name\u0026gt; search based on executable name TIPS 如果需要记录被审计对象被删除事件，则需要审计该对象的上一级目录。\nMonitor/audit file delete on Linux\n","date":"2018-06-20T17:02:58+08:00","image":"https://tab.deoops.com/posts/audit-file/audit_hu9569bb820aed3f9fb44d2a8c45252134_264385_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/audit-file/","title":"审计目录"},{"content":"众所周知centos的内核版本选择很保守，很多新内核的新特性，特别是网络和debug方面的特性都没有，所以我们来给centos升级下 kernel吧。\n整个升级安装的过程其实挺简单的一共分为4步：\n找到repo源； yum安装最新的kernel； 修改grub2启动项； 移除旧的kernel。 安装 elrepo 访问elrepo website查看对应 centos 版本最新的kernel repo源。 然后使用rpm添加kernel源：\nrpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm install 安装内核：\nyum --disablerepo=\u0026#34;\\*\u0026#34; --enablerepo=\u0026#34;elrepo-kernel\u0026#34; list available yum --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel boot 修改grub2启动项，开机使用新的内核：\nawk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print i++ \u0026#34; : \u0026#34; $2}\u0026#39; /etc/grub2.cfg grub2-set-default 0 init 6 cleanup 删除旧内核\nyum install yum-utils package-cleanup --oldkernels --count=1 uname -a 参考 How to Upgrade Kernel on CentOS 7\nHow to Install or Upgrade to Kernel 4.15 in CentOS 7\n","date":"2018-05-16T16:50:31+08:00","image":"https://tab.deoops.com/posts/kernel-upgrade/kernel_hu93aa80e4f919fc7fd0c6fdaad113f462_172612_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/kernel-upgrade/","title":"内核升级"},{"content":"使用vim有7，8年了，整理一下自己用到的vim配置，方便自查。\nvim basic 基本设置，语法高亮，行号，等等：\nexecute pathogen#infect() syntax on filetype plugin indent on set number paste 复制粘贴： disable auto indent\nset paste Notice the “– INSERT (paste) –” at the bottom of the Vim window.\nfold 代码折叠： Folding wiki\n\u0026#34; close zc \u0026#34; open zo font 安装字体： install powerline font\nwget https://github.com/powerline/powerline/raw/develop/font/PowerlineSymbols.otf open . # double click the otf file you\u0026#39;ve just downloaded. kubectl eidt issue 使用kubectl edit时，当yaml文件的annotation行过大会报错:\nannotation: kubectl.kubernetes.io/last-applied-configuration is too large\n\u0026#34; annotation: kubectl.kubernetes.io/last-applied-configuration \u0026#34; is too large set maxmempattern=200000 neovim neovim v0.5.1 今年（2020），我从vim 8 迁移到了neovim，下面是 ~/.config/nvim/init.vim的配置内容\n\u0026#34; cp from ~/.vimrc \u0026#34; execute pathogen#infect() \u0026#34;syntax on \u0026#34;filetype plugin indent on language en_US.UTF-8 set rtp+=/usr/local/opt/fzf set number set encoding=utf-8 \u0026#34; You need to set ignorecase if you want to use what smartcase provides set ignorecase set smartcase \u0026#34; kubectl edit \u0026#34; annotation: kubectl.kubernetes.io/last-applied-configuration \u0026#34; is too large set maxmempattern=200000 \u0026#34;set foldmethod=syntax \u0026#34;set clipboard=unnamed \u0026#34;autocmd vimenter * NERDTree \u0026#34; \u0026#34;autocmd StdinReadPre * let s:std_in=1 \u0026#34;autocmd VimEnter * if argc() == 0 \u0026amp;\u0026amp; !exists(\u0026#34;s:std_in\u0026#34;) | NERDTree | endif autocmd StdinReadPre * let s:std_in=1 autocmd VimEnter * if argc() == 1 \u0026amp;\u0026amp; isdirectory(argv()[0]) \u0026amp;\u0026amp; !exists(\u0026#34;s:std_in\u0026#34;) | exe \u0026#39;NERDTree\u0026#39; argv()[0] | wincmd p | ene | exe \u0026#39;cd \u0026#39;.argv()[0] | endif call plug#begin(stdpath(\u0026#39;data\u0026#39;) .\u0026#39;/plugged\u0026#39;) \u0026#34; call plug#begin(\u0026#39;~/.vim/plugged\u0026#39;) \u0026#34; Make sure you use single quotes \u0026#34; Plug \u0026#39;tpope/vim-commentary\u0026#39; Plug \u0026#39;vim-airline/vim-airline\u0026#39; Plug \u0026#39;vim-airline/vim-airline-themes\u0026#39; \u0026#34; \u0026#34; Shorthand notation; fetches https://github.com/junegunn/vim-easy-align Plug \u0026#39;junegunn/vim-easy-align\u0026#39; \u0026#34; \u0026#34; \u0026#34; Any valid git URL is allowed Plug \u0026#39;https://github.com/junegunn/vim-github-dashboard.git\u0026#39; \u0026#34; \u0026#34; \u0026#34; Multiple Plug commands can be written in a single line using | separators Plug \u0026#39;SirVer/ultisnips\u0026#39; | Plug \u0026#39;honza/vim-snippets\u0026#39; \u0026#34; \u0026#34; \u0026#34; On-demand loading Plug \u0026#39;scrooloose/nerdtree\u0026#39;, { \u0026#39;on\u0026#39;: \u0026#39;NERDTreeToggle\u0026#39; } Plug \u0026#39;preservim/tagbar\u0026#39; \u0026#34; Plug \u0026#39;tpope/vim-fireplace\u0026#39;, { \u0026#39;for\u0026#39;: \u0026#39;clojure\u0026#39; } \u0026#34; \u0026#34; \u0026#34; Using a non-default branch Plug \u0026#39;rdnetto/YCM-Generator\u0026#39;, { \u0026#39;branch\u0026#39;: \u0026#39;stable\u0026#39; } \u0026#34; \u0026#34; \u0026#34; Using a tagged release; wildcard allowed (requires git 1.9.2 or above) Plug \u0026#39;fatih/vim-go\u0026#39;, { \u0026#39;do\u0026#39;: \u0026#39;:GoUpdateBinaries\u0026#39; } \u0026#34; \u0026#34; \u0026#34; \u0026#34; Plugin outside ~/.vim/plugged with post-update hook Plug \u0026#39;junegunn/fzf\u0026#39;, { \u0026#39;dir\u0026#39;: \u0026#39;~/.fzf\u0026#39;, \u0026#39;do\u0026#39;: \u0026#39;./install --all\u0026#39; } Plug \u0026#39;junegunn/fzf.vim\u0026#39; \u0026#34; Track the engine. Plug \u0026#39;SirVer/ultisnips\u0026#39; \u0026#34; \u0026#34; Snippets are separated from the engine. Add this if you want them: Plug \u0026#39;honza/vim-snippets\u0026#39; if has(\u0026#39;nvim\u0026#39;) Plug \u0026#39;Shougo/deoplete.nvim\u0026#39;, { \u0026#39;do\u0026#39;: \u0026#39;:UpdateRemotePlugins\u0026#39; } else Plug \u0026#39;Shougo/deoplete.nvim\u0026#39; Plug \u0026#39;roxma/nvim-yarp\u0026#39; Plug \u0026#39;roxma/vim-hug-neovim-rpc\u0026#39; endif Plug \u0026#39;tpope/vim-sensible\u0026#39; Plug \u0026#39;rust-lang/rust.vim\u0026#39; \u0026#34; for reactjs jsx \u0026#34; \u0026#34; \u0026#34; Plug \u0026#39;pangloss/vim-javascript\u0026#39; Plug \u0026#39;leafgarland/typescript-vim\u0026#39; Plug \u0026#39;peitalin/vim-jsx-typescript\u0026#39; Plug \u0026#39;styled-components/vim-styled-components\u0026#39;, { \u0026#39;branch\u0026#39;: \u0026#39;main\u0026#39; } \u0026#34; Plug \u0026#39;autozimu/LanguageClient-neovim\u0026#39;, {\u0026#39;branch\u0026#39;: \u0026#39;next\u0026#39;, \u0026#39;do\u0026#39;: \u0026#39;bash install.sh\u0026#39; } \u0026#34;Plug \u0026#39;jparise/vim-graphql\u0026#39; \u0026#34;Plug \u0026#39;neoclide/coc.nvim\u0026#39;, {\u0026#39;branch\u0026#39;: \u0026#39;release\u0026#39;} call plug#end() \u0026#34; let g:coc_global_extensions = [\u0026#39;coc-json\u0026#39;, \u0026#39;coc-tsserver\u0026#39;] \u0026#34; for js/ts synax \u0026#34; syntax highlighting can get out of sync i \u0026#34; https://thoughtbot.com/blog/modern-typescript-and-react-development-in-vim autocmd BufEnter *.{js,jsx,ts,tsx} :syntax sync fromstart autocmd BufLeave *.{js,jsx,ts,tsx} :syntax sync clear let g:deoplete#enable_at_startup = 1 let g:go_def_mode=\u0026#39;gopls\u0026#39; let g:go_info_mode=\u0026#39;gopls\u0026#39; let g:UltiSnipsEditSplit=\u0026#34;vertical\u0026#34; call deoplete#custom#option(\u0026#39;omni_patterns\u0026#39;, { \u0026#39;go\u0026#39;: \u0026#39;[^. *\\t]\\.\\w*\u0026#39; }) \u0026#34;set list \u0026#34;set listchars=tab:\u0026gt;- set tabstop=4 \u0026#34; \u0026#34;set expandtab \u0026#34;set shiftwidth=2 set autoindent set smartindent \u0026#34; let mapleader=\u0026#39;,\u0026#39;\tnmap \u0026lt;C-P\u0026gt; :FZF\u0026lt;CR\u0026gt; let NERDTreeShowHidden=1 \u0026#34; If more than one window and previous buffer was NERDTree, go back to it. autocmd BufEnter * if bufname(\u0026#39;#\u0026#39;) =~# \u0026#34;^NERD_tree_\u0026#34; \u0026amp;\u0026amp; winnr(\u0026#39;$\u0026#39;) \u0026gt; 1 | b# | endif let g:plug_window = \u0026#39;noautocmd vertical topleft new\u0026#39; if has(\u0026#39;nvim\u0026#39;) \u0026#34;highlight! link TermCursor Cursor highlight! TermCursorNC guibg=red guifg=white ctermbg=1 ctermfg=15 endif set clipboard+=unnamedplus let g:go_imports_autosave = 1 \u0026#34; nmap \u0026lt;silent\u0026gt; gd \u0026lt;Plug\u0026gt;(coc-definition) \u0026#34; nmap \u0026lt;silent\u0026gt; gy \u0026lt;Plug\u0026gt;(coc-type-definition) \u0026#34; nmap \u0026lt;silent\u0026gt; gr \u0026lt;Plug\u0026gt;(coc-references) \u0026#34; nmap \u0026lt;leader\u0026gt;rn \u0026lt;Plug\u0026gt;(coc-rename) \u0026#34;let g:LanguageClient_serverCommands = {\u0026#39;javascript\u0026#39;: [\u0026#39;javascript-typescript-stdio\u0026#39;]} ","date":"2018-05-14T11:42:23+08:00","image":"https://tab.deoops.com/posts/vim-conf/vim_huc1453ee2b26537225217496510c9e8ad_145424_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/vim-conf/","title":"vim配置"},{"content":"离职一段时间了，需要自己写点前端代码。 奈何vim的js插件对jsx的支持不太友好，所以转向vscode写点jsx。\n写了些react app代码后，IDE到处飘红色的波浪线〰️〰️〰️，很是恼人。\n全局配置react eslint好多了， 记录下配置的过程备查。\n配置 基本上是用了airbnb的配置：\n具体的步骤很简单，两步就好了：\nnpm安装eslint和要用到plugin； 根据需求配置全局的eslintrc文件 plugin npm install -g jshint npm install -g eslint eslint-config-airbnb-base eslint-plugin-import vi .eslintrc.js ls -alh /usr/local/bin/npm ls /usr/local/lib/node_modules/eslint-config-airbnb-base npm link eslint-config-airbnb-base ls node_modules npm link eslint-plugin-import npm i -g eslint-plugin-react npm i -g eslint-plugin-jsx-a11y npm link eslint-plugin-jsx-a11y eslint-plugin-react vi .eslintrc.js .elinttc.js // ~/.eslintrc.js module.exports = { parser: \u0026#34;babel-eslint\u0026#34;, \u0026#34;plugins\u0026#34;: [\u0026#34;react\u0026#34;], \u0026#34;extends\u0026#34;: [ \u0026#34;airbnb-base\u0026#34;, \u0026#34;eslint:recommended\u0026#34;, \u0026#34;plugin:react/recommended\u0026#34;, ], \u0026#34;rules\u0026#34;: { // \u0026#34;no-unused-vars\u0026#34;:0, \u0026#34;no-console\u0026#34;: \u0026#39;off\u0026#39;, \u0026#34;max-len\u0026#34;: [1,120,2,{ignoreComments: true}] // \u0026#34;prop-types\u0026#34;: [2] }, \u0026#34;env\u0026#34;: { \u0026#34;browser\u0026#34;: true, \u0026#34;node\u0026#34;: true, \u0026#34;jasmine\u0026#34;: true } }; 参考 react eslint webpack babel\nUse ESLint Like a Pro with ES6 and React\n","date":"2018-04-16T16:34:09+08:00","image":"https://tab.deoops.com/posts/vscode-eslint/eslint-airbnb_hue35271ac4524df007877384f8b1c06a2_74499_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/vscode-eslint/","title":"配置vscode eslint"},{"content":"今天在digitocean一台新申请的主机上部署web应用。部署完成，打开浏览器发现报错403。\n部署的web应用很简单，后端用nginx做了反向代理，应该没啥大问题。\n进一步打开chrome的console，发现对static file的访问报错403，还没到后端就已经报错了， 估计后面的 upstream socket也会报错。\nssh登陆到服务器上看了下nginx的日志，发现是权限的问题。\n进一步debug了之后发现虚拟机开启selinux，当时心头就一紧，估计要改selinux配置了，这是个麻烦事儿。\n想简单点直接关闭selinux，转念一想digitocean的主机直接暴露在interner上，开着selinux 其实是个很好的保护。digitocean打开自有它打开的道理，我猜可能有很多vm被攻破沦为僵尸网络了。\ngoogle搜索了selinux的web server常用的配置，验证后，解决了nginx 403的问题。记录分享如下：\ncheck seLinux 查看系统状态 查看selinux配置和状态\n[root@deo ~]# sestatus -v SELinux status: enabled SELinuxfs mount: /sys/fs/selinux SELinux root directory: /etc/selinux Loaded policy name: targeted Current mode: enforcing Mode from config file: enforcing Policy MLS status: enabled Policy deny_unknown status: allowed Max kernel policy version: 28 Process contexts: Current context: unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 Init context: system_u:system_r:init_t:s0 /usr/sbin/sshd system_u:system_r:sshd_t:s0-s0:c0.c1023 File contexts: Controlling terminal: unconfined_u:object_r:user_devpts_t:s0 /etc/passwd system_u:object_r:passwd_file_t:s0 /etc/shadow system_u:object_r:shadow_t:s0 /bin/bash system_u:object_r:shell_exec_t:s0 /bin/login system_u:object_r:login_exec_t:s0 /bin/sh system_u:object_r:bin_t:s0 -\u0026gt; system_u:object_r:shell_exec_t:s0 /sbin/agetty system_u:object_r:getty_exec_t:s0 /sbin/init system_u:object_r:bin_t:s0 -\u0026gt; system_u:object_r:init_exec_t:s0 /usr/sbin/sshd system_u:object_r:sshd_exec_t:s0 查看文件目录的selinux label [root@deo ]# ls -Z /opt/todolist -rw-r--r--. root root unconfined_u:object_r:admin_home_t:s0 index.html drwxr-xr-x. root root unconfined_u:object_r:admin_home_t:s0 static [root@deo ]# ls -Z /usr/share/nginx/html -rw-r--r--. root root system_u:object_r:httpd_sys_content_t:s0 50x.html -rw-r--r--. root root system_u:object_r:httpd_sys_content_t:s0 index.html meat 下面两种方法选一种就可以了\nlabel 修改目录label使得selinux 放行nginx:\nchcon -Rt httpd_sys_content_t /opt/todolist/ ls -Z /opt/todolist/ -rw-r--r--. root root unconfined_u:object_r:httpd_sys_content_t:s0 index.html drwxr-xr-x. root root unconfined_u:object_r:httpd_sys_content_t:s0 static setsebool -P httpd_can_network_connect 1 ; setsebool -P httpd_enable_homedirs on chmod 701 /home/dir semanage 或者使用semanage命令修改enforcement mode：\n~ [root@jp ~]# cat fix_selinux_ng.sh semanage permissive -a httpd_tp segmanage语法：\n[root@jp ~]# semanage -h usage: semanage [-h] {import,export,login,user,port,ibpkey,ibendport,interface,module,node,fcontext,boolean,permissive,dontaudit} ... semanage is used to configure certain elements of SELinux policy with-out requiring modification to or recompilation from policy source. positional arguments: {import,export,login,user,port,ibpkey,ibendport,interface,module,node,fcontext,boolean,permissive,dontaudit} import Import local customizations export Output local customizations login Manage login mappings between linux users and SELinux confined users user Manage SELinux confined users (Roles and levels for an SELinux user) port Manage network port type definitions ibpkey Manage infiniband ibpkey type definitions ibendport Manage infiniband end port type definitions interface Manage network interface type definitions module Manage SELinux policy modules node Manage network node type definitions fcontext Manage file context mapping definitions boolean Manage booleans to selectively enable functionality permissive Manage process type enforcement mode dontaudit Disable/Enable dontaudit rules in policy optional arguments: -h, --help show this help message and exit 参考 Fixing 403 errors when using nginx with SELinux\nNGinX cannot connect to Jenkins on CentOS 7\nNginx refuses to read new directory in /home\n","date":"2018-04-16T16:10:45+08:00","image":"https://tab.deoops.com/posts/http-selinux/selinux_hu2703889cd274f47cf99c8f755f05e2d0_11156_120x120_fill_q75_h2_box_smart1_2.webp","permalink":"https://tab.deoops.com/posts/http-selinux/","title":"配置http selinux"},{"content":"默认配置命令git log会在新的窗口打印日志内容，需要敲一下键盘q 才能返回当前目录，不方便连续查看:\n➜ lgthw_orign git:(otherbranch) git log --oneline --decorate --all --graph ## NOTE content below will be displayed on new window/buff * 40303b7 (HEAD -\u0026gt; otherbranch) thirdcommit | * 3e6e2f7 (master) secondcommit |/ * f40475e (tag: firstcommittag) firstcommit (END) ## press `q` to exist ➜ lgthw_orign git:(master) git log --no-pager fatal: unrecognized argument: --no-pager 可以把默认的分页改为inline模式，可以更快的查看连续的日志：\nmeat # use --no-pager options # or set pager to cat git config --global core.pager cat # or set pager to less # git config --global core.pager \u0026#34;less -erX\u0026#34; demo ➜ lgthw_orign git:(master) git log --oneline --decorate --all --graph * f40475e (HEAD -\u0026gt; master) firstcommit ➜ lgthw_orign git:(master) git branch otherbranch ➜ lgthw_orign git:(master) git tag firstcommittag ➜ lgthw_orign git:(master) git log --oneline --decorate --all --graph * f40475e (HEAD -\u0026gt; master, tag: firstcommittag, otherbranch) firstcommit ➜ lgthw_orign git:(master) date \u0026gt;\u0026gt; afile ➜ lgthw_orign git:(master) ✗ git commit -am secondcommit [master 3e6e2f7] secondcommit 1 file changed, 1 insertion(+) ➜ lgthw_orign git:(master) git checkout . ➜ lgthw_orign git:(master) git log --oneline --decorate --all --graph * 3e6e2f7 (HEAD -\u0026gt; master) secondcommit * f40475e (tag: firstcommittag, otherbranch) firstcommit ➜ lgthw_orign git:(master) git checkout otherbranch Switched to branch \u0026#39;otherbranch\u0026#39; ➜ lgthw_orign git:(otherbranch) git log --oneline --decorate --all --graph * 3e6e2f7 (master) secondcommit * f40475e (HEAD -\u0026gt; otherbranch, tag: firstcommittag) firstcommit ➜ lgthw_orign git:(otherbranch) date \u0026gt;\u0026gt; afile ➜ lgthw_orign git:(otherbranch) ✗ git commit -am thirdcommit [otherbranch 40303b7] thirdcommit 1 file changed, 1 insertion(+) ➜ lgthw_orign git:(otherbranch) git log --oneline --decorate --all --graph * 40303b7 (HEAD -\u0026gt; otherbranch) thirdcommit | * 3e6e2f7 (master) secondcommit |/ * f40475e (tag: firstcommittag) firstcommit 参考 Changing the Display of Git Log\nFYI, cat is not the ideal pager for me, since it displays the full git log from the beginning if I don\u0026rsquo;t append a -1 in the end of the command. more was not a good candidate either, since colors was not well displayed in the console with more I preferred to keep less as the pager, but display content in the console. So for me : git config \u0026ndash;global core.pager \u0026ldquo;less -erX\u0026rdquo; (important option here is the -X option)\n","date":"2018-04-14T10:38:59+08:00","image":"https://tab.deoops.com/posts/git-pager/pager_hu4a9bc118edabcc52dadf803528096b0c_288981_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/git-pager/","title":"分页打印日志"},{"content":"遇到一个奇怪的问题执行certbot会报错，moudle conflict和 No module，但是yum install certbot的时候没有报错。\ncertbot Traceback (most recent call last): File \u0026#34;/bin/certbot\u0026#34;, line 7, in \u0026lt;module\u0026gt; from certbot.main import main File \u0026#34;/usr/lib/python2.7/site-packages/certbot/main.py\u0026#34;, line 17, in \u0026lt;module\u0026gt; from certbot import account File \u0026#34;/usr/lib/python2.7/site-packages/certbot/account.py\u0026#34;, line 17, in \u0026lt;module\u0026gt; from acme import messages File \u0026#34;/usr/lib/python2.7/site-packages/acme/messages.py\u0026#34;, line 7, in \u0026lt;module\u0026gt; from acme import challenges File \u0026#34;/usr/lib/python2.7/site-packages/acme/challenges.py\u0026#34;, line 11, in \u0026lt;module\u0026gt; import requests File \u0026#34;/usr/lib/python2.7/site-packages/requests/__init__.py\u0026#34;, line 58, in \u0026lt;module\u0026gt; from . import utils File \u0026#34;/usr/lib/python2.7/site-packages/requests/utils.py\u0026#34;, line 32, in \u0026lt;module\u0026gt; from .exceptions import InvalidURL File \u0026#34;/usr/lib/python2.7/site-packages/requests/exceptions.py\u0026#34;, line 10, in \u0026lt;module\u0026gt; from .packages.urllib3.exceptions import HTTPError as BaseHTTPError File \u0026#34;/usr/lib/python2.7/site-packages/requests/packages/__init__.py\u0026#34;, line 95, in load_module raise ImportError(\u0026#34;No module named \u0026#39;%s\u0026#39;\u0026#34; % (name,)) ImportError: No module named \u0026#39;requests.packages.urllib3\u0026#39; pip install requests urllib3 pyOpenSSL --force --upgrade certbot An unexpected error occurred: VersionConflict: (setuptools 0.9.8 (/usr/lib/python2.7/site-packages), Requirement.parse(\u0026#39;setuptools\u0026gt;=1.0\u0026#39;)) pip install --upgrade pip setuptools certbot ls meat 弄了很久决定抛弃yum直接使用 pip安装certbot，安装完成后，发现不再报错：\nyum install openssl-devel python-devel pip install --upgrade pip setuptools pip install certbot pip install requests urllib3 pyOpenSSL --force --upgrade certbot -d tab.deoops.com --manual --preferred-challenges dns certonly 小结 本来是不想写这篇博文的，估摸着letsencrypt官方 迟早会修复certbot rpm包的，结果一等就是两个月， letsencrypt都没修复 rpm包。\n最近多台服务器的certbot安装又遇到这个不兼容的问题，每次去其它的服务器上找command history有点麻烦，所以记下来方便查找。\n","date":"2018-03-30T10:21:25+08:00","image":"https://tab.deoops.com/posts/python-import/pip_hu14873f9ce72059666dd108380cb75b5f_111886_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/python-import/","title":"pip包冲突"},{"content":"客户需要部署一套 moodle 教学系统。\n去moodle官网大致看了一圈，发现moodle 是一个典型的PHP web应用。\n其实这种LAMP (Linux, Apache, MySQL, PHP/Perl/Python)的应用，\n我一般会用docker componse快速部署的，比如这个docker componse看上去就很不错。\n但是客户不想用docker，要求直接在vm上部署。\n初步确认部署环境为： nginx(let's encrypt) + php 7.2 + pg 10 + Centos 7.4 。\n安装软件 初始化主机 hostnamectl set-hostname deoops.com # disable passwd login; use ssh-key only vi /etc/ssh/sshd_config yum update -y yum upgrade -y init 6 # add remi repo rpm -Uvh https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm rpm -Uvh http://rpms.famillecollet.com/enterprise/remi-release-7.rpm 安装nginx yum install nginx yum -y install yum-utils yum-config-manager --enable rhui-REGION-rhel-server-extras rhui-REGION-rhel-server-optional ## 安装let\u0026#39;s encrypt certbot yum install certbot-nginx systemctl enable nginx systemctl start nginx ## 签发证书 certbot --nginx certonly ls -alh /etc/nginx/ 安装php dependency yum --enablerepo=remi,remi-php72 install php-fpm php-common php-opcache php-pecl-apcu php-cli php-pear php-pdo php-mysqlnd php-pgsql php-pecl-mongodb php-pecl-redis php-pecl-memcache php-pecl-memcached php-gd php-mbstring php-mcrypt php-xml 配置nginx + php-fpm 详细的配置内容看这里\nvi /etc/nginx/nginx.conf ls mv mood.conf /etc/nginx/conf.d/ ls -ahl /etc/php-fpm.d/ mv www.conf /etc/php-fpm.d/ mv php.ini /etc/ systemctl start php-fpm.service systemctl enable php-fpm.service 安装postgresql 10 fdisk -l bash newDisk.sh /dev/vdb mkdir /media/data/mood mkdir /media/data/pgdata yum install https://download.postgresql.org/pub/repos/yum/10/redhat/rhel-7-x86_64/pgdg-centos10-10-2.noarch.rpm -y yum install postgresql10 -y yum install postgresql10-server postgresql10-contrib -y ls -alh /var/lib/pgsql/10/data/ chown -R postgres:postgres /media/data/pgdata vi /usr/lib/systemd/system/postgresql-10.service /usr/pgsql-10/bin/postgresql-10-setup initdb systemctl enable postgresql-10 systemctl start postgresql-10 配置postgresql hba netstat -nlp vi /media/data/pgdata/pg_hba.conf systemctl restart postgresql-10 netstat -nlp 安装moodle 34 ls mv moodle-latest-34.tgz zh_cn.zip /media/data/mood/ cd /media/data/mood/ ls tar xzf moodle-latest-34.tgz vi /etc/nginx/conf.d/mood.conf chown -R nginx:nginx /media/data/mood ls -alh /var/lib/php/session/ ls -alh /run/php-fpm/ netstat -nlp | grep php chown -R nginx:nginx /var/lib/php/session/ systemctl restart php-fpm su - postgres 安装moodle依赖包 安装 zip,xmlrpc,soap等依赖包：\nyum --enablerepo=remi,remi-php72 install php72-php-zip systemctl restart php-fpm.service systemctl restart nginx.service init 6 yum --enablerepo=remi,remi-php72 install php72-php-pecl-zip yum --enablerepo=remi,remi-php72 install php-zip systemctl restart php-fpm.service systemctl restart nginx yum --enablerepo=remi,remi-php72 install php-intl yum --enablerepo=remi,remi-php72 install phpxmlrpc yum --enablerepo=remi,remi-php72 install php-xmlrpc yum --enablerepo=remi,remi-php72 install php-soap systemctl restart nginx systemctl restart php-fpm.service 配置 配置nginx php-fpm moodle使用的nginx 配置，基本适用于所有的php 应用：\n# PHP Upstream Handler upstream php-handler { server unix:/run/php-fpm/php-fpm.sock; } server { server_name moodle-demo.deoops.com; ssl on; listen [::]:443 ssl ipv6only=on; # managed by Certbot listen 443 ssl; # managed by Certbot ssl_certificate /etc/letsencrypt/live/moodle-demo.deoops.com/fullchain.pem; # managed by Certbot ssl_certificate_key /etc/letsencrypt/live/moodle-demo.deoops.com/privkey.pem; # managed by Certbot include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot root /media/data/mood/moodle; rewrite ^/(.*\\.php)(/)(.*)$ /$1?file=/$3 last; location ^~ / { try_files $uri $uri/ /index.php?q=$request_uri; index index.php index.html index.htm; location ~ \\.php$ { include fastcgi.conf; fastcgi_pass php-handler; } } } # http -\u0026gt; https server { if ($host = moodle-demo.deoops.com) { return 301 https://$host$request_uri; } # managed by Certbot listen 80 ; listen [::]:80 ; server_name moodle-demo.deoops.com; return 404; # managed by Certbot } 配置https证书 配置let\u0026rsquo;s encrypt自更新crontab job:\ncertbot renew --dry-run crontab -e crontab -l 0 0,12 * * * python -c \u0026#39;import random; import time; time.sleep(random.random() * 3600)\u0026#39; \u0026amp;\u0026amp; certbot renew 参考 moodle: set up postgresql counts and database\nPHP(php-fpm) nginx\n","date":"2018-03-20T13:58:27+08:00","image":"https://tab.deoops.com/posts/install-moodle/moodle_huc8bc81a9f0a50eb186d9f1a801c3ecb2_27882_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/install-moodle/","title":"部署moodle"},{"content":"工作需要定时备份postgresql slave数据库数据数据，服务器上运行了两个slave实例，隶属于两个不同的master。\n备份 两个slave server实例分别监听在 5432和 4432端口\n#!/bin/bash # # Daily PostgreSQL maintenance: vacuuming and backuping. # ## set -e for port in 5432 4432; do BACKDIR=\u0026#34;/data/pg_back/$port\u0026#34; [ -d $BACKDIR ] || mkdir -p $BACKDIR echo \u0026#34;[`date`] begin Maintaining pg on port $port\u0026#34; # no need to use -U option for DB in $(psql -l -t -p $port |awk \u0026#39;{ print $1}\u0026#39; |grep -vE \u0026#39;^-|:|^List|^Name|template[0|1]|postgres|\\|\u0026#39;); do ### swith form \u0026#39;awk and grep\u0026#39; hacks to psql options and \u0026#39;select sql\u0026#39; ### which is more dbaer professioner :) for DB in $(psql -AqXtc \u0026#39;SELECT datname FROM pg_database WHERE datistemplate = false;\u0026#39;); do echo \u0026#34; [`date`] Maintaining $DB\u0026#34; PREFIX=\u0026#34;$BACKDIR/$DB\u0026#34; # NO need to do `vacuum` on slaves # do `vacunm` on master instead # echo \u0026#39;VACUUM\u0026#39; | psql -U postgres -hlocalhost -d $DB DUMP=\u0026#34;$PREFIX.`date \u0026#39;+%Y%m%d\u0026#39;`.sql.gz\u0026#34; # no need for -U postgres option pg_dump -p $port $DB | gzip -c \u0026gt; $DUMP PREV=\u0026#34;$PREFIX.`date -d\u0026#39;1 day ago\u0026#39; \u0026#39;+%Y%m%d\u0026#39;`.sql.gz\u0026#34; # md5sum -b $DUMP \u0026gt; $DUMP.md5 md5=($(md5sum -b $DUMP)) echo $md5 \u0026gt; $DUMP.md5 if [ -f $PREV.md5 ] \u0026amp;\u0026amp; diff $PREV.md5 $DUMP.md5; then rm -f $PREV $PREV.md5 fi ## delete too old backup TOOOLD=\u0026#34;$PREFIX.`date -d\u0026#39;15 day ago\u0026#39; \u0026#39;+%Y%m%d\u0026#39;`.sql.gz\u0026#34; [ -f $TOOOLD ] || rm -f $TOOOLD done echo \u0026#34;[`date`] Maintain pg on port $port finished\u0026#34; done 参考：Automatic Offsite PostgreSQL Backups Without a Password\n参考：Only get hash value using md5sum (without filename) 定时 chmod +x pg_backup.sh su - postgres crontab -e # 16 2 * * * /var/lib/pgsql/pg_backup.sh \u0026gt;\u0026gt; /var/log/psql_corn_bak.log # */3 * * * * /var/lib/pgsql/pg_backup.sh \u0026gt;\u0026gt; /var/log/psql_corn_bak.log 2\u0026gt;\u0026amp;1 ## every 3 minutes psql 一般情况下psql的工作模式是和人的相互交互模式(interpreter)，在shell脚本里可以使用下面的options -AqXt -c 会更实用写\n-A: The output is not aligned; by default, the output is aligned. -q (quiet): This option forces psql not to write a welcome message or any other informational output. -t: This option tells psql to write the tuples only, without any header information. -X: This option informs psql to ignore the psql configuration that is stored in ~/.psqlrc file. -o: This option specifies psql to output the query result to a certain location. -F: This option determines the field separator between columns. This option can be used to generate CSV, which is useful to import data to Excel files. PGOPTIONS: psql can use PGOPTIONS to add command-line options to send to the server at runtime. This can be used to control statement behavior such as to allow index scan only or to specify the statement timeout. demo #!/bin/bash connection_number=`PGOPTIONS=\u0026#39;--statement_timeout=0\u0026#39; psql -AqXt -c\u0026#34;SELECT count(*) FROM pg_stat_activity\u0026#34;` # The result of the command psql -AqXt –d postgres -c \u0026#34;SELECT count(*) FROM pg_stat_activity\u0026#34; is assigned to a bash variable. #The options -AqXt, as discussed previously, cause psql to return only the result without any decoration, as follows: psql -AqXt -c \u0026#34;SELECT count(*) FROM pg_stat_activity\u0026#34; 1 ","date":"2018-02-24T11:21:55+08:00","image":"https://tab.deoops.com/posts/postgresql-backup/postgresql-backup_hu30910665284931ace4f57faa1e01d828_94157_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/postgresql-backup/","title":"数据库备份"},{"content":"一般postgres高可用集群是一个master配一个slave，但是开发这边需要做db的读写分离，所以运维这边 又添加了一台slave专门暴露出来做读操作。原来的slave还是只做备份。\nHA 一主一从高可用的配置可以参考下面这篇文章 postgres streaming replication，有时间的话我可能会搬运一下 :)\n安装配置 因为pg数据库集群已经配置好了一主一从，所以在master主机上不需要配置pg_hba.conf, 或者CREATE ROLE等等。\n添加第二个slave需要注意以下两点：\n等待pg_basebackupreplicas stream数据同步完成后，再启动 postgresql-9.6 service; 修改PG_DATA_DIR目录的权限; rm /etc/yum.repos.d/pgdg-96-redhat.repo yum install https://download.postgresql.org/pub/repos/yum/9.6/redhat/rhel-7-x86_64/pgdg-centos96-9.6-3.noarch.rpm yum install -y postgresql96 yum install -y postgresql96-server yum install -y postgresql96-contrib vi /usr/lib/systemd/system/postgresql-9.6.service mkdir /data/pg9.6 chown postgres:postgres /data/pg9.6/ ls -alh /data/pg9.6/ pg_basebackup --help ## you can add option --checkpoint=fast for an instance backup ## qhich is not recommend pg_basebackup -X stream -D /data/pg9.6/ -P -R -h 10.3.3.3 -U replicator ls /data/pg9.6/ cat /data/pg9.6/recovery.conf vi /data/pg9.6/postgresql.conf pwd systemctl start postgresql-9.6.service ls -alh /data/pg9.6/ chown -R postgres:postgres /data/pg9.6 chmod 700 /data/pg9.6 systemctl start postgresql-9.6.service netstat -nlp | grep 5432 su - postgres systemctl enable postgresql-9.6.service check 在master主机上查看pg_stat_replication表数据，验证第二个slave是否正常工作：\n# on MASTER machine [dba_lol@pg_master ~]$ sudo su - postgres 上一次登录：五 2月 23 17:12:02 HKT 2018pts/1 上 -bash-4.2$ psql psql (9.6.6) Type \u0026#34;help\u0026#34; for help. postgres=# select * from pg_stat_replication; pid | usesysid | usename | application_name | client_addr | client_hostname | client_port | backend_start | backend_xmin | state | sent_location | write_location | flush_location | replay_location | sync_priorit y | sync_state ------+----------+---------+------------------+-------------+-----------------+-------------+-------------------------------+--------------+-----------+---------------+----------------+----------------+-----------------+------------- --+------------ 6720 | 19367 | replicator | walreceiver | 10.2.2.2 | | 49308 | 2018-01-24 18:20:47.114058+08 | | streaming | 82/6B07DD60 | 82/6B07DD60 | 82/6B07DD60 | 82/6B07DD20 | 0 | async 446 | 19367 | replicator | walreceiver | 10.2.2.4 | | 54836 | 2018-02-21 13:21:39.420745+08 | | streaming | 82/6B07DD60 | 82/6B07DD60 | 82/6B07DD60 | 82/6B07DD20 | 0 | async (2 rows) postgres=# PS add slave to slave 编辑recover.conf 文件 的 primary_conninfo地址信息可以实现slave -\u0026gt; slave的数据同步。\n","date":"2018-02-05T21:55:41+08:00","image":"https://tab.deoops.com/posts/postgre-two-slaves/postgres-replication_huca2c20491ce105e13b07fad9910c3805_99263_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/postgre-two-slaves/","title":"两个奴隶"},{"content":"几天之前试用过了kong效果不理想 ，今天来使用下小米出品（存疑？）的 orange网关。\n一个明显的区别是 kong的后端存储使用了postgresql，orange使用的是mysql。\n好了，废话不多说，贴出安装部署的过程如下：\n安装 mysql #!/bin/bash wget http://repo.mysql.com/mysql-community-release-el7-5.noarch.rpm sudo rpm -ivh mysql-community-release-el7-5.noarch.rpm sudo yum update sudo yum install mysql-server sudo systemctl start mysqld sudo systemctl enable mysqld sudo mysql_secure_installation vi save_your_root_pwd git clone https://github.com/sumory/orange.git cd orange/ ls cd install/ ls head orange-v0.6.4.sql head -n 100 orange-v0.6.4.sql head -n 50 orange-v0.6.4.sql mysql -V mysql -u root -p cd orange/ cd install/ ls mysql -u o_usr -p o_database \u0026lt; orange-v0.6.4.sql mysql -u o_usr -p o_database orange #!/bin/bash yum remove kong-community-edition ## get rid of annoying lua 5.1 version conflict cd /data/nginx/conf/ cp api.conf api.conf.bak.18.03.09 ## always backup conf files : ) nginx -s stop netstat -nlp | grep 443 mv /usr/sbin/nginx /usr/sbin/nginx_old yum install yum-utils yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo yum install openresty openresty-resty -y git clone https://github.com/sumory/orange.git git clone https://github.com/sumory/lor.git pwd mv lor ~ mv orange ~ cd ls cd lor/ make install cd ../orange/ make install cd ln -s /usr/local/bin/orange /bin/orange ln -s /usr/local/openresty/nginx/sbin/nginx /bin/nginx mv /home/deoops/orange.conf /home/deoops/nginx.conf /usr/local/orange/conf/ vi /usr/local/orange/conf/orange.conf ## make sure orange.conf has the right mysql server info vi /data/nginx/conf/api.conf chown root:root /usr/local/orange/conf/*.conf orange start netstat -nlp | grep 443 小结 用了几天，感觉UI比起kong来说简单些，开箱即用的功能比kong也多一些。 稳定性还有待进一步的观察。\n","date":"2018-02-05T19:37:23+08:00","image":"https://tab.deoops.com/posts/try-orange/orange_huaf4bb76d1e84ff1eb3641c169af667d6_43273_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/try-orange/","title":"orange网关"},{"content":"updated: kong不满足要求，后面调研了另外一个API产品 orange\n2015年接触openresty的时候接触过kong，了解到kong是基于openresty二次开发的商业产品，\n正好目前新公司要调研稳定好用的API Gateway产品，所以本文简单记录下我对kong的安装配置感受。\n安装 yum install https://bintray.com/kong/kong-community-edition-rpm/download_file?file_path=centos/7/kong-community-edition-0.12.1.el7.noarch.rpm kong systemctl stop nginx netstat -nlp netstat -nlp|grep 80 ls date vi /etc/kong/kong.conf.default vipw su - postgres create user kong; create database kong ownner kong; create database kong owner kong; cd /etc/kong/ cp kong.conf.default kong.conf vi kong.conf kong migrations up psql -U kong tail /media/data/pgdata/log/postgresql-Wed.log vi /media/data/pgdata/pg_hba.conf systemctl restart postgresql-10.service kong migrations up kong kong check kong start netstat -nlp | grep 80 which nginx curl localhost: netstat -nlp vi /usr/local/kong/nginx-kong.conf which -a nginx kong kong restart --nginx-conf /etc/nginx/nginx.conf cp /etc/nginx/nginx.conf /etc/nginx/nginx.conf.tmpl vi /etc/nginx/nginx.conf.tmpl kong restart --nginx-conf /etc/nginx/nginx.conf.tmpl which -a npm 安装控制台 使用\nwhich -a docker yum remove docker docker-common docker-selinux docker-engine yum install -y yum-utils device-mapper-persistent-data lvm2 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo yum install docker-ce systemctl start docker docker run hello-world curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://2b77623e.m.daocloud.io systemctl restart docker systemctl enable docker docker run -d --rm -p 8080:8080 pgbi/kong-dashboard start --kong-url http://10.0.0.1:8001 --basic-auth usrtest=pwdtest docker ps -a ### registe the very first api through kong admin api curl -i -X POST --url http://10.0.0.11:8001/apis --data \u0026#39;name=admin-api\u0026#39; --data \u0026#39;hosts=a.k.deoops.com\u0026#39; --data \u0026#39;upstream_url=http://localhost:8080\u0026#39; 小结 开源Kong 的限制比较多，反向代理配置基本上靠插件，对现存的nginx的conf兼容性不友好。 简单的proxy_cache配置一定要企业版才支持，下面的更加精细配置也没有支持：\nvalid_referers *.deoops.cn deoops.cn *.deoops.com deoops.com; add_header \u0026#39;Access-Control-Allow-Headers\u0026#39; \u0026#39;os, ver, hwid, innerver, channel, net, token, et\u0026#39;; proxy_cache my_cache; proxy_cache_valid 200 302 304 10s; proxy_ignore_headers Expires Cache-Control Set-Cookie; proxy_cache_key $host$request_uri$is_args$args$http_token; if ($request_uri ~ img_vercode ){ set $no_proxy_cache 1; } if ($remote_addr = 114.13.118.24){ set $no_proxy_cache 1; } ","date":"2018-02-03T18:05:04+08:00","image":"https://tab.deoops.com/posts/try-kong/kong_hu72b17c962d5d607193e13498b681225b_9130_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/try-kong/","title":"kong网关"},{"content":"假设一个golang项目的三个源文件a.go,b.go, c.go，都定义了function inint(){}函数，\n其中c.go文件初始化了一个全局变量globalVar，同时a.go 或者b.go的init func 引用了这个全局变量globalVar。\n那么这个时候就会出现一个问题，在a.go和 b.go的init func中 globalVar的引用是空值。\n示例 文件结构 ❯ tree . ├── a.go ├── b.go ├── c.go ├── go.mod └── main.go 0 directories, 5 files 源代码 // file `a.go` package main import \u0026#34;fmt\u0026#34; func init() { // globalVar is empty fmt.Println(\u0026#34;globalVar in a.go:\u0026#34;, globalVar) } // file `b.go` package main import \u0026#34;fmt\u0026#34; func init() { // globalVar is empty fmt.Println(\u0026#34;globalVar in b.go:\u0026#34;, globalVar) } // file `c.go` package main import ( \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; ) func init() { globalVar = initVar() fmt.Println(\u0026#34;globalVar in c.go:\u0026#34;, globalVar) } func initVar() string { time.Sleep(20 * time.Millisecond) return \u0026#34;late is better than never\u0026#34; } // file `main.go` package main import \u0026#34;fmt\u0026#34; var globalVar = \u0026#34;\u0026#34; func main() { fmt.Println(\u0026#34;vim-go\u0026#34;) } result ❯ go build -o demo ❯ ./demo globalVar in a.go: globalVar in b.go: globalVar in c.go: late is better than never vim-go 解决办法 简单的解决办法可以是重命名c.go为0a.go保证0a.go中的init最早执行完成，\nmv c.go 01.go 这个解决方案的优点是可以不用修改代码，但是不够优雅。\n比较好的解决方式应该是，不要在init func里初始化全局变量，应该直接在top block context中对全局变量初始化：\n// file `c.go` package main var globalVar = initVar() func init() { // ... } ","date":"2018-01-23T16:23:01+08:00","image":"https://tab.deoops.com/posts/golang-init/init_hu78bbcb6727b96f6a349cc768fb1f363a_291492_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/golang-init/","title":"init优先级"},{"content":"换了份工作，新公司是做加密币交易所的，服务器都在国外。所以有机会接触到了微软的azure云服务， 服务器基本都在亚太区新加坡。\nps：这是我第一次实操单台配置32c64g的虚拟机，纪念一下。以前工作中最多就8c16g，数量的话两三百台机器。\nhistory uname -a # kernel info cat /etc/redhat-release df -alh # query disk info ifconfig ping 10.0.0.6 ssh 10.0.0.6 ls .ssh/ mv azagent .ssh/id_rsa ls -alh .ssh/id_rsa ssh 10.0.0.6 ping jd.com yum install yum-utils sudo yum install yum-utils yum -y upgrade sudo yum -y upgrade sudo yum -y update sudo yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo sudo yum install openresty # install web server sudo vi /etc/ssh/sshd_config systemctl status sshd systemctl restart sshd sudo systemctl restart sshd yum install ansible sudo yum install ansible sudo systemctl status openresty sudo systemctl enable openresty sudo systemctl start openresty curl -I localhost uptime date sudo install git sudo yum install git locate openresty rpm -qc openresty # query configuration file wget https://copr.fedorainfracloud.org/coprs/dheche/prometheus/repo/epel-7/dheche-prometheus-epel-7.repo ls head dheche-prometheus-epel-7.repo suod mv dheche-prometheus-epel-7.repo /etc/yum.repos.d/prometheus.repo sudo mv dheche-prometheus-epel-7.repo /etc/yum.repos.d/prometheus.repo sudo yum install prometheus sudo yum install prometheus-node sudo vi /etc/yum.repos.d/prometheus.repo sudo yum install prometheus sudo yum install prometheus2 sudo yum install node_exporter sudo yum install alertmanager vi /etc/hosts sudo vi /etc/hosts ssh ten sudo vi /etc/yum.conf ls /var/cache/yum/x86_64/7/prometheus/ ls /var/cache/yum/x86_64/7/prometheus/packages/ ls -alh /var/cache/yum/x86_64/7/prometheus/packages/ ls sudo yum install yum-utils history sudo yumdownloader prometheus ls ls -alh sudo yumdownloader prometheus2 # download installed rmp packages sudo yumdownloader node_exporter.x86_64 sudo yumdownloader alertmanager.x86_64 ls -alh scp node_exporter-0.15.2-1.el7.centos.x86_64.rpm ten: ssh ten ssh ten \u0026#39;sudo systemctl enable node_exporter\u0026#39; ssh ten \u0026#39;sudo systemctl start node_exporter\u0026#39; sudo systemctl start node_exporter sudo systemctl enable node_exporter sudo systemctl enable alertmanger sudo systemctl enable alertmanager sudo systemctl start alertmanager netstat -nlp sudo -i # su to root pwd ls rm prometheus-1.8.2-1.el7.centos.x86_64.rpm ls sudo systemctl enable prometheus sudo systemctl start prometheus cat/etc/passwd rpm -qc prometheus2 ls -alh /etc/default/prometheus id prometheus cat /etc/default/prometheus ls -alh /etc/prometheus/ sudo -i 总的说来敲了100来条指令，比较重要的是下面三条指令：\nsudo -i切换为root用户； yumdownloader 保存通过yum安装的rpm包 (配合yum -C install 和SCP可以加快LAN下所有主机的安装速度)； rpm -qc 查询安装包配置信息; timeout 默认情况下，Azure 的Linux主机的sshd 服务会把闲着超过1分钟的客户端踢下线ssh session timeout。 修改本地ssh 客户端连接配置添加ServerAliveInterval等配置项就可以规避上面说的timeout限制：\n#### .ssh/config snippet HOST singapore2 HostName 40.50.60.70 IdentityFile ~/.ssh/az ServerAliveInterval 120 ServerAliveCountMax 30 ConnectTimeout 30 ","date":"2017-11-15T10:48:20+08:00","image":"https://tab.deoops.com/posts/azure-centos/Azure_hu06c085390a6f76b660a41a8f6ece78bc_31082_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/azure-centos/","title":"试用Azure Centos虚拟机"},{"content":"TLDR; find: 有很多过滤规则查找文件/目录/设备，而且可以递归查询某一个目录下的目录或文件，最后除了打印查询结果以外，还可以做其它操作（比如删除该文件）； locate: 则简单很多，根据关键字 在缓存index中 检索出含有该关键字的文件或者目录； fzf: 实时模糊查询，可以集成到常用的IDE中（比如fzf.vim)。 demo find The syntax of the Find command is:\nfind [-H] [-L] [-P] [-D debugopts] [-Olevel] [starting-point...] [expression]\n#!/bin/bash find . -name \u0026#34;*tar*gz\u0026#34; -delete find . -name \u0026#34;*tar*xz\u0026#34; -delete find . -name \u0026#34;*tar.xz\u0026#34; du -sh . find . -name \u0026#34;*zip*\u0026#34; -delete find . -type f | perl -lne \u0026#39;print if -B\u0026#39; | xargs rm -f # delete all binary files under . recursivly locate The syntax of the Locate command is:\nlocate [OPTION]... PATTERN..\n查询postgreql db的配置文件：\n#!/bin/bash locate *hba_conf* # equal to this su - postgres psql show config_file; show hba_file; fzf tldr fzf fzf Command-line fuzzy finder. Similar to sk. More information: https://github.com/junegunn/fzf. - Start fzf on all files in the specified directory: find path/to/directory -type f | fzf - Start fzf for running processes: ps aux | fzf - Select multiple files with Shift + Tab and write to a file: find path/to/directory -type f | fzf --multi \u0026gt; filename - Start fzf with a specified query: fzf --query \u0026#34;query\u0026#34; - Start fzf on entries that start with core and end with either go, rb, or py: fzf --query \u0026#34;^core go$ | rb$ | py$\u0026#34; - Start fzf on entries that not match pyc and match exactly travis: fzf --query \u0026#34;!pyc \u0026#39;travis\u0026#34; See also: sk fzf search result ","date":"2017-09-16T08:09:57+08:00","image":"https://tab.deoops.com/posts/find-locate/fzf-command_hu9f9cae896af5f1cefeda9b7392bef7a4_42033_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/find-locate/","title":"查找文件"},{"content":"代理 proxies (正向)代理 代理一般是指正向代理，比如翻墙软件shadowsocks就是一种正向代理。 shadowsocks通过socks 5协议 在代理服务器上，\n代理我们（client）去访问被墙的资源（google/twitter/Facebook等服务器）。\nforward proxy server A proxy server, sometimes referred to as a forward proxy, is a server that routes traffic between client(s) and another system, usually external to the network. By doing so, it can regulate traffic according to preset policies, convert and mask client IP addresses, enforce security protocols, and block unknown traffic.\n反向代理 反向代理是说我们（client）被代理了， 我们自己还不知道。 我们以为和我们打交道的（处理我们的请求）的是nginx 服务器，其实nginx真正处理我们请求的是ngix后面的upstream在 处理我们的请求逻辑。\nreverse proxy server A reverse proxy is a type of proxy server. Unlike a traditional proxy server, which is used to protect clients, a reverse proxy is used to protect servers. A reverse proxy is a server that accepts a request from a client, forwards the request to another one of many other servers, and returns the results from the server that actually processed the request to the client as if the proxy server had processed the request itself. The client only communicates directly with the reverse proxy server and it does not know that some other server actually processed its request.\nconfig ### common set_header # /opt/nginx/conf.d/common.proxy; proxy_hide_header Vary; proxy_set_header Accept-Encoding \u0026#39;\u0026#39;; proxy_ignore_headers Cache-Control Expires; proxy_set_header Cookie $http_cookie; proxy_set_header Referer $http_referer; proxy_set_header X-Forwarded-Host $host; proxy_set_header X-Forwarded-Server $host; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; # csrf proxy_set_header X-CSRF-TOKEN $http_x_xsrf_token; proxy_buffer_size 128; proxy_buffers 16 64k; proxy_busy_buffers_size 128; proxy_temp_file_write_size 128; demo 下面的两个demo都使用了include /opt/nginx/conf.d/common.proxy指令，来包含上面的proxy配置。\nhttp ### demo1-http upstream upV1 { server 172.26.2.5:9090 fail_timeout=0; server 172.26.2.6:9090 fail_timeout=0; } upstream upV2 { server 172.26.2.5:8080 fail_timeout=0; server 172.26.2.6:8080 fail_timeout=0; } server { listen 80 default backlog=16384; server_name api.deoops.com; access_log /data/nginx/logs/grafana_access.log main; error_log /data/nginx/logs/grafanar_err.log ; include /opt/nginx/conf.d/common.proxy; location /v1 { proxy_redirect off; proxy_pass http://upV1; } location /v2 { proxy_redirect off; proxy_pass http://upV2; } } https ### demo2-https server { listen 443 default ssl; server_name deoops.com; root /usr/local/nginx/html; index index.html; proxy_ssl_session_reuse off; ssl_session_cache shared:SSL:1m; ssl_session_timeout 10m; ssl_certificate /var/lib/teleport/deoops.com/cert.pem; ssl_certificate_key /var/lib/teleport/deoops.com/key.pem; ssl_verify_client off; ssl_protocols SSLv3 TLSv1 TLSv1.1 TLSv1.2; ssl_ciphers RC4:HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; location / { proxy_redirect off; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwared-For $proxy_add_x_forwarded_for; proxy_pass https://localhost:33000; } } ","date":"2017-05-19T10:30:05+08:00","image":"https://tab.deoops.com/posts/nginx-proxy/reverse-proxy-packet-flow_hu8a37caf62bee1d6c91ad288e729da131_104181_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/nginx-proxy/","title":"代理和反向代理"},{"content":"众所周知nginx有很强的分发静态文件的能力，很多时候nginx对静态资源分发能力的瓶颈和redis一样在主机的网卡上。\n(一般虚拟机的网卡只有500mbps，如果你使用的是万兆的物理网卡就当我没说）\n和redis对比，nginx有另外一个瓶颈在服务器的硬盘IO上，SSD硬盘情况会好一些， 所以很多情况下，我们会把 nginx的cache 做在系统的ssd硬盘上，\n其实还可以直接把cache放到内存文件系统里，进一步提升磁盘io吞吐。\ntmpfs differences between ramfs and tmpfs\n#!/bin/bash mkdir /mnt/ramdisk mount -t tmpfs -o size=512m tmpfs /mnt/ramdisk echo \u0026#39;tmpfs /mnt/ramdisk tmpfs nodev,nosuid,noexec,nodiratime,size=1024M 0 0\u0026#39; \u0026gt;\u0026gt; /etc/fstab nginx http cache config http { more_set_headers \u0026#39;Server: CachedLOL\u0026#39;; proxy_cache_path /var/cache/nginx levels=1:2 use_temp_path=on keys_zone=one:500m max_size=5g inactive=120m; proxy_temp_path /var/cache/nginx/tmp 1 2; } location conf upstream upV1 { server 172.26.2.5:9090 fail_timeout=0; server 172.26.2.6:9090 fail_timeout=0; } server { listen 80 default backlog=16384; server_name tab.deoops.com; location ~* ( /static.*|/list.+|/ )$ { proxy_redirect off; proxy_cache one; proxy_ignore_headers \u0026#34;Set-Cookie\u0026#34;; proxy_hide_header \u0026#34;Set-Cookie\u0026#34;; add_header X-Cache $upstream_cache_status; proxy_cache_key $uri$is_args$args$mobile; proxy_cache_min_uses 1; proxy_cache_valid 120m; proxy_cache_use_stale error timeout; proxy_buffering on; proxy_pass http://upV1; } ","date":"2017-04-29T15:36:18+08:00","image":"https://tab.deoops.com/posts/nginx-cache/tmpfs_hu7dcef1ecd08a87c487abf54ac3038b05_40316_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/nginx-cache/","title":"缓存静态文件"},{"content":"开发需要能调用facebook的接口，我们运维这边需要配置一台测试服务器能访问facebook，用shadowsocks 和squid 代理，性能不够好。所以决定上openvpn。\n简单记录下openVPN的安装配置过程，服务端和客户端使用的操作系统均是centos 7。\n服务端 安装 #!/bin/bash yum install epel-release -y yum install openvpn openssl -y 自签名证书 使用openssl工具生产自签名的ca，证书，client.key，并把这些证书传给客户端：\n#!/bin/bash ### CA openssl dhparam -out /etc/openvpn/dh.pem 2048 openssl genrsa -out /etc/openvpn/ca.key 2048 openssl req -new -key /etc/openvpn/ca.key -out /etc/openvpn/ca.csr -subj /CN=OpenVPN-CA/ openssl x509 -req -in /etc/openvpn/ca.csr -out /etc/openvpn/ca.crt -signkey /etc/openvpn/ca.key -days 3650 echo 01 \u0026gt; /etc/openvpn/ca.srl chmod 600 /etc/openvpn/ca.key ### Server openssl genrsa -out /etc/openvpn/server.key 2048 openssl req -new -key /etc/openvpn/server.key -out /etc/openvpn/server.csr -subj /CN=OpenVPN/ openssl x509 -req -in /etc/openvpn/server.csr -out /etc/openvpn/server.crt -CA /etc/openvpn/ca.crt -CAkey /etc/openvpn/ca.key -days 3650 chmod 600 /etc/openvpn/server.key ### Client openssl genrsa -out /etc/openvpn/client.key 2048 openssl req -new -key /etc/openvpn/client.key -out /etc/openvpn/client.csr -subj /CN=OpenVPN-Client/ openssl x509 -req -in /etc/openvpn/client.csr -out /etc/openvpn/client.crt -CA /etc/openvpn/ca.crt -CAkey /etc/openvpn/ca.key -days 3650 chmod 600 /etc/openvpn/client.key ### 把clinet的证书私钥和ca正式传给客户端 scp /etc/openvpn/ca.crt /etc/openvpn/client.crt /etc/openvpn/client.key client: 配置 服务端配置文件: # /etc/openvpn/server.conf server 10.8.0.0 255.255.255.0 verb 3 key /etc/openvpn/server.key ca /etc/openvpn/ca.crt cert /etc/openvpn/server.crt dh /etc/openvpn/dh.pem keepalive 10 120 persist-key persist-tun comp-lzo push \u0026#34;redirect-gateway def1 bypass-dhcp\u0026#34; push \u0026#34;dhcp-option DNS 8.8.8.8\u0026#34; push \u0026#34;dhcp-option DNS 8.8.4.4\u0026#34; user nobody group nogroup proto udp port 1194 dev tun1194 status openvpn-status.log kernel iptables 打开服务器路由配置：\n#!/bin/bash echo \u0026#39;net.ipv4.ip_forward=1\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p iptables -t nat -A POSTROUTING -s 10.8.0.0/24 -o eth0 -j MASQUERADE 启动 服务器启动openVPN服务：\n#!/bin/bash systemctl enable openvpn@server systemctl start openvpn@server 客户端 安装 #!/bin/bash yum install epel-release -y yum install openvpn -y 配置 证书 把前面服务端签名的证书移动到配置目录下：\n#!/bin/bash mv ~/ca.crt /etc/openvpn mv ~/client* /etc/openvpn/ 配置文件 # /etc/openvpn/client.conf client nobind dev tun redirect-gateway def1 bypass-dhcp remote CHANGE_TO_YOUR_SERVER_IP 1194 udp comp-lzo yes #duplicate-cn key /etc/openvpn/client.key cert /etc/openvpn/client.crt ca /etc/openvpn/ca.crt 启动 客户端启动openVPN服务：\n#!/bin/bash systemctl enable openvpn@client systemctl start openvpn@client 参考How to Install OpenVPN on CentOS 7\n","date":"2017-02-20T13:24:19+08:00","image":"https://tab.deoops.com/posts/install-openvpn/openvpnflows_hu2d0b3a655f7670ace66d99e0fd5bbe0d_106468_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/install-openvpn/","title":"安装配置openvpn"},{"content":"本文不定期更新 :)\nA system administrator\u0026rsquo;s guide to getting started with Ansible\nad-hoc 管理集群的时候，常常来不及写playbooks，只需要执行一些ad-hoc查看某些主机的状态， 或者批量修改/上传配置文件到某些主机。\nansible all -m copy -a \\ \u0026#39;src=dvd.repo dest=/etc/yum.repos.d owner=root group=root mode=0644\u0026#39; -b playbook ansible-playbook -i prod_hosts demo.yml --skip-tag downloaded host file [api] tt ansible_host=test tt3 ansible_host=test3 tt8 ansible_host=test8 [db] pg1 ansible_host=db88 pg2 ansible_host=db98 task # demo.yml --- - hosts: db vars: tar_src: \u0026#34;tars/postgres_exporter_v0.4.1_linux-amd64.tar.gz\u0026#34; tar_dest: \u0026#34;/usr/bin/\u0026#34; service_src: \u0026#34;services/postgres_exporter.service\u0026#34; service_dest: \u0026#34;/usr/lib/systemd/system/\u0026#34; # works on centos; ubuntu is \u0026#39;/etc/systemd/system/ tasks: - debug: var=ansible_default_ipv4.address - name: untar to /usr/bin unarchive: src: \u0026#34;{{ tar_src }}\u0026#34; dest: \u0026#34;{{ tar_dest }}\u0026#34; become: true - name: download and untar prometheus tarball tags: downloaded unarchive: src: \u0026#34;{{ prometheus_tarball_url }}\u0026#34; dest: \u0026#34;{{ prometheus_install_path }}\u0026#34; copy: no - name: copy service file copy: src: \u0026#34;{{ service_src }}\u0026#34; dest: \u0026#34;{{ service_dest }}\u0026#34; become: true - name: ensure node_export is ebalbe and running systemd: name: postgres_exporter enabled: yes daemon_reload: yes state: started become: true ","date":"2016-04-16T11:33:05+08:00","image":"https://tab.deoops.com/posts/ansible-memo/ansible_hu3bbbeae4508296fbc307f486d3109a0d_38952_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/ansible-memo/","title":"Ansible"},{"content":"我们一般会调整内核tcp参数以提高web服务器(比如ngin)的性能。\nsysctl 加载Linux 内核配置\nsysctl -p /etc/sysctl.d/xxx-xxx.conf meat # /etc/sysctl.d/00-network.conf # Receive Queue Size per CPU Core, number of packets # Example server: 8 cores net.core.netdev_max_backlog = 4096 # SYN Backlog Queue, number of half-open connections net.ipv4.tcp_max_syn_backlog = 32768 # Accept Queue Limit, maximum number of established # connections waiting for accept() per listener. net.core.somaxconn = 65535 # Maximum number of SYN and SYN+ACK retries before # packet expires. net.ipv4.tcp_syn_retries = 1 net.ipv4.tcp_synack_retries = 1 # Timeout in seconds to close client connections in # TIME_WAIT after receiving FIN packet. net.ipv4.tcp_fin_timeout = 5 # Disable SYN cookie flood protection net.ipv4.tcp_syncookies = 0 # Maximum number of threads system can have, total. # Commented, may not be needed. See user limits. #kernel.threads-max = 3261780 # Maximum number of file descriptors system can have, total. # Commented, may not be needed. See user limits. #fs.file-max = 3261780 ","date":"2016-04-05T18:58:52+08:00","image":"https://tab.deoops.com/posts/tcp-tune/sysctl_hue759758d58191cbb59e001a219022668_168191_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/tcp-tune/","title":"tcp性能调优"},{"content":"很久以前有幸遇到一款很干净简洁 blog saas产品，可以通过email和dropbox写/备份文章。\n今天发现scripttogr 已经关闭不运营了。\n可惜了。\n好的产品不一定能活下去，可惜了。\n","date":"2015-07-13T19:29:06+08:00","image":"https://tab.deoops.com/posts/blog-scripttogr/scriptogr_hua4314aba5df0dc7a2b72f3a924b77952_65215_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/blog-scripttogr/","title":"再见scripttogr👋"},{"content":"使用opkg安装软件时，常常需要对候软件包进行初始化或者自定义化操作，这种开发需求一般写给shell脚本就可以对付了。 现在的问题是当这些脚本多了之后，原作者也不愿意修改安装包，我们怎么分发这些自定义的脚本，能不能把自定义的这些脚本编译到opkg包里？\n位置 把本地的shell脚本放在openwert 仓库的这个目录，编译openwrt的时候就会被打包到对应opkg二进制文件中：\n/barrier_breaker/package/package-abc # package makefile文件所在 package-abc/files # shell脚本放置目录 步骤 修改Makefile 在package目录下任意找一个package目录，比如chinadns, 然后修改Makefile文件。 在install语句后添加 :\n$(INSTALL_BIN) ./files/your_script.sh $(1)/etc/config/your_script.sh 放置脚本 将脚本your_script.sh放置在files目录下；\n选择opkg 在make menuconfig 图像界面中选择修改过的包（chinadns）\n附 package 和注入文件相关的部分makefile代码:\n## include $(TOPDIR)/rules.mk PKG_NAME:=xxx PKG_RELEASE:=1 PKG_SOURCE:=$(PKG_NAME)-$(PKG_VERSION).tar.gz PKG_SOURCE_URL:=https://github.com/xxx/releases/download/v$(PKG_VERSION) PKG_MD5SUM:=f772a750580243cfxcsfd2xc39d7b9171b1 PKG_BUILD_DIR:=$(BUILD_DIR)/$(PKG_NAME) include $(INCLUDE_DIR)/package.mk ## define Package/xxx SECTION:=net CATEGORY:=Network TITLE:=xxx endef ### define Package/xxx/description button haha upgrade. endef ### #define Package/xxx/conffiles #/etc/config/system #/etc/hotplug.d/button/00-button #endef ### define Package/wps_button/install #$(INSTALL_DIR) $(1)/etc/init.d $(INSTALL_BIN) ./files/xxx $(1)/etc/config/xxx #$(INSTALL_DATA) ./files/system.conf $(1)/etc/config/system endef ### $(eval $(call BuildPackage,xxx)) ## ","date":"2015-06-18T13:32:58+08:00","image":"https://tab.deoops.com/posts/opkg-inject/opkg_hudcc7dbad3673a4c7158406c58241e484_47683_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/opkg-inject/","title":"脚本注入"},{"content":"运维人员/系统管理员每天要在终端敲入大量命令，也要修改查看大量文本配置文件，日志信息。 甚至可以夸张一点说Linux/Unix system admin的全部工作就是和字符串打交道。\nbinary 大家都知道文本文件是字符串组成的，其实二进制文件里面其实也包含了很多字符串：\n➜ infra-api git:(dev) file infra-api infra-api: Mach-O 64-bit executable x86_64 ➜ infra-api git:(dev) strings infra-api | head flag hash mime path sort sync time *int AAAA Addr ➜ infra-api git:(dev) 命令 收集一些常用的shell 字符串操作命令\ncut ❯ tldr cut Cut out fields from stdin or files. More information: https://www.gnu.org/software/coreutils/cut. - Cut out the first sixteen characters of each line of stdin: cut -c 1-16 - Cut out the first sixteen characters of each line of the given files: cut -c 1-16 file - Cut out everything from the 3rd character to the end of each line: cut -c 3- - Cut out the fifth field of each line, using a colon as a field delimiter (default delimiter is tab): cut -d\u0026#39;:\u0026#39; -f5 - Cut out the 2nd and 10th fields of each line, using a semicolon as a delimiter: cut -d\u0026#39;;\u0026#39; -f2,10 - Cut out the fields 3 through to the end of each line, using a space as a delimiter: cut -d\u0026#39; \u0026#39; -f3- cat/less cat，主要有三大使用场景：\n一次显示整个文件 cat filename; 从标准输入流（键盘）新建一个文件。cat \u0026gt; filename; 将几个文件合并为一个文件： cat file1 file2 \u0026gt; file。 参数： -n 或 \u0026ndash;number 由 1 开始对所有输出的行数编号 -b 或 \u0026ndash;number-nonblank 和 -n 相似，只不过对于空白行不编号 -s 或 \u0026ndash;squeeze-blank 当遇到有连续两行以上的空白行，就代换为一行的空白行 -v 或 \u0026ndash;show-nonprinting 例：\n把 textfile1 的档案内容加上行号后输入 textfile2 这个档案里 cat -n textfile1 \u0026gt; textfile2 把 textfile1 和 textfile2 的档案内容加上行号（空白行不加）之后将内容附加到 textfile3 里。 cat -b textfile1 textfile2 \u0026gt;\u0026gt; textfile3 清空test.txt cat /dev/null \u0026gt; /etc/test.txt 另外，本文读取文件的操作可用less命令替代，less命令速度会更快些\nawk awk可以算独立的一门编程语言，举两个例子：\n截取路由器MAC地址后四位 $(cat /sys/class/ieee80211/${dev}/macaddress) | awk -F \u0026#34;:\u0026#34; \u0026#39;{ print $5\u0026#34;\u0026#34;$6 }\u0026#39; | tr a-z A-Z echo sd:3s:xf:0h:w4:n8 | awk -F \u0026#34;:\u0026#34; \u0026#39;{ print $5\u0026#34;\u0026#34;$6 }\u0026#39; | tr a-z A-Z W4N8 对某一列求和 awk -F\u0026#39;,\u0026#39; \u0026#39;{sum+=$2;} END{print sum;}\u0026#39; file.txt 运行方式 命令行方式 awk [-F field-separator] \u0026#39;commands\u0026#39; input-file(s) 其中，commands 是awk命令，[-F域分隔符]是可选的。 input-file(s) 是待处理的文件。 在awk中，文件的每一行中，由域分隔符分开的每一项称为一个域。通常，在不指名-F域分隔符的情况下，默认的域分隔符是空格。\n脚本方式 将所有的awk指令写入一个文件，并使awk程序可执行，然后把awk解释器作为脚本的首行，即可运行awk脚本。 把脚本首行（magic bang）：#!/bin/sh 换成： #!/bin/awk 即可，以 which awk 的输出为准。\n将所有的awk命令插入一个单独文件，然后调用：\nawk -f awk-script-file input-file(s) 其中，-f选项加载awk-script-file中的awk脚本，input-file(s)跟上面的是一样的。\ntr tr 命令从标准输入删除或替换字符，并将结果写入标准输出。根据由 String1 和 String2 变量指定的字符串以及指定的标志，tr 命令可执行三种操作。\n本文使用了简单的替换操作。小写字母转为大写字母。\n","date":"2015-05-17T15:11:27+08:00","image":"https://tab.deoops.com/posts/shell-string/comma-seperator_hua66cc9b82d283c00f3eead5bea7f7922_5819_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/shell-string/","title":"字符串"},{"content":"很多路由器的flash容量只有4m大，所以绝大多数openwrt固件也是4m大小。 当我们手动改造路由器加大flash容量后，可以调整openwrt默认设置使得编译出来的factory可以有8m的大小， 从而安装更多的内置软件。\n本文以wr703n路由器 为例子，简单介绍一下如何加大固件的容量，让我们预安装更多内置软件。\n查看 # ./tools/firmware-utils/src/mktplinkfw.c fw_max_len为0xfc0000，16M flash fw_max_len为0x7c0000，8M flash 修改 # ./target/linux/ar71xx/image/Makefile # 将703n的4Mlzma改为8Mlzma或16Mlzma $(eval $(call SingleProfile,TPLINK-LZMA,$(fs_64kraw),TLWR703,tl-wr703n-v1,TL-WR703N,ttyATH0,115200,0x07030101,1,8Mlzma)) ","date":"2015-05-07T11:45:48+08:00","image":"https://tab.deoops.com/posts/firmware-size/wr703n_hubffc7a239a3de7b3f5b1ee9ca3ee129e_14730_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/firmware-size/","title":"增加固件大小"},{"content":"交叉编译 在ubuntu下安装编译工具（gcc,xmllib,cmake, git 等)； git克隆openwrt仓库：git clone git://git.openwrt.org/14.07/openwrt.git； 自定义kernel target： 源代码做两处修改 :\ntarget/linux/ramips/image/Makefile /base-files/lib/ramips.sh target/linux/ramips/base-files/lib/preinit/06_set_iface_mac 在弹出的make menuconfig 图像界面中选择cpu型号； 打开vpn开始编译固件。 结果 交叉编译完成后，根据上一步选择的安装包的多少，bin目录下会生成对应的opkg包，和固件：\nfactory文件，可以称作底包； sysupgrade文件，可以称作升级包； web/uboot烧录刷机 接通路由器电源，按住WPS按钮不放，然后按电源键开机， power LED快闪即松开WPS键，此时路由器已加入升级模式； 访问路由器web地址（如：http://192.168.1.1), 按照web界面提示选取factory文件完成固件烧录刷机； ssh/ftp 烧录 可以直接执行sysupgrade命令烧录估计：\nssh-keygen -f \u0026#34;/home/openwrt-qqm/.ssh/known_hosts\u0026#34; -R 192.168.1.1 #可以省略此条命令 scp xxxxxx-squashfs-sysupgrade.bin root@192.168.1.1:/tmp/ ssh root@192.168.1.1 cd /tmp/ sysupgrade -n xxxxxxxxxxx-sysupgrade.bin opkg 烧录完系统固件后，可以使用opkg安装软件包，比如china-dns, shadowsocks, openvpn，等等。\nhg255d router ","date":"2015-03-17T09:09:58+08:00","image":"https://tab.deoops.com/posts/router-hg255d/hg255d_hu95abe093e5988d14ccd200bc31cd3495_35267_120x120_fill_q75_box_smart1.jpg","permalink":"https://tab.deoops.com/posts/router-hg255d/","title":"刷机HG255d"},{"content":"刚刚在google+上看到google 推出了自己的域名注册服务，截两张图，纪念下。\ngoogle推广自己的 com.google 域名注册托管服务，把首页变成了镜像模式 google domain 搜索正常，不过搜索结果也使用了镜像效果： serach page ","date":"2014-06-17T14:22:06+08:00","image":"https://tab.deoops.com/posts/google-domain/google-domain_hu8addcdeb3c155fbae74eb60aed2e0ca1_53610_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/google-domain/","title":"Google Domain"},{"content":"从Firefox的查看元素\u0026gt;查看器入手，发现有搞头。然后又谷歌查阅了些资料，发现我想的方法对firefox浏览器的依赖性太强，最多也就能做成firefox的一个插件。\n简要记下我原来的思路：\nhtml源文档，用word打开的话，就可以自动去除所有的\u0026lt;tag\u0026gt;，当然也可以用perl写些正则表达式，所以搞到html源文档就有搞头。\n百度文库啊，豆丁啊，刀客88啊，什么的，表面上好像把源文档藏的好好的，但是只要用firefox查看下元素，他们就怂了，穿什么色的内裤都看的一清二楚。\n然后，成也萧何败也萧何，不想太依赖firefox。\n所以，作罢。\n不过也有些小收获，可以做些小的恶作剧。\n比如下图一，和下图二的恶作剧：\nquiz （图一，恶搞的是百度文库的每个考试试卷文档，还不算太歪）\nweibo （图二，恶搞的是文章。。。~~话说人家劈人家的腿与你何干？ ~~话说我没有节操又与他们劈不劈腿何干？）\n","date":"2014-05-12T16:05:52+08:00","image":"https://tab.deoops.com/posts/wai-lou/wl_hu1ef2eff8c91d20922d459e203a102b41_263557_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/wai-lou/","title":"歪楼"},{"content":"接连下了一周的雨，早上醒来的时候感觉被子有点潮潮的。\n听着外面的雨声，思索着要不要起床，忽然意识到明天要上班，于是又蒙头睡过。\n又过了会，醒了，起床洗漱，刷牙的时候我喜欢听歌，洗脸的时候也喜欢听歌，洗澡的时候也是，还有洗头的时候。\n感觉昨天晚餐还没有消化完全，所以就不急着吃早餐。四处晃了晃，下雨感觉干什么都没劲，打开电脑，然后是google.com.hk\ngoogle doodles 哦，今天是母亲节啊。 思绪一下子被打开了好大个缺口，好多情感向我袭来，\n眼看我就要被种种幸福的、委屈的、亲切的、美好的、懊悔的、恋恋不舍的，等等，回忆包围，\n毕业一年多，漂泊在外的我突然发现我好无助，然后不知道为啥，这些回忆就突然间模糊烟消云散了。\nchild 慢慢来吧，生活总会是想要的那样的。\n下楼买早点去了，话说这把伞有点小了吧。\numbrella 撑着伞在雨里走着，想起刚来这工作的时候，和某个人一起打伞，还给了她50块钱，现在还没还给我。\nAnyway ，HAPPY MOTHER\u0026rsquo;S DAY! mother ","date":"2014-05-10T16:21:02+08:00","image":"https://tab.deoops.com/posts/da-yu/rain_hucaec8e4da44ebda5edf82238e0330ad2_83156_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/da-yu/","title":"雨好大"},{"content":"在天朝呆久了，有点迫害妄想症。用vpngate出来转转看看，发现确实是我想多了。\nanyway，谢谢vpngate，挺不错的vpn。\n","date":"2014-05-09T16:39:24+08:00","image":"https://tab.deoops.com/posts/try-vpngate/vpngate_hu20fc8f9a8163927a082655de56da28e2_239034_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/try-vpngate/","title":"谢谢vpngate"},{"content":"文章已丢失 :(\n","date":"2014-05-08T16:34:49+08:00","image":"https://tab.deoops.com/posts/flow-3d/software_flow_science_hu30f49a79fd758234ca8cd7ff1ae76c1b_254580_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/flow-3d/","title":"第七章FLOW3d铸造模具热循环分析"},{"content":"updated at 2014/2/16 坑太大，挑战太多，挑战失败，放弃啦 :)\n看完Ruby On Rails tutorial，感觉热血沸腾。 来吧，少年，来写个CMS吧。\nmodel 数据库设计\ngenerate 生成model schema数据模型：\nrails g model App \\ title:string icon:binary descript:text \\ get_url:string hits:integer downloaded:integer score:decimal \\ version:string require_os_version:string author:references rails g model Author name:string descript:text website:string populate 填充假数据，便于测试。 faker gem\n#db/seed.rb 30.times do |n| ne = Faker::App.author dt = Faker::Lorem.paragraph we = Faker::Internet.url Author.create!(name: ne, descript: dt, website: we ) end users = Author.order(:created_at).take(17) 50.times do users.each do |u| te = Faker::App.name dt = Faker::Hacker.say_something_smart gu = Faker::Internet.url hs = Faker::Number.number(5) dd = Faker::Number.number(4) se = Faker::Commerce.price vn = Faker::App.version rn = \u0026#34;android 1.6+ | ios 6.0+\u0026#34; u.apps.create!(icon: nil, title: te, descript: dt, get_url: gu, hits: hs, downloaded: dd, score: se, version: vn, require_os_version: rn) end end 数据录入 手动/人工录入 form表单 机器抓取 nokogiri gem UI/static_pages_controller controller 新建controller\nrails g controller StaticPages index rails g controller Apps update destroy #destroy widget rails d controller widgets rails d model Widget scss 调整scss\n# 修改gemfile gem \u0026#39;bootstrap-will_paginate\u0026#39; gem \u0026#39;will_paginate\u0026#39; gem \u0026#39;bootstrap-sass\u0026#39; rename scss文件\nmv path/to/application.css path/to/application.scss @import \u0026#34;bootstrap-sprockets\u0026#34;; @import \u0026#34;bootstrap\u0026#34;; /* mixins, variables, etc. */ $gray-medium-light: #eaeaea; @mixin box_sizing { -moz-box-sizing: border-box; -webkit-box-sizing: border-box; box-sizing: border-box; } /* universal */ html { overflow-y: scroll; } body { padding-top: 60px; } /* ....... */ erb 添加title自定义支持\n# application.html.erb添加 \u0026lt;title\u0026gt;\u0026lt;%= full_title(yield(:title)) %\u0026gt;\u0026lt;/title\u0026gt; # static_pages_helper.rb添加 def full_title(page_title = \u0026#39;\u0026#39;) base_title = \u0026#34;LOL\u0026#34; if page_title.empty? base_title else \u0026#34;#{page_title} | #{base_title}\u0026#34; end end action 编辑index action\n# static_pages_controller.rb def index App.per_page = 8 @apps = App.paginate(page: params[:page]) end # views/static_pages/insex.html.erb \u0026lt;% provide(:title, \u0026#34;All apps\u0026#34;) %\u0026gt; # 页面标题 \u0026lt;%= render @apps %\u0026gt; \u0026lt;%= will_paginate @apps %\u0026gt; # views/apps/_app.html.erb \u0026lt;span\u0026gt; \u0026lt;%= app.title %\u0026gt; * * \u0026lt;/span\u0026gt; ","date":"2013-12-23T13:28:34+08:00","image":"https://tab.deoops.com/posts/rails-cms/ror_hu2407918d44206a06e3da94e6d5a4c08a_66970_120x120_fill_q75_box_smart1.jpeg","permalink":"https://tab.deoops.com/posts/rails-cms/","title":"想写个cms"},{"content":"跟着rails tutorial 学习rails框架时，遇到了db链接的问题\n问题 rake db:create failed PG::ConnectionBad: could not connect to server: No such file or directory Is the server running locally and accepting connections on Unix domain socket \u0026#34;/var/pgsql_socket/.s.PGSQL.5432\u0026#34;? Google之后发现是database.yml配置文件没有加上host：localhost配置项。\n过一会，发现PATH没有包含psql命令。\nvi ~/.bash_profile #添加下面一行内容 export PATH=\u0026#34;/Applications/Postgres.app/Contents/Versions/9.4/bin:$PATH\u0026#34; exit ## or echo export PATH=\u0026#34;/Applications/Postgres.app/Contents/Versions/9.4/bin:$PATH\u0026#34; \u0026gt;\u0026gt; ~/.zshrc 终于rake db:migrate 成功。\n附录 附上database.yml（production环境使用heroku环境变量)\ndefault: \u0026amp;default adapter: postgresql encoding: unicode # For details on connection pooling, see rails configuration guide # http://guides.rubyonrails.org/configuring.html#database-pooling pool: 5 development: \u0026lt;\u0026lt;: *default host: localhost database: xxx_development test: \u0026lt;\u0026lt;: *default host: localhost database: xxx_test production: \u0026lt;\u0026lt;: *default database: xxx username: xxx password: \u0026lt;%= ENV[\u0026#39;xxx_xxx_PASSWORD\u0026#39;] %\u0026gt; ","date":"2013-11-07T10:06:54+08:00","image":"https://tab.deoops.com/posts/rails-pg/rails-pg_hu20816ecef9f64b14409d959567a3af94_88803_120x120_fill_box_smart1_3.png","permalink":"https://tab.deoops.com/posts/rails-pg/","title":"连接数据库"}]